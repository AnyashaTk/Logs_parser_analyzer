2022-12-14 12:51:16,526 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting ResourceManager
STARTUP_MSG:   host = adh-test-00-mnode1.ru-central1.internal/10.130.0.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.2
STARTUP_MSG:   classpath = /etc/hadoop/conf:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/stax2-api.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jetty-io-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-security-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/jetty-server-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/woodstox-core.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/zookeeper-3.4.13.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jetty-util-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/httpclient-4.5.2.jar:/usr/lib/hadoop/lib/httpcore-4.4.4.jar:/usr/lib/hadoop/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/jackson-annotations-2.7.8.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/jackson-core-2.7.8.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jackson-databind-2.7.8.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-xml-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/jetty-http-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.1.2.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-auth-3.1.2.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-common-3.1.2-tests.jar:/usr/lib/hadoop/.//hadoop-common-3.1.2.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms-3.1.2.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-nfs-3.1.2.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.2.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.1.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.1.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.1.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.1.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-2.8.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.271.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-0.8.2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//lz4-1.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//netty-buffer-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//netty-codec-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//netty-codec-http-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//netty-common-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//netty-handler-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//netty-resolver-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//netty-transport-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.1.2-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-yarn/./:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar
STARTUP_MSG:   build = git@ssh.gitlab.adsw.io:arenadata/infrastructure/code/ci/prj_adh.git -r 15525ac41ebda8bc83c285e9b6ea7fb9921fd7fd; compiled by 'jenkins' on 2022-09-08T14:33Z
STARTUP_MSG:   java = 1.8.0_352
************************************************************/
2022-12-14 12:51:16,534 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]
2022-12-14 12:51:16,707 INFO org.apache.hadoop.conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf/core-site.xml
2022-12-14 12:51:16,800 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found
2022-12-14 12:51:16,801 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.
2022-12-14 12:51:16,889 INFO org.apache.hadoop.conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf/yarn-site.xml
2022-12-14 12:51:16,928 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
2022-12-14 12:51:16,963 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
2022-12-14 12:51:16,967 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
2022-12-14 12:51:16,971 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
2022-12-14 12:51:17,033 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreFactory: Using RMStateStore implementation - class org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore
2022-12-14 12:51:17,034 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
2022-12-14 12:51:17,039 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
2022-12-14 12:51:17,039 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
2022-12-14 12:51:17,076 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher
2022-12-14 12:51:17,077 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher
2022-12-14 12:51:17,078 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher
2022-12-14 12:51:17,079 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher
2022-12-14 12:51:17,192 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-resourcemanager.properties,hadoop-metrics2.properties
2022-12-14 12:51:17,209 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2022-12-14 12:51:17,209 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system started
2022-12-14 12:51:17,227 INFO org.apache.hadoop.yarn.security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.
2022-12-14 12:51:17,229 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
2022-12-14 12:51:17,237 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
2022-12-14 12:51:17,239 INFO org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo: Registered RMNMInfo MBean
2022-12-14 12:51:17,239 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.
2022-12-14 12:51:17,245 INFO org.apache.hadoop.util.HostsFileReader: Refreshing hosts (include/exclude) list
2022-12-14 12:51:17,245 INFO org.apache.hadoop.util.HostsFileReader: Adding a node "adh-test-00-mnode1.ru-central1.internal" to the list of included hosts from /etc/hadoop/conf/include-path
2022-12-14 12:51:17,245 INFO org.apache.hadoop.util.HostsFileReader: Adding a node "adh-test-00-mnode2.ru-central1.internal" to the list of included hosts from /etc/hadoop/conf/include-path
2022-12-14 12:51:17,245 INFO org.apache.hadoop.util.HostsFileReader: Adding a node "adh-test-00-mnode3.ru-central1.internal" to the list of included hosts from /etc/hadoop/conf/include-path
2022-12-14 12:51:17,274 INFO org.apache.hadoop.conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf/capacity-scheduler.xml
2022-12-14 12:51:17,289 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1024, vCores:1>
2022-12-14 12:51:17,289 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Maximum allocation = <memory:4096, vCores:2>
2022-12-14 12:51:17,327 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined
2022-12-14 12:51:17,327 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined
2022-12-14 12:51:17,337 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=ADMINISTER_QUEUE:*SUBMIT_APP:*, labels=*,
, reservationsContinueLooking=true, orderingPolicy=utilization, priority=0
2022-12-14 12:51:17,338 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root
2022-12-14 12:51:17,352 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined
2022-12-14 12:51:17,352 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined
2022-12-14 12:51:17,357 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Initializing default
capacity = 1.0 [= (float) configuredCapacity / 100 ]
absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]
maxCapacity = 1.0 [= configuredMaxCapacity ]
absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
effectiveMinResource=<memory:0, vCores:0>
 , effectiveMaxResource=<memory:0, vCores:0>
userLimit = 100 [= configuredUserLimit ]
userLimitFactor = 1.0 [= configuredUserLimitFactor ]
maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
maxAMResourcePerQueuePercent = 0.2 [= configuredMaximumAMResourcePercent ]
minimumAllocationFactor = 0.75 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
maximumAllocation = <memory:4096, vCores:2> [= configuredMaxAllocation ]
numContainers = 0 [= currentNumContainers ]
state = RUNNING [= configuredState ]
acls = ADMINISTER_QUEUE:*SUBMIT_APP:* [= configuredAcls ]
nodeLocalityDelay = 40
rackLocalityAdditionalDelay = -1
labels=*,
reservationsContinueLooking = true
preemptionDisabled = true
defaultAppPriorityPerQueue = 0
priority = 0
maxLifetime = -1 seconds
defaultLifetime = -1 seconds
2022-12-14 12:51:17,357 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>
2022-12-14 12:51:17,357 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2022-12-14 12:51:17,359 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2022-12-14 12:51:17,359 INFO org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false
2022-12-14 12:51:17,360 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAllocation=<<memory:4096, vCores:2>>, asynchronousScheduling=false, asyncScheduleInterval=5ms
2022-12-14 12:51:17,363 INFO org.apache.hadoop.conf.Configuration: dynamic-resources.xml not found
2022-12-14 12:51:17,365 INFO org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].
2022-12-14 12:51:17,365 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.
2022-12-14 12:51:17,368 INFO org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. 
2022-12-14 12:51:17,572 INFO org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl: Timeline service address: adh-test-00-mnode2.ru-central1.internal:8188
2022-12-14 12:51:17,842 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-14 12:51:17,842 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-14 12:51:17,842 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-14 12:51:17,842 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-14 12:51:17,842 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-14 12:51:17,842 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-14 12:51:17,842 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-14 12:51:17,842 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-14 12:51:17,842 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-14 12:51:17,842 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-14 12:51:17,889 INFO org.eclipse.jetty.util.log: Logging initialized @1921ms
2022-12-14 12:51:17,937 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Using webapps at: file:/usr/lib/hadoop-yarn/webapps/ui2
2022-12-14 12:51:17,990 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2022-12-14 12:51:17,994 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined
2022-12-14 12:51:17,998 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2022-12-14 12:51:18,003 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster
2022-12-14 12:51:18,003 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs
2022-12-14 12:51:18,003 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static
2022-12-14 12:51:18,004 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster
2022-12-14 12:51:18,004 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2022-12-14 12:51:18,004 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2022-12-14 12:51:18,006 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /cluster/*
2022-12-14 12:51:18,006 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /ws/*
2022-12-14 12:51:18,006 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /app/*
2022-12-14 12:51:18,447 INFO org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules
2022-12-14 12:51:18,455 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 8088
2022-12-14 12:51:18,457 INFO org.eclipse.jetty.server.Server: jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2022-12-14 12:51:18,490 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2022-12-14 12:51:18,501 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2022-12-14 12:51:18,505 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2022-12-14 12:51:18,505 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2022-12-14 12:51:18,506 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@61526469{/logs,file:///var/log/hadoop-yarn/,AVAILABLE}
2022-12-14 12:51:18,507 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c351808{/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.1.2.jar!/webapps/static,AVAILABLE}
2022-12-14 12:51:19,460 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@16c587de{/,file:///tmp/jetty-0.0.0.0-8088-cluster-_-any-7341471459559095666.dir/webapp/,AVAILABLE}{/cluster}
2022-12-14 12:51:19,498 INFO org.eclipse.jetty.webapp.StandardDescriptorProcessor: NO JSP Support for /ui2, did not find org.eclipse.jetty.jsp.JettyJspServlet
2022-12-14 12:51:19,506 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@3b1ed14b{/ui2,file:///usr/lib/hadoop-yarn/webapps/ui2/,AVAILABLE}
2022-12-14 12:51:19,513 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@13741d5a{HTTP/1.1,[http/1.1]}{0.0.0.0:8088}
2022-12-14 12:51:19,514 INFO org.eclipse.jetty.server.Server: Started @3548ms
2022-12-14 12:51:19,514 INFO org.apache.hadoop.yarn.webapp.WebApps: Web app cluster started at 8088
2022-12-14 12:51:19,683 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 100 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2022-12-14 12:51:19,695 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8033
2022-12-14 12:51:19,729 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server
2022-12-14 12:51:19,729 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2022-12-14 12:51:19,729 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8033: starting
2022-12-14 12:51:19,732 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning to active state
2022-12-14 12:51:19,845 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Using state database at /srv/hadoop-yarn/leveldb-state-store/yarn-rm-state for recovery
2022-12-14 12:51:19,865 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Creating state database at /srv/hadoop-yarn/leveldb-state-store/yarn-rm-state
2022-12-14 12:51:19,886 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Recovery started
2022-12-14 12:51:19,889 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Loaded RM state version info 1.1
2022-12-14 12:51:19,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 0 RM delegation token master keys
2022-12-14 12:51:19,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 0 RM delegation tokens
2022-12-14 12:51:19,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 0 applications and 0 application attempts
2022-12-14 12:51:19,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 0 reservations
2022-12-14 12:51:19,896 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: recovering RMDelegationTokenSecretManager.
2022-12-14 12:51:19,896 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager: Recovering 0 applications
2022-12-14 12:51:19,896 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager: Successfully recovered 0 out of 0 applications
2022-12-14 12:51:19,896 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Recovery ended
2022-12-14 12:51:19,906 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2022-12-14 12:51:19,907 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2022-12-14 12:51:19,907 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2022-12-14 12:51:19,907 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2022-12-14 12:51:19,907 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 1
2022-12-14 12:51:19,907 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2022-12-14 12:51:19,908 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2022-12-14 12:51:19,909 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2022-12-14 12:51:19,909 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 2
2022-12-14 12:51:19,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2022-12-14 12:51:20,596 INFO org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore: Finished write mirror at:hdfs:/system/yarn/node-labels/nodelabel.mirror
2022-12-14 12:51:20,596 INFO org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore: Finished create editlog file at:hdfs:/system/yarn/node-labels/nodelabel.editlog
2022-12-14 12:51:20,597 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler
2022-12-14 12:51:20,613 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2022-12-14 12:51:20,614 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8031
2022-12-14 12:51:20,617 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server
2022-12-14 12:51:20,617 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2022-12-14 12:51:20,617 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8031: starting
2022-12-14 12:51:20,630 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2022-12-14 12:51:20,641 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2022-12-14 12:51:20,648 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8030
2022-12-14 12:51:20,656 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server
2022-12-14 12:51:20,658 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2022-12-14 12:51:20,661 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8030: starting
2022-12-14 12:51:20,811 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2022-12-14 12:51:20,812 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8032
2022-12-14 12:51:20,815 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server
2022-12-14 12:51:20,815 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2022-12-14 12:51:20,815 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8032: starting
2022-12-14 12:51:20,824 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioned to active state
2022-12-14 12:51:56,007 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node adh-test-00-mnode2.ru-central1.internal(cmPort: 8041 httpPort: 8042) registered with capability: <memory:7057, vCores:2>, assigned nodeId adh-test-00-mnode2.ru-central1.internal:8041
2022-12-14 12:51:56,013 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: adh-test-00-mnode2.ru-central1.internal:8041 Node Transitioned from NEW to RUNNING
2022-12-14 12:51:56,042 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node adh-test-00-mnode2.ru-central1.internal:8041 clusterResource: <memory:7057, vCores:2>
2022-12-14 12:51:56,084 INFO org.apache.hadoop.yarn.server.resourcemanager.RMActiveServiceContext: Scheduler recovery is done. Start allocating new containers.
2022-12-14 12:51:56,232 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node adh-test-00-mnode3.ru-central1.internal(cmPort: 8041 httpPort: 8042) registered with capability: <memory:7057, vCores:2>, assigned nodeId adh-test-00-mnode3.ru-central1.internal:8041
2022-12-14 12:51:56,234 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: adh-test-00-mnode3.ru-central1.internal:8041 Node Transitioned from NEW to RUNNING
2022-12-14 12:51:56,236 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node adh-test-00-mnode3.ru-central1.internal:8041 clusterResource: <memory:14114, vCores:4>
2022-12-14 12:51:56,738 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node adh-test-00-mnode1.ru-central1.internal(cmPort: 8041 httpPort: 8042) registered with capability: <memory:7057, vCores:2>, assigned nodeId adh-test-00-mnode1.ru-central1.internal:8041
2022-12-14 12:51:56,738 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: adh-test-00-mnode1.ru-central1.internal:8041 Node Transitioned from NEW to RUNNING
2022-12-14 12:51:56,742 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node adh-test-00-mnode1.ru-central1.internal:8041 clusterResource: <memory:21171, vCores:6>
2022-12-14 12:55:04,429 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 1
2022-12-14 12:55:05,242 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1671022279734_0001' is submitted without priority hence considering default queue/cluster priority: 0
2022-12-14 12:55:05,242 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1671022279734_0001
2022-12-14 12:55:05,267 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 1 submitted by user yarn
2022-12-14 12:55:05,268 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1671022279734_0001
2022-12-14 12:55:05,270 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	IP=10.130.0.4	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1671022279734_0001	CALLERCONTEXT=CLI	QUEUENAME=default
2022-12-14 12:55:05,276 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1671022279734_0001
2022-12-14 12:55:05,276 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1671022279734_0001 State change from NEW to NEW_SAVING on event = START
2022-12-14 12:55:05,330 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1671022279734_0001 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2022-12-14 12:55:05,337 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1671022279734_0001 user: yarn leaf-queue of parent: root #applications: 1
2022-12-14 12:55:05,338 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1671022279734_0001 from user: yarn, in queue: default
2022-12-14 12:55:05,352 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1671022279734_0001 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2022-12-14 12:55:05,391 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1671022279734_0001_000001
2022-12-14 12:55:05,393 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1671022279734_0001_000001 State change from NEW to SUBMITTED on event = START
2022-12-14 12:55:05,423 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1671022279734_0001 from user: yarn activated in queue: default
2022-12-14 12:55:05,423 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1671022279734_0001 user: yarn, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2022-12-14 12:55:05,423 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1671022279734_0001_000001 to scheduler from user yarn in queue default
2022-12-14 12:55:05,434 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1671022279734_0001_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2022-12-14 12:55:05,633 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0001_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2022-12-14 12:55:05,639 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000001 Container Transitioned from NEW to ALLOCATED
2022-12-14 12:55:05,640 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0001_01_000001 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2022-12-14 12:55:05,640 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000001	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2022-12-14 12:55:05,675 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_1671022279734_0001_01_000001
2022-12-14 12:55:05,693 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 12:55:05,695 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1671022279734_0001_000001
2022-12-14 12:55:05,695 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1671022279734_0001 AttemptId: appattempt_1671022279734_0001_000001 MasterContainer: Container: [ContainerId: container_1671022279734_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ]
2022-12-14 12:55:05,698 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.0967361 absoluteUsedCapacity=0.0967361 used=<memory:2048, vCores:1> cluster=<memory:21171, vCores:6>
2022-12-14 12:55:05,698 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 12:55:05,708 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1671022279734_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2022-12-14 12:55:05,714 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1671022279734_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2022-12-14 12:55:05,719 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1671022279734_0001_000001
2022-12-14 12:55:05,797 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1671022279734_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1671022279734_0001_000001
2022-12-14 12:55:05,798 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1671022279734_0001_000001
2022-12-14 12:55:05,943 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1671022279734_0001_000001
2022-12-14 12:55:06,249 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1671022279734_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1671022279734_0001_000001
2022-12-14 12:55:06,249 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1671022279734_0001_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2022-12-14 12:55:06,635 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 12:55:10,642 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1671022279734_0001_000001 (auth:SIMPLE)
2022-12-14 12:55:10,660 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1671022279734_0001_000001
2022-12-14 12:55:10,661 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	IP=10.130.0.5	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1671022279734_0001	APPATTEMPTID=appattempt_1671022279734_0001_000001
2022-12-14 12:55:10,661 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1671022279734_0001_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2022-12-14 12:55:10,661 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1671022279734_0001 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2022-12-14 12:55:11,959 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0001_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 12:55:11,960 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000002 Container Transitioned from NEW to ALLOCATED
2022-12-14 12:55:11,960 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0001_01_000002 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2022-12-14 12:55:11,960 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000002	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:11,960 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.14510415 absoluteUsedCapacity=0.14510415 used=<memory:3072, vCores:2> cluster=<memory:21171, vCores:6>
2022-12-14 12:55:11,960 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 12:55:11,960 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0001_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 12:55:11,961 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000003 Container Transitioned from NEW to ALLOCATED
2022-12-14 12:55:11,961 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0001_01_000003 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 2 containers, <memory:2048, vCores:2> used and <memory:5009, vCores:0> available after allocation
2022-12-14 12:55:11,961 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000003	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:11,961 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.1934722 absoluteUsedCapacity=0.1934722 used=<memory:4096, vCores:3> cluster=<memory:21171, vCores:6>
2022-12-14 12:55:11,961 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 12:55:11,961 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0001_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 12:55:11,962 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000004 Container Transitioned from NEW to ALLOCATED
2022-12-14 12:55:11,962 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0001_01_000004 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 3 containers, <memory:3072, vCores:3> used and <memory:3985, vCores:-1> available after allocation
2022-12-14 12:55:11,962 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000004	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:11,962 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.24184026 absoluteUsedCapacity=0.24184026 used=<memory:5120, vCores:4> cluster=<memory:21171, vCores:6>
2022-12-14 12:55:11,962 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 12:55:11,962 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0001_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 12:55:11,963 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000005 Container Transitioned from NEW to ALLOCATED
2022-12-14 12:55:11,963 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0001_01_000005 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 4 containers, <memory:4096, vCores:4> used and <memory:2961, vCores:-2> available after allocation
2022-12-14 12:55:11,963 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000005	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:11,963 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.2902083 absoluteUsedCapacity=0.2902083 used=<memory:6144, vCores:5> cluster=<memory:21171, vCores:6>
2022-12-14 12:55:11,963 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 12:55:11,964 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0001_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 12:55:11,964 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000006 Container Transitioned from NEW to ALLOCATED
2022-12-14 12:55:11,964 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0001_01_000006 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 5 containers, <memory:5120, vCores:5> used and <memory:1937, vCores:-3> available after allocation
2022-12-14 12:55:11,964 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000006	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:11,964 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.33857635 absoluteUsedCapacity=0.33857635 used=<memory:7168, vCores:6> cluster=<memory:21171, vCores:6>
2022-12-14 12:55:11,964 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 12:55:11,965 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0001_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 12:55:11,965 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000007 Container Transitioned from NEW to ALLOCATED
2022-12-14 12:55:11,965 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0001_01_000007 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 6 containers, <memory:6144, vCores:6> used and <memory:913, vCores:-4> available after allocation
2022-12-14 12:55:11,965 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000007	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:11,965 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.3869444 absoluteUsedCapacity=0.3869444 used=<memory:8192, vCores:7> cluster=<memory:21171, vCores:6>
2022-12-14 12:55:11,965 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 12:55:12,210 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0001_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 12:55:12,211 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000008 Container Transitioned from NEW to ALLOCATED
2022-12-14 12:55:12,211 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0001_01_000008 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2022-12-14 12:55:12,211 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000008	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:12,211 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.43531245 absoluteUsedCapacity=0.43531245 used=<memory:9216, vCores:8> cluster=<memory:21171, vCores:6>
2022-12-14 12:55:12,211 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 12:55:12,211 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0001_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 12:55:12,212 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000009 Container Transitioned from NEW to ALLOCATED
2022-12-14 12:55:12,212 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0001_01_000009 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 2 containers, <memory:2048, vCores:2> used and <memory:5009, vCores:0> available after allocation
2022-12-14 12:55:12,212 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000009	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:12,212 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.48368052 absoluteUsedCapacity=0.48368052 used=<memory:10240, vCores:9> cluster=<memory:21171, vCores:6>
2022-12-14 12:55:12,212 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 12:55:12,212 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0001_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 12:55:12,212 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000010 Container Transitioned from NEW to ALLOCATED
2022-12-14 12:55:12,213 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0001_01_000010 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 3 containers, <memory:3072, vCores:3> used and <memory:3985, vCores:-1> available after allocation
2022-12-14 12:55:12,213 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000010	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:12,213 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.5320486 absoluteUsedCapacity=0.5320486 used=<memory:11264, vCores:10> cluster=<memory:21171, vCores:6>
2022-12-14 12:55:12,213 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 12:55:12,213 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0001_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 12:55:12,213 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000011 Container Transitioned from NEW to ALLOCATED
2022-12-14 12:55:12,213 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0001_01_000011 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 4 containers, <memory:4096, vCores:4> used and <memory:2961, vCores:-2> available after allocation
2022-12-14 12:55:12,213 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000011	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:12,213 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.5804166 absoluteUsedCapacity=0.5804166 used=<memory:12288, vCores:11> cluster=<memory:21171, vCores:6>
2022-12-14 12:55:12,213 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 12:55:12,214 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0001_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 12:55:12,214 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000012 Container Transitioned from NEW to ALLOCATED
2022-12-14 12:55:12,214 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0001_01_000012 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 5 containers, <memory:5120, vCores:5> used and <memory:1937, vCores:-3> available after allocation
2022-12-14 12:55:12,214 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000012	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:12,214 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.62878466 absoluteUsedCapacity=0.62878466 used=<memory:13312, vCores:12> cluster=<memory:21171, vCores:6>
2022-12-14 12:55:12,214 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 12:55:12,214 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0001_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 12:55:12,215 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000013 Container Transitioned from NEW to ALLOCATED
2022-12-14 12:55:12,215 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0001_01_000013 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 6 containers, <memory:6144, vCores:6> used and <memory:913, vCores:-4> available after allocation
2022-12-14 12:55:12,215 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000013	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:12,215 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.6771527 absoluteUsedCapacity=0.6771527 used=<memory:14336, vCores:13> cluster=<memory:21171, vCores:6>
2022-12-14 12:55:12,215 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 12:55:12,651 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0001_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 12:55:12,652 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000014 Container Transitioned from NEW to ALLOCATED
2022-12-14 12:55:12,652 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0001_01_000014 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 2 containers, <memory:3072, vCores:2> used and <memory:3985, vCores:0> available after allocation
2022-12-14 12:55:12,652 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000014	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:12,652 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.72552073 absoluteUsedCapacity=0.72552073 used=<memory:15360, vCores:14> cluster=<memory:21171, vCores:6>
2022-12-14 12:55:12,652 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 12:55:12,653 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0001_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 12:55:12,653 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000015 Container Transitioned from NEW to ALLOCATED
2022-12-14 12:55:12,653 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0001_01_000015 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 3 containers, <memory:4096, vCores:3> used and <memory:2961, vCores:-1> available after allocation
2022-12-14 12:55:12,653 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000015	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:12,653 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.7738888 absoluteUsedCapacity=0.7738888 used=<memory:16384, vCores:15> cluster=<memory:21171, vCores:6>
2022-12-14 12:55:12,653 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 12:55:12,654 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0001_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 12:55:12,654 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000016 Container Transitioned from NEW to ALLOCATED
2022-12-14 12:55:12,654 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0001_01_000016 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 4 containers, <memory:5120, vCores:4> used and <memory:1937, vCores:-2> available after allocation
2022-12-14 12:55:12,654 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000016	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:12,654 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.82225686 absoluteUsedCapacity=0.82225686 used=<memory:17408, vCores:16> cluster=<memory:21171, vCores:6>
2022-12-14 12:55:12,654 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 12:55:12,654 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0001_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 12:55:12,655 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000017 Container Transitioned from NEW to ALLOCATED
2022-12-14 12:55:12,655 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0001_01_000017 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 5 containers, <memory:6144, vCores:5> used and <memory:913, vCores:-3> available after allocation
2022-12-14 12:55:12,655 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000017	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:12,656 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.8706249 absoluteUsedCapacity=0.8706249 used=<memory:18432, vCores:17> cluster=<memory:21171, vCores:6>
2022-12-14 12:55:12,656 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 12:55:12,738 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_1671022279734_0001_01_000002
2022-12-14 12:55:12,739 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 12:55:12,741 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 12:55:12,744 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000004 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 12:55:12,746 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000005 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 12:55:12,748 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000006 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 12:55:12,749 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000007 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 12:55:12,757 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode1.ru-central1.internal:8041 for container : container_1671022279734_0001_01_000008
2022-12-14 12:55:12,758 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000008 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 12:55:12,760 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000009 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 12:55:12,761 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000010 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 12:55:12,762 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000011 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 12:55:12,763 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000012 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 12:55:12,764 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000013 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 12:55:12,765 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_1671022279734_0001_01_000014
2022-12-14 12:55:12,766 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000014 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 12:55:12,768 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000015 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 12:55:12,769 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000016 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 12:55:12,770 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000017 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 12:55:13,660 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000014 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 12:55:13,661 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000015 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 12:55:13,662 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000016 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 12:55:13,662 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000017 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 12:55:13,812 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1671022279734_0001
2022-12-14 12:55:13,974 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000002 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 12:55:13,975 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000003 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 12:55:13,975 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000004 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 12:55:13,975 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000005 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 12:55:13,976 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000006 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 12:55:13,976 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000007 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 12:55:14,229 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000008 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 12:55:14,230 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000009 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 12:55:14,231 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000010 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 12:55:14,231 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000011 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 12:55:14,231 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000012 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 12:55:14,232 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000013 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 12:55:19,066 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000014 Container Transitioned from RUNNING to COMPLETED
2022-12-14 12:55:19,066 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000014	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:19,307 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000017 Container Transitioned from RUNNING to COMPLETED
2022-12-14 12:55:19,307 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000017	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:19,348 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000015 Container Transitioned from RUNNING to COMPLETED
2022-12-14 12:55:19,348 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000015	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:20,388 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000016 Container Transitioned from RUNNING to COMPLETED
2022-12-14 12:55:20,388 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000016	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:20,390 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0001_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2022-12-14 12:55:20,391 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000018 Container Transitioned from NEW to ALLOCATED
2022-12-14 12:55:20,391 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0001_01_000018 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 2 containers, <memory:3072, vCores:2> used and <memory:3985, vCores:0> available after allocation
2022-12-14 12:55:20,391 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000018	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:20,391 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.72552073 absoluteUsedCapacity=0.72552073 used=<memory:15360, vCores:14> cluster=<memory:21171, vCores:6>
2022-12-14 12:55:20,391 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 12:55:21,136 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000018 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 12:55:21,407 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000018 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 12:55:22,166 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1671022279734_0001
2022-12-14 12:55:23,342 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000002 Container Transitioned from RUNNING to COMPLETED
2022-12-14 12:55:23,342 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000002	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:23,389 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000005 Container Transitioned from RUNNING to COMPLETED
2022-12-14 12:55:23,389 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000005	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:23,444 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000007 Container Transitioned from RUNNING to COMPLETED
2022-12-14 12:55:23,444 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000007	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:23,484 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000004 Container Transitioned from RUNNING to COMPLETED
2022-12-14 12:55:23,484 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000004	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:23,582 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000006 Container Transitioned from RUNNING to COMPLETED
2022-12-14 12:55:23,582 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000006	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:23,674 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000003 Container Transitioned from RUNNING to COMPLETED
2022-12-14 12:55:23,675 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000003	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:24,060 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000013 Container Transitioned from RUNNING to COMPLETED
2022-12-14 12:55:24,060 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000013	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:24,062 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000011 Container Transitioned from RUNNING to COMPLETED
2022-12-14 12:55:24,062 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000011	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:24,329 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000009 Container Transitioned from RUNNING to COMPLETED
2022-12-14 12:55:24,329 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000009	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:24,355 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000010 Container Transitioned from RUNNING to COMPLETED
2022-12-14 12:55:24,355 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000010	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:24,388 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000012 Container Transitioned from RUNNING to COMPLETED
2022-12-14 12:55:24,388 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000012	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:24,412 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000008 Container Transitioned from RUNNING to COMPLETED
2022-12-14 12:55:24,412 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000008	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:24,544 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000018 Container Transitioned from RUNNING to COMPLETED
2022-12-14 12:55:24,544 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000018	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 12:55:24,606 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1671022279734_0001_000001 with final state: FINISHING, and exit status: -1000
2022-12-14 12:55:24,607 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1671022279734_0001_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2022-12-14 12:55:24,607 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1671022279734_0001 with final state: FINISHING
2022-12-14 12:55:24,608 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1671022279734_0001 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2022-12-14 12:55:24,608 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1671022279734_0001
2022-12-14 12:55:24,608 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1671022279734_0001_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2022-12-14 12:55:24,608 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1671022279734_0001 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2022-12-14 12:55:25,608 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1671022279734_0001 unregistered successfully. 
2022-12-14 12:55:30,984 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0001_01_000001 Container Transitioned from RUNNING to COMPLETED
2022-12-14 12:55:30,984 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1671022279734_0001_000001
2022-12-14 12:55:30,985 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0001	CONTAINERID=container_1671022279734_0001_01_000001	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2022-12-14 12:55:30,986 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1671022279734_0001_000001
2022-12-14 12:55:30,988 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1671022279734_0001_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2022-12-14 12:55:30,991 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1671022279734_0001 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2022-12-14 12:55:30,991 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1671022279734_0001_000001 is done. finalState=FINISHED
2022-12-14 12:55:30,991 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1671022279734_0001 requests cleared
2022-12-14 12:55:30,992 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1671022279734_0001_000001
2022-12-14 12:55:30,993 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1671022279734_0001
2022-12-14 12:55:30,995 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1671022279734_0001 user: yarn queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2022-12-14 12:55:30,995 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1671022279734_0001 user: yarn leaf-queue of parent: root #applications: 0
2022-12-14 12:55:30,995 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1671022279734_0001,name=QuasiMonteCarlo,user=yarn,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1671022279734_0001/,appMasterHost=adh-test-00-mnode2.ru-central1.internal,submitTime=1671022505181,startTime=1671022505266,finishTime=1671022524607,finalStatus=SUCCEEDED,memorySeconds=229187,vcoreSeconds=190,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=229187 MB-seconds\, 190 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2022-12-14 13:01:17,274 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Release request cache is cleaned up
2022-12-14 13:14:35,386 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 2
2022-12-14 13:14:36,043 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1671022279734_0002' is submitted without priority hence considering default queue/cluster priority: 0
2022-12-14 13:14:36,043 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1671022279734_0002
2022-12-14 13:14:36,044 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 2 submitted by user hdfs
2022-12-14 13:14:36,044 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.4	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1671022279734_0002	CALLERCONTEXT=CLI	QUEUENAME=default
2022-12-14 13:14:36,044 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1671022279734_0002
2022-12-14 13:14:36,045 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1671022279734_0002
2022-12-14 13:14:36,045 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1671022279734_0002 State change from NEW to NEW_SAVING on event = START
2022-12-14 13:14:36,046 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1671022279734_0002 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2022-12-14 13:14:36,049 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1671022279734_0002 user: hdfs leaf-queue of parent: root #applications: 1
2022-12-14 13:14:36,049 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1671022279734_0002 from user: hdfs, in queue: default
2022-12-14 13:14:36,049 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1671022279734_0002 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2022-12-14 13:14:36,049 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1671022279734_0002_000001
2022-12-14 13:14:36,049 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1671022279734_0002_000001 State change from NEW to SUBMITTED on event = START
2022-12-14 13:14:36,050 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1671022279734_0002 from user: hdfs activated in queue: default
2022-12-14 13:14:36,050 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1671022279734_0002 user: hdfs, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2022-12-14 13:14:36,050 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1671022279734_0002_000001 to scheduler from user hdfs in queue default
2022-12-14 13:14:36,053 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1671022279734_0002_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2022-12-14 13:14:36,055 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2022-12-14 13:14:36,057 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000001 Container Transitioned from NEW to ALLOCATED
2022-12-14 13:14:36,058 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0002_01_000001 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2022-12-14 13:14:36,058 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000001	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2022-12-14 13:14:36,059 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_1671022279734_0002_01_000001
2022-12-14 13:14:36,063 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 13:14:36,063 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1671022279734_0002_000001
2022-12-14 13:14:36,063 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1671022279734_0002 AttemptId: appattempt_1671022279734_0002_000001 MasterContainer: Container: [ContainerId: container_1671022279734_0002_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ]
2022-12-14 13:14:36,064 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.0967361 absoluteUsedCapacity=0.0967361 used=<memory:2048, vCores:1> cluster=<memory:21171, vCores:6>
2022-12-14 13:14:36,064 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 13:14:36,064 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1671022279734_0002_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2022-12-14 13:14:36,066 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1671022279734_0002_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2022-12-14 13:14:36,067 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1671022279734_0002_000001
2022-12-14 13:14:36,070 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1671022279734_0002_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1671022279734_0002_000001
2022-12-14 13:14:36,071 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1671022279734_0002_000001
2022-12-14 13:14:36,071 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1671022279734_0002_000001
2022-12-14 13:14:36,114 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1671022279734_0002_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1671022279734_0002_000001
2022-12-14 13:14:36,114 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1671022279734_0002_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2022-12-14 13:14:37,058 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000001 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 13:14:39,654 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1671022279734_0002_000001 (auth:SIMPLE)
2022-12-14 13:14:39,658 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1671022279734_0002_000001
2022-12-14 13:14:39,659 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.5	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1671022279734_0002	APPATTEMPTID=appattempt_1671022279734_0002_000001
2022-12-14 13:14:39,659 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1671022279734_0002_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2022-12-14 13:14:39,659 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1671022279734_0002 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2022-12-14 13:14:40,870 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 13:14:40,871 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000002 Container Transitioned from NEW to ALLOCATED
2022-12-14 13:14:40,871 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0002_01_000002 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2022-12-14 13:14:40,871 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000002	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:40,871 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.14510415 absoluteUsedCapacity=0.14510415 used=<memory:3072, vCores:2> cluster=<memory:21171, vCores:6>
2022-12-14 13:14:40,871 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 13:14:40,871 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 13:14:40,872 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000003 Container Transitioned from NEW to ALLOCATED
2022-12-14 13:14:40,872 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0002_01_000003 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 2 containers, <memory:2048, vCores:2> used and <memory:5009, vCores:0> available after allocation
2022-12-14 13:14:40,872 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000003	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:40,872 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.1934722 absoluteUsedCapacity=0.1934722 used=<memory:4096, vCores:3> cluster=<memory:21171, vCores:6>
2022-12-14 13:14:40,872 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 13:14:40,872 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 13:14:40,872 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000004 Container Transitioned from NEW to ALLOCATED
2022-12-14 13:14:40,872 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0002_01_000004 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 3 containers, <memory:3072, vCores:3> used and <memory:3985, vCores:-1> available after allocation
2022-12-14 13:14:40,872 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000004	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:40,872 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.24184026 absoluteUsedCapacity=0.24184026 used=<memory:5120, vCores:4> cluster=<memory:21171, vCores:6>
2022-12-14 13:14:40,872 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 13:14:40,872 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 13:14:40,873 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000005 Container Transitioned from NEW to ALLOCATED
2022-12-14 13:14:40,873 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0002_01_000005 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 4 containers, <memory:4096, vCores:4> used and <memory:2961, vCores:-2> available after allocation
2022-12-14 13:14:40,873 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000005	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:40,873 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.2902083 absoluteUsedCapacity=0.2902083 used=<memory:6144, vCores:5> cluster=<memory:21171, vCores:6>
2022-12-14 13:14:40,873 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 13:14:40,873 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 13:14:40,873 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000006 Container Transitioned from NEW to ALLOCATED
2022-12-14 13:14:40,873 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0002_01_000006 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 5 containers, <memory:5120, vCores:5> used and <memory:1937, vCores:-3> available after allocation
2022-12-14 13:14:40,873 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000006	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:40,873 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.33857635 absoluteUsedCapacity=0.33857635 used=<memory:7168, vCores:6> cluster=<memory:21171, vCores:6>
2022-12-14 13:14:40,873 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 13:14:40,873 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 13:14:40,874 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000007 Container Transitioned from NEW to ALLOCATED
2022-12-14 13:14:40,874 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0002_01_000007 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 6 containers, <memory:6144, vCores:6> used and <memory:913, vCores:-4> available after allocation
2022-12-14 13:14:40,874 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000007	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:40,874 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.3869444 absoluteUsedCapacity=0.3869444 used=<memory:8192, vCores:7> cluster=<memory:21171, vCores:6>
2022-12-14 13:14:40,874 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 13:14:40,960 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 13:14:40,960 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000008 Container Transitioned from NEW to ALLOCATED
2022-12-14 13:14:40,960 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0002_01_000008 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2022-12-14 13:14:40,960 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000008	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:40,960 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.43531245 absoluteUsedCapacity=0.43531245 used=<memory:9216, vCores:8> cluster=<memory:21171, vCores:6>
2022-12-14 13:14:40,961 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 13:14:40,961 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 13:14:40,961 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000009 Container Transitioned from NEW to ALLOCATED
2022-12-14 13:14:40,961 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0002_01_000009 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 2 containers, <memory:2048, vCores:2> used and <memory:5009, vCores:0> available after allocation
2022-12-14 13:14:40,961 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000009	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:40,961 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.48368052 absoluteUsedCapacity=0.48368052 used=<memory:10240, vCores:9> cluster=<memory:21171, vCores:6>
2022-12-14 13:14:40,961 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 13:14:40,961 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 13:14:40,961 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000010 Container Transitioned from NEW to ALLOCATED
2022-12-14 13:14:40,961 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0002_01_000010 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 3 containers, <memory:3072, vCores:3> used and <memory:3985, vCores:-1> available after allocation
2022-12-14 13:14:40,962 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000010	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:40,962 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.5320486 absoluteUsedCapacity=0.5320486 used=<memory:11264, vCores:10> cluster=<memory:21171, vCores:6>
2022-12-14 13:14:40,962 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 13:14:40,962 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 13:14:40,962 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000011 Container Transitioned from NEW to ALLOCATED
2022-12-14 13:14:40,962 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0002_01_000011 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 4 containers, <memory:4096, vCores:4> used and <memory:2961, vCores:-2> available after allocation
2022-12-14 13:14:40,962 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000011	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:40,962 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.5804166 absoluteUsedCapacity=0.5804166 used=<memory:12288, vCores:11> cluster=<memory:21171, vCores:6>
2022-12-14 13:14:40,962 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 13:14:40,962 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 13:14:40,963 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000012 Container Transitioned from NEW to ALLOCATED
2022-12-14 13:14:40,963 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0002_01_000012 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 5 containers, <memory:5120, vCores:5> used and <memory:1937, vCores:-3> available after allocation
2022-12-14 13:14:40,963 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000012	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:40,963 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.62878466 absoluteUsedCapacity=0.62878466 used=<memory:13312, vCores:12> cluster=<memory:21171, vCores:6>
2022-12-14 13:14:40,963 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 13:14:40,963 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 13:14:40,963 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000013 Container Transitioned from NEW to ALLOCATED
2022-12-14 13:14:40,963 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0002_01_000013 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 6 containers, <memory:6144, vCores:6> used and <memory:913, vCores:-4> available after allocation
2022-12-14 13:14:40,963 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000013	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:40,963 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.6771527 absoluteUsedCapacity=0.6771527 used=<memory:14336, vCores:13> cluster=<memory:21171, vCores:6>
2022-12-14 13:14:40,963 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 13:14:41,065 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 13:14:41,065 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000014 Container Transitioned from NEW to ALLOCATED
2022-12-14 13:14:41,066 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0002_01_000014 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 2 containers, <memory:3072, vCores:2> used and <memory:3985, vCores:0> available after allocation
2022-12-14 13:14:41,066 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000014	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:41,066 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.72552073 absoluteUsedCapacity=0.72552073 used=<memory:15360, vCores:14> cluster=<memory:21171, vCores:6>
2022-12-14 13:14:41,066 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 13:14:41,066 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 13:14:41,066 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000015 Container Transitioned from NEW to ALLOCATED
2022-12-14 13:14:41,066 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0002_01_000015 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 3 containers, <memory:4096, vCores:3> used and <memory:2961, vCores:-1> available after allocation
2022-12-14 13:14:41,066 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000015	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:41,066 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.7738888 absoluteUsedCapacity=0.7738888 used=<memory:16384, vCores:15> cluster=<memory:21171, vCores:6>
2022-12-14 13:14:41,066 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 13:14:41,067 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 13:14:41,067 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000016 Container Transitioned from NEW to ALLOCATED
2022-12-14 13:14:41,067 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0002_01_000016 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 4 containers, <memory:5120, vCores:4> used and <memory:1937, vCores:-2> available after allocation
2022-12-14 13:14:41,067 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000016	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:41,067 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.82225686 absoluteUsedCapacity=0.82225686 used=<memory:17408, vCores:16> cluster=<memory:21171, vCores:6>
2022-12-14 13:14:41,067 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 13:14:41,067 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 13:14:41,067 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000017 Container Transitioned from NEW to ALLOCATED
2022-12-14 13:14:41,068 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0002_01_000017 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 5 containers, <memory:6144, vCores:5> used and <memory:913, vCores:-3> available after allocation
2022-12-14 13:14:41,068 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000017	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:41,068 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.8706249 absoluteUsedCapacity=0.8706249 used=<memory:18432, vCores:17> cluster=<memory:21171, vCores:6>
2022-12-14 13:14:41,068 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 13:14:41,720 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_1671022279734_0002_01_000002
2022-12-14 13:14:41,721 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 13:14:41,723 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 13:14:41,727 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000004 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 13:14:41,728 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000005 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 13:14:41,730 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000006 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 13:14:41,731 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000007 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 13:14:41,731 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode1.ru-central1.internal:8041 for container : container_1671022279734_0002_01_000008
2022-12-14 13:14:41,732 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000008 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 13:14:41,733 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000009 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 13:14:41,734 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000010 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 13:14:41,735 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000011 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 13:14:41,736 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000012 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 13:14:41,737 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000013 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 13:14:41,738 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_1671022279734_0002_01_000014
2022-12-14 13:14:41,739 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000014 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 13:14:41,740 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000015 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 13:14:41,741 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000016 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 13:14:41,743 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000017 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 13:14:42,917 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000002 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 13:14:42,918 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000003 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 13:14:42,919 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000004 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 13:14:42,919 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000005 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 13:14:42,919 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000006 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 13:14:42,920 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000007 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 13:14:42,985 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000008 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 13:14:42,986 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000009 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 13:14:42,986 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000010 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 13:14:42,987 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000011 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 13:14:42,987 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000012 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 13:14:42,987 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000013 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 13:14:43,108 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000014 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 13:14:43,109 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000015 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 13:14:43,109 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000016 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 13:14:43,109 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000017 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 13:14:47,834 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000015 Container Transitioned from RUNNING to COMPLETED
2022-12-14 13:14:47,834 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000015	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:47,835 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 13:14:47,835 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000018 Container Transitioned from NEW to ALLOCATED
2022-12-14 13:14:47,835 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0002_01_000018 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 5 containers, <memory:6144, vCores:5> used and <memory:913, vCores:-3> available after allocation
2022-12-14 13:14:47,835 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000018	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:47,836 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.8706249 absoluteUsedCapacity=0.8706249 used=<memory:18432, vCores:17> cluster=<memory:21171, vCores:6>
2022-12-14 13:14:47,836 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 13:14:47,961 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000016 Container Transitioned from RUNNING to COMPLETED
2022-12-14 13:14:47,962 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000016	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:47,962 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 13:14:47,962 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000019 Container Transitioned from NEW to ALLOCATED
2022-12-14 13:14:47,963 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0002_01_000019 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 5 containers, <memory:6144, vCores:5> used and <memory:913, vCores:-3> available after allocation
2022-12-14 13:14:47,963 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000019	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:47,963 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.8706249 absoluteUsedCapacity=0.8706249 used=<memory:18432, vCores:17> cluster=<memory:21171, vCores:6>
2022-12-14 13:14:47,963 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 13:14:48,055 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000017 Container Transitioned from RUNNING to COMPLETED
2022-12-14 13:14:48,055 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000017	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:48,056 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 13:14:48,056 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000020 Container Transitioned from NEW to ALLOCATED
2022-12-14 13:14:48,056 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0002_01_000020 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 5 containers, <memory:6144, vCores:5> used and <memory:913, vCores:-3> available after allocation
2022-12-14 13:14:48,056 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000020	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:48,057 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.8706249 absoluteUsedCapacity=0.8706249 used=<memory:18432, vCores:17> cluster=<memory:21171, vCores:6>
2022-12-14 13:14:48,057 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 13:14:48,102 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000018 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 13:14:48,103 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000019 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 13:14:48,104 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000020 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 13:14:48,104 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000014 Container Transitioned from RUNNING to COMPLETED
2022-12-14 13:14:48,104 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000014	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:48,105 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-14 13:14:48,114 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000021 Container Transitioned from NEW to ALLOCATED
2022-12-14 13:14:48,114 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0002_01_000021 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 5 containers, <memory:6144, vCores:5> used and <memory:913, vCores:-3> available after allocation
2022-12-14 13:14:48,114 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000021	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:48,114 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.8706249 absoluteUsedCapacity=0.8706249 used=<memory:18432, vCores:17> cluster=<memory:21171, vCores:6>
2022-12-14 13:14:48,114 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 13:14:49,115 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000018 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 13:14:49,116 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000019 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 13:14:49,116 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000020 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 13:14:49,193 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000021 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 13:14:50,149 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000021 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 13:14:50,244 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1671022279734_0002
2022-12-14 13:14:51,200 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000007 Container Transitioned from RUNNING to COMPLETED
2022-12-14 13:14:51,200 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000007	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:51,202 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1671022279734_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2022-12-14 13:14:51,202 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000022 Container Transitioned from NEW to ALLOCATED
2022-12-14 13:14:51,202 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_1671022279734_0002_01_000022 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 6 containers, <memory:6144, vCores:6> used and <memory:913, vCores:-4> available after allocation
2022-12-14 13:14:51,202 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000022	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:51,203 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.8706249 absoluteUsedCapacity=0.8706249 used=<memory:18432, vCores:17> cluster=<memory:21171, vCores:6>
2022-12-14 13:14:51,203 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-14 13:14:51,295 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000022 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-14 13:14:51,306 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000005 Container Transitioned from RUNNING to COMPLETED
2022-12-14 13:14:51,306 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000005	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:51,319 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000002 Container Transitioned from RUNNING to COMPLETED
2022-12-14 13:14:51,319 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000002	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:51,340 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000006 Container Transitioned from RUNNING to COMPLETED
2022-12-14 13:14:51,340 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000006	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:51,400 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000004 Container Transitioned from RUNNING to COMPLETED
2022-12-14 13:14:51,400 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000004	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:51,637 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000012 Container Transitioned from RUNNING to COMPLETED
2022-12-14 13:14:51,637 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000012	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:51,822 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000011 Container Transitioned from RUNNING to COMPLETED
2022-12-14 13:14:51,822 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000011	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:52,010 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000009 Container Transitioned from RUNNING to COMPLETED
2022-12-14 13:14:52,011 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000009	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:52,075 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000010 Container Transitioned from RUNNING to COMPLETED
2022-12-14 13:14:52,075 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000010	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:52,094 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000013 Container Transitioned from RUNNING to COMPLETED
2022-12-14 13:14:52,094 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000013	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:52,127 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000008 Container Transitioned from RUNNING to COMPLETED
2022-12-14 13:14:52,127 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000008	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:52,344 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1671022279734_0002
2022-12-14 13:14:52,408 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000022 Container Transitioned from ACQUIRED to RUNNING
2022-12-14 13:14:52,408 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000003 Container Transitioned from RUNNING to COMPLETED
2022-12-14 13:14:52,408 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000003	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:53,718 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000018 Container Transitioned from RUNNING to COMPLETED
2022-12-14 13:14:53,718 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000018	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:53,886 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000019 Container Transitioned from RUNNING to COMPLETED
2022-12-14 13:14:53,886 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000019	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:54,155 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000020 Container Transitioned from RUNNING to COMPLETED
2022-12-14 13:14:54,155 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000020	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:54,404 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000021 Container Transitioned from RUNNING to COMPLETED
2022-12-14 13:14:54,404 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000021	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:54,897 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1671022279734_0002_000001 with final state: FINISHING, and exit status: -1000
2022-12-14 13:14:54,897 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1671022279734_0002_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2022-12-14 13:14:54,897 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1671022279734_0002 with final state: FINISHING
2022-12-14 13:14:54,897 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1671022279734_0002 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2022-12-14 13:14:54,898 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1671022279734_0002
2022-12-14 13:14:54,898 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1671022279734_0002_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2022-12-14 13:14:54,898 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1671022279734_0002 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2022-12-14 13:14:54,899 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000022 Container Transitioned from RUNNING to COMPLETED
2022-12-14 13:14:54,899 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000022	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-14 13:14:55,899 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1671022279734_0002 unregistered successfully. 
2022-12-14 13:15:01,304 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1671022279734_0002_000001
2022-12-14 13:15:01,304 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1671022279734_0002_01_000001 Container Transitioned from RUNNING to COMPLETED
2022-12-14 13:15:01,304 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1671022279734_0002_000001
2022-12-14 13:15:01,304 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1671022279734_0002	CONTAINERID=container_1671022279734_0002_01_000001	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2022-12-14 13:15:01,305 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1671022279734_0002_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2022-12-14 13:15:01,306 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1671022279734_0002 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2022-12-14 13:15:01,306 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1671022279734_0002_000001 is done. finalState=FINISHED
2022-12-14 13:15:01,306 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1671022279734_0002
2022-12-14 13:15:01,306 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1671022279734_0002 requests cleared
2022-12-14 13:15:01,306 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1671022279734_0002 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2022-12-14 13:15:01,306 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1671022279734_0002 user: hdfs leaf-queue of parent: root #applications: 0
2022-12-14 13:15:01,306 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1671022279734_0002_000001
2022-12-14 13:15:01,306 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1671022279734_0002,name=QuasiMonteCarlo,user=hdfs,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1671022279734_0002/,appMasterHost=adh-test-00-mnode2.ru-central1.internal,submitTime=1671023676043,startTime=1671023676044,finishTime=1671023694897,finalStatus=SUCCEEDED,memorySeconds=241475,vcoreSeconds=200,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=241475 MB-seconds\, 200 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2022-12-14 13:51:19,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-14 13:51:19,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2022-12-14 14:51:19,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-14 14:51:19,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2022-12-14 15:51:19,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-14 15:51:19,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2022-12-14 16:51:19,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-14 16:51:19,902 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2022-12-14 17:51:19,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-14 17:51:19,898 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2022-12-14 18:51:19,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-14 18:51:19,891 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2022-12-14 19:51:19,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-14 19:51:19,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2022-12-14 20:51:19,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-14 20:51:19,890 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2022-12-14 21:51:19,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-14 21:51:19,890 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2022-12-14 22:51:19,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-14 22:51:19,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2022-12-14 23:51:19,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-14 23:51:19,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2022-12-15 00:51:19,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-15 00:51:19,891 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2022-12-15 01:51:19,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-15 01:51:19,905 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2022-12-15 02:51:19,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-15 02:51:19,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2022-12-15 03:51:19,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-15 03:51:19,894 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2022-12-15 04:51:19,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-15 04:51:19,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2022-12-15 05:51:19,889 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-15 05:51:19,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2022-12-15 06:51:19,889 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-15 06:51:19,904 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2022-12-15 07:51:19,889 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-15 07:51:19,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2022-12-15 08:51:19,889 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-15 08:51:19,905 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2022-12-15 09:51:19,889 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-15 09:51:19,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2022-12-15 10:51:19,889 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-15 10:51:19,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2022-12-15 11:51:19,889 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-15 11:51:19,906 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2022-12-15 19:01:05,592 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting ResourceManager
STARTUP_MSG:   host = adh-test-00-mnode1.ru-central1.internal/10.130.0.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.2
STARTUP_MSG:   classpath = /etc/hadoop/conf:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/stax2-api.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jetty-io-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-security-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/jetty-server-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/woodstox-core.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/zookeeper-3.4.13.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jetty-util-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/httpclient-4.5.2.jar:/usr/lib/hadoop/lib/httpcore-4.4.4.jar:/usr/lib/hadoop/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/jackson-annotations-2.7.8.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/jackson-core-2.7.8.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jackson-databind-2.7.8.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-xml-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/jetty-http-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.1.2.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-auth-3.1.2.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-common-3.1.2-tests.jar:/usr/lib/hadoop/.//hadoop-common-3.1.2.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms-3.1.2.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-nfs-3.1.2.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.2.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.1.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.1.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.1.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.1.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-2.8.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.271.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-0.8.2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//lz4-1.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//netty-buffer-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//netty-codec-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//netty-codec-http-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//netty-common-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//netty-handler-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//netty-resolver-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//netty-transport-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.1.2-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-yarn/./:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar
STARTUP_MSG:   build = git@ssh.gitlab.adsw.io:arenadata/infrastructure/code/ci/prj_adh.git -r 15525ac41ebda8bc83c285e9b6ea7fb9921fd7fd; compiled by 'jenkins' on 2022-09-08T14:33Z
STARTUP_MSG:   java = 1.8.0_352
************************************************************/
2022-12-15 19:01:05,645 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]
2022-12-15 19:01:06,107 INFO org.apache.hadoop.conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf/core-site.xml
2022-12-15 19:01:06,385 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found
2022-12-15 19:01:06,385 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.
2022-12-15 19:01:06,628 INFO org.apache.hadoop.conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf/yarn-site.xml
2022-12-15 19:01:06,688 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
2022-12-15 19:01:06,747 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
2022-12-15 19:01:06,752 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
2022-12-15 19:01:06,760 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
2022-12-15 19:01:06,873 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreFactory: Using RMStateStore implementation - class org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore
2022-12-15 19:01:06,875 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
2022-12-15 19:01:06,881 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
2022-12-15 19:01:06,881 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
2022-12-15 19:01:06,958 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher
2022-12-15 19:01:06,960 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher
2022-12-15 19:01:06,961 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher
2022-12-15 19:01:06,962 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher
2022-12-15 19:01:07,126 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-resourcemanager.properties,hadoop-metrics2.properties
2022-12-15 19:01:07,148 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2022-12-15 19:01:07,148 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system started
2022-12-15 19:01:07,168 INFO org.apache.hadoop.yarn.security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.
2022-12-15 19:01:07,171 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
2022-12-15 19:01:07,183 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
2022-12-15 19:01:07,216 INFO org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo: Registered RMNMInfo MBean
2022-12-15 19:01:07,217 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.
2022-12-15 19:01:07,240 INFO org.apache.hadoop.util.HostsFileReader: Refreshing hosts (include/exclude) list
2022-12-15 19:01:07,251 INFO org.apache.hadoop.util.HostsFileReader: Adding a node "adh-test-00-mnode1.ru-central1.internal" to the list of included hosts from /etc/hadoop/conf/include-path
2022-12-15 19:01:07,252 INFO org.apache.hadoop.util.HostsFileReader: Adding a node "adh-test-00-mnode2.ru-central1.internal" to the list of included hosts from /etc/hadoop/conf/include-path
2022-12-15 19:01:07,252 INFO org.apache.hadoop.util.HostsFileReader: Adding a node "adh-test-00-mnode3.ru-central1.internal" to the list of included hosts from /etc/hadoop/conf/include-path
2022-12-15 19:01:07,609 INFO org.apache.hadoop.conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf/capacity-scheduler.xml
2022-12-15 19:01:07,650 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1024, vCores:1>
2022-12-15 19:01:07,650 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Maximum allocation = <memory:4096, vCores:2>
2022-12-15 19:01:07,702 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined
2022-12-15 19:01:07,702 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined
2022-12-15 19:01:07,721 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,
, reservationsContinueLooking=true, orderingPolicy=utilization, priority=0
2022-12-15 19:01:07,721 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root
2022-12-15 19:01:07,748 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined
2022-12-15 19:01:07,749 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined
2022-12-15 19:01:07,754 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Initializing default
capacity = 1.0 [= (float) configuredCapacity / 100 ]
absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]
maxCapacity = 1.0 [= configuredMaxCapacity ]
absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
effectiveMinResource=<memory:0, vCores:0>
 , effectiveMaxResource=<memory:0, vCores:0>
userLimit = 100 [= configuredUserLimit ]
userLimitFactor = 1.0 [= configuredUserLimitFactor ]
maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
maxAMResourcePerQueuePercent = 0.2 [= configuredMaximumAMResourcePercent ]
minimumAllocationFactor = 0.75 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
maximumAllocation = <memory:4096, vCores:2> [= configuredMaxAllocation ]
numContainers = 0 [= currentNumContainers ]
state = RUNNING [= configuredState ]
acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]
nodeLocalityDelay = 40
rackLocalityAdditionalDelay = -1
labels=*,
reservationsContinueLooking = true
preemptionDisabled = true
defaultAppPriorityPerQueue = 0
priority = 0
maxLifetime = -1 seconds
defaultLifetime = -1 seconds
2022-12-15 19:01:07,755 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>
2022-12-15 19:01:07,755 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2022-12-15 19:01:07,784 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2022-12-15 19:01:07,785 INFO org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false
2022-12-15 19:01:07,786 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAllocation=<<memory:4096, vCores:2>>, asynchronousScheduling=false, asyncScheduleInterval=5ms
2022-12-15 19:01:07,790 INFO org.apache.hadoop.conf.Configuration: dynamic-resources.xml not found
2022-12-15 19:01:07,796 INFO org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].
2022-12-15 19:01:07,796 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.
2022-12-15 19:01:07,800 INFO org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. 
2022-12-15 19:01:08,225 INFO org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl: Timeline service address: adh-test-00-mnode2.ru-central1.internal:8188
2022-12-15 19:01:08,650 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-15 19:01:08,650 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-15 19:01:08,650 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-15 19:01:08,650 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-15 19:01:08,650 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-15 19:01:08,650 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-15 19:01:08,650 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-15 19:01:08,650 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-15 19:01:08,650 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-15 19:01:08,650 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-15 19:01:08,704 INFO org.eclipse.jetty.util.log: Logging initialized @17463ms
2022-12-15 19:01:08,752 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Using webapps at: file:/usr/lib/hadoop-yarn/webapps/ui2
2022-12-15 19:01:08,797 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2022-12-15 19:01:08,804 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined
2022-12-15 19:01:08,808 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2022-12-15 19:01:08,811 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster
2022-12-15 19:01:08,812 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs
2022-12-15 19:01:08,812 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static
2022-12-15 19:01:08,812 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster
2022-12-15 19:01:08,814 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2022-12-15 19:01:08,814 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2022-12-15 19:01:08,817 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /cluster/*
2022-12-15 19:01:08,817 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /ws/*
2022-12-15 19:01:08,817 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /app/*
2022-12-15 19:01:09,295 INFO org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules
2022-12-15 19:01:09,303 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 8088
2022-12-15 19:01:09,304 INFO org.eclipse.jetty.server.Server: jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2022-12-15 19:01:09,344 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2022-12-15 19:01:09,382 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2022-12-15 19:01:09,387 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@569bf9eb{/logs,file:///var/log/hadoop-yarn/,AVAILABLE}
2022-12-15 19:01:09,388 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2022-12-15 19:01:09,388 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@eb6449b{/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.1.2.jar!/webapps/static,AVAILABLE}
2022-12-15 19:01:09,406 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2022-12-15 19:01:11,097 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@ebd06a9{/,file:///tmp/jetty-0.0.0.0-8088-cluster-_-any-1366849252542494468.dir/webapp/,AVAILABLE}{/cluster}
2022-12-15 19:01:11,148 INFO org.eclipse.jetty.webapp.StandardDescriptorProcessor: NO JSP Support for /ui2, did not find org.eclipse.jetty.jsp.JettyJspServlet
2022-12-15 19:01:11,156 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@a7ad6e5{/ui2,file:///usr/lib/hadoop-yarn/webapps/ui2/,AVAILABLE}
2022-12-15 19:01:11,163 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@1b5c3e5f{HTTP/1.1,[http/1.1]}{0.0.0.0:8088}
2022-12-15 19:01:11,163 INFO org.eclipse.jetty.server.Server: Started @19924ms
2022-12-15 19:01:11,163 INFO org.apache.hadoop.yarn.webapp.WebApps: Web app cluster started at 8088
2022-12-15 19:01:11,333 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 100 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2022-12-15 19:01:11,344 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8033
2022-12-15 19:01:11,374 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server
2022-12-15 19:01:11,375 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2022-12-15 19:01:11,376 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8033: starting
2022-12-15 19:01:11,382 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning to active state
2022-12-15 19:01:11,568 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Using state database at /srv/hadoop-yarn/leveldb-state-store/yarn-rm-state for recovery
2022-12-15 19:01:11,814 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Recovery started
2022-12-15 19:01:11,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Loaded RM state version info 1.1
2022-12-15 19:01:11,857 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 2 RM delegation token master keys
2022-12-15 19:01:11,857 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 0 RM delegation tokens
2022-12-15 19:01:11,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 2 applications and 2 application attempts
2022-12-15 19:01:11,958 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 0 reservations
2022-12-15 19:01:11,958 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: recovering RMDelegationTokenSecretManager.
2022-12-15 19:01:11,961 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager: Recovering 2 applications
2022-12-15 19:01:12,032 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2022-12-15 19:01:12,070 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1671022279734_0001
2022-12-15 19:01:12,073 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1671022279734_0001,name=QuasiMonteCarlo,user=yarn,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1671022279734_0001/,appMasterHost=N/A,submitTime=1671022505181,startTime=1671022505266,finishTime=1671022524607,finalStatus=SUCCEEDED,memorySeconds=216125,vcoreSeconds=183,preemptedMemorySeconds=216125,preemptedVcoreSeconds=183,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=216125 MB-seconds\, 183 vcore-seconds,preemptedResourceSeconds=216125 MB-seconds\, 183 vcore-seconds
2022-12-15 19:01:12,073 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 2 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2022-12-15 19:01:12,075 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager: Successfully recovered 2 out of 2 applications
2022-12-15 19:01:12,075 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Recovery ended
2022-12-15 19:01:12,075 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2022-12-15 19:01:12,076 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1671022279734_0002
2022-12-15 19:01:12,078 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2022-12-15 19:01:12,078 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2022-12-15 19:01:12,078 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 3
2022-12-15 19:01:12,078 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2022-12-15 19:01:12,080 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2022-12-15 19:01:12,080 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2022-12-15 19:01:12,087 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 4
2022-12-15 19:01:12,087 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2022-12-15 19:01:12,088 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1671022279734_0002,name=QuasiMonteCarlo,user=hdfs,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1671022279734_0002/,appMasterHost=N/A,submitTime=1671023676043,startTime=1671023676044,finishTime=1671023694897,finalStatus=SUCCEEDED,memorySeconds=228352,vcoreSeconds=193,preemptedMemorySeconds=228352,preemptedVcoreSeconds=193,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=228352 MB-seconds\, 193 vcore-seconds,preemptedResourceSeconds=228352 MB-seconds\, 193 vcore-seconds
2022-12-15 19:01:12,656 INFO org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager failed in state STARTED
org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /system/yarn/node-labels. Name node is in safe mode.
The reported blocks 0 needs additional 11 blocks to reach the threshold 0.9990 of total blocks 12.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:adh-test-00-mnode1.ru-central1.internal
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1461)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1448)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3145)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1126)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:707)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2421)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2395)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1325)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1322)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1339)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1314)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)
	at org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore.init(FileSystemNodeLabelsStore.java:87)
	at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.initNodeLabelStore(CommonNodeLabelsManager.java:253)
	at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.serviceStart(CommonNodeLabelsManager.java:268)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:869)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1228)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1269)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1265)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1265)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1316)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1501)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /system/yarn/node-labels. Name node is in safe mode.
The reported blocks 0 needs additional 11 blocks to reach the threshold 0.9990 of total blocks 12.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:adh-test-00-mnode1.ru-central1.internal
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1461)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1448)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3145)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1126)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:707)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy89.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:653)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy90.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2419)
	... 24 more
2022-12-15 19:01:12,762 INFO org.apache.hadoop.service.AbstractService: Service RMActiveServices failed in state STARTED
org.apache.hadoop.service.ServiceStateException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /system/yarn/node-labels. Name node is in safe mode.
The reported blocks 0 needs additional 11 blocks to reach the threshold 0.9990 of total blocks 12.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:adh-test-00-mnode1.ru-central1.internal
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1461)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1448)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3145)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1126)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:707)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)

	at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:203)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:869)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1228)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1269)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1265)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1265)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1316)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1501)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /system/yarn/node-labels. Name node is in safe mode.
The reported blocks 0 needs additional 11 blocks to reach the threshold 0.9990 of total blocks 12.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:adh-test-00-mnode1.ru-central1.internal
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1461)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1448)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3145)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1126)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:707)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2421)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2395)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1325)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1322)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1339)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1314)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)
	at org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore.init(FileSystemNodeLabelsStore.java:87)
	at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.initNodeLabelStore(CommonNodeLabelsManager.java:253)
	at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.serviceStart(CommonNodeLabelsManager.java:268)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	... 13 more
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /system/yarn/node-labels. Name node is in safe mode.
The reported blocks 0 needs additional 11 blocks to reach the threshold 0.9990 of total blocks 12.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:adh-test-00-mnode1.ru-central1.internal
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1461)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1448)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3145)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1126)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:707)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy89.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:653)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy90.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2419)
	... 24 more
2022-12-15 19:01:12,764 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: AMLivelinessMonitor thread interrupted
2022-12-15 19:01:12,765 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppLifetimeMonitor thread interrupted
2022-12-15 19:01:12,765 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.ContainerAllocationExpirer thread interrupted
2022-12-15 19:01:12,765 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: AMLivelinessMonitor thread interrupted
2022-12-15 19:01:12,764 ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted
2022-12-15 19:01:12,770 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping ResourceManager metrics system...
2022-12-15 19:01:12,770 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system stopped.
2022-12-15 19:01:12,770 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system shutdown complete.
2022-12-15 19:01:12,770 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2022-12-15 19:01:12,774 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
2022-12-15 19:01:12,774 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
2022-12-15 19:01:12,775 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
2022-12-15 19:01:12,775 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
2022-12-15 19:01:12,775 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreFactory: Using RMStateStore implementation - class org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore
2022-12-15 19:01:12,775 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
2022-12-15 19:01:12,775 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
2022-12-15 19:01:12,775 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
2022-12-15 19:01:12,775 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher
2022-12-15 19:01:12,775 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher
2022-12-15 19:01:12,775 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher
2022-12-15 19:01:12,775 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher
2022-12-15 19:01:12,779 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-resourcemanager.properties,hadoop-metrics2.properties
2022-12-15 19:01:12,782 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2022-12-15 19:01:12,782 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system started
2022-12-15 19:01:12,784 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
2022-12-15 19:01:12,784 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
2022-12-15 19:01:12,784 WARN org.apache.hadoop.metrics2.util.MBeans: Failed to register MBean "Hadoop:service=ResourceManager,name=RMNMInfo": Instance already exists.
2022-12-15 19:01:12,784 INFO org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo: Registered RMNMInfo MBean
2022-12-15 19:01:12,784 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.
2022-12-15 19:01:12,784 INFO org.apache.hadoop.util.HostsFileReader: Refreshing hosts (include/exclude) list
2022-12-15 19:01:12,784 INFO org.apache.hadoop.util.HostsFileReader: Adding a node "adh-test-00-mnode1.ru-central1.internal" to the list of included hosts from /etc/hadoop/conf/include-path
2022-12-15 19:01:12,784 INFO org.apache.hadoop.util.HostsFileReader: Adding a node "adh-test-00-mnode2.ru-central1.internal" to the list of included hosts from /etc/hadoop/conf/include-path
2022-12-15 19:01:12,784 INFO org.apache.hadoop.util.HostsFileReader: Adding a node "adh-test-00-mnode3.ru-central1.internal" to the list of included hosts from /etc/hadoop/conf/include-path
2022-12-15 19:01:12,787 INFO org.apache.hadoop.conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf/capacity-scheduler.xml
2022-12-15 19:01:12,798 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1024, vCores:1>
2022-12-15 19:01:12,798 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Maximum allocation = <memory:4096, vCores:2>
2022-12-15 19:01:12,803 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined
2022-12-15 19:01:12,803 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined
2022-12-15 19:01:12,804 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,
, reservationsContinueLooking=true, orderingPolicy=utilization, priority=0
2022-12-15 19:01:12,804 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root
2022-12-15 19:01:12,807 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined
2022-12-15 19:01:12,807 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined
2022-12-15 19:01:12,809 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Initializing default
capacity = 1.0 [= (float) configuredCapacity / 100 ]
absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]
maxCapacity = 1.0 [= configuredMaxCapacity ]
absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
effectiveMinResource=<memory:0, vCores:0>
 , effectiveMaxResource=<memory:0, vCores:0>
userLimit = 100 [= configuredUserLimit ]
userLimitFactor = 1.0 [= configuredUserLimitFactor ]
maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
maxAMResourcePerQueuePercent = 0.2 [= configuredMaximumAMResourcePercent ]
minimumAllocationFactor = 0.75 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
maximumAllocation = <memory:4096, vCores:2> [= configuredMaxAllocation ]
numContainers = 0 [= currentNumContainers ]
state = RUNNING [= configuredState ]
acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]
nodeLocalityDelay = 40
rackLocalityAdditionalDelay = -1
labels=*,
reservationsContinueLooking = true
preemptionDisabled = true
defaultAppPriorityPerQueue = 0
priority = 0
maxLifetime = -1 seconds
defaultLifetime = -1 seconds
2022-12-15 19:01:12,809 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>
2022-12-15 19:01:12,809 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2022-12-15 19:01:12,809 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2022-12-15 19:01:12,810 INFO org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false
2022-12-15 19:01:12,810 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAllocation=<<memory:4096, vCores:2>>, asynchronousScheduling=false, asyncScheduleInterval=5ms
2022-12-15 19:01:12,810 INFO org.apache.hadoop.conf.Configuration: dynamic-resources.xml not found
2022-12-15 19:01:12,811 INFO org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].
2022-12-15 19:01:12,811 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.
2022-12-15 19:01:12,811 INFO org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. 
2022-12-15 19:01:12,812 INFO org.apache.hadoop.service.AbstractService: Service ResourceManager failed in state STARTED
org.apache.hadoop.service.ServiceStateException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /system/yarn/node-labels. Name node is in safe mode.
The reported blocks 0 needs additional 11 blocks to reach the threshold 0.9990 of total blocks 12.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:adh-test-00-mnode1.ru-central1.internal
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1461)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1448)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3145)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1126)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:707)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)

	at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:203)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:869)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1228)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1269)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1265)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1265)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1316)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1501)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /system/yarn/node-labels. Name node is in safe mode.
The reported blocks 0 needs additional 11 blocks to reach the threshold 0.9990 of total blocks 12.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:adh-test-00-mnode1.ru-central1.internal
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1461)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1448)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3145)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1126)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:707)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2421)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2395)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1325)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1322)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1339)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1314)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)
	at org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore.init(FileSystemNodeLabelsStore.java:87)
	at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.initNodeLabelStore(CommonNodeLabelsManager.java:253)
	at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.serviceStart(CommonNodeLabelsManager.java:268)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	... 13 more
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /system/yarn/node-labels. Name node is in safe mode.
The reported blocks 0 needs additional 11 blocks to reach the threshold 0.9990 of total blocks 12.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:adh-test-00-mnode1.ru-central1.internal
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1461)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1448)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3145)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1126)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:707)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy89.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:653)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy90.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2419)
	... 24 more
2022-12-15 19:01:12,816 ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted
2022-12-15 19:01:12,818 INFO org.eclipse.jetty.server.handler.ContextHandler: Stopped o.e.j.w.WebAppContext@ebd06a9{/,null,UNAVAILABLE}{/cluster}
2022-12-15 19:01:12,827 INFO org.eclipse.jetty.server.AbstractConnector: Stopped ServerConnector@1b5c3e5f{HTTP/1.1,[http/1.1]}{0.0.0.0:8088}
2022-12-15 19:01:12,829 INFO org.eclipse.jetty.server.handler.ContextHandler: Stopped o.e.j.w.WebAppContext@a7ad6e5{/ui2,file:///usr/lib/hadoop-yarn/webapps/ui2/,UNAVAILABLE}
2022-12-15 19:01:12,829 INFO org.eclipse.jetty.server.handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@eb6449b{/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.1.2.jar!/webapps/static,UNAVAILABLE}
2022-12-15 19:01:12,830 INFO org.eclipse.jetty.server.handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@569bf9eb{/logs,file:///var/log/hadoop-yarn/,UNAVAILABLE}
2022-12-15 19:01:12,837 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2022-12-15 19:01:12,837 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2022-12-15 19:01:12,938 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:13,038 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:13,138 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:13,240 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:13,340 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:13,441 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:13,541 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:13,642 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:13,742 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:13,842 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:13,942 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:14,043 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:14,143 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:14,243 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:14,344 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:14,444 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:14,544 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:14,644 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:14,745 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:14,845 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:14,945 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:15,046 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:15,146 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:15,246 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:15,346 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:15,447 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:15,547 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:15,647 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:15,747 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:15,847 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:15,948 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:16,048 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:16,148 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:16,248 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:16,349 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:16,449 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:16,549 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:16,650 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:16,750 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:16,850 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:16,951 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:17,051 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:17,151 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:17,252 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:17,352 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:17,453 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:17,553 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:17,654 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:17,754 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:17,854 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:17,954 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:18,054 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:18,155 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:18,255 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:18,355 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:18,456 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:18,556 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:18,656 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:18,756 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:18,857 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:18,957 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:19,058 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:19,158 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:19,258 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:19,359 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:19,459 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:19,559 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:19,659 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:19,760 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:19,860 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:19,960 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:20,061 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:20,163 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:20,263 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:20,363 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:20,463 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:20,564 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:20,664 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:20,764 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:20,864 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:20,965 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:21,065 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:21,165 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:21,265 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:21,366 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:21,466 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:21,566 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:21,666 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:21,767 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:21,867 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:21,967 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:22,067 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:22,168 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:22,268 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:22,368 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:22,469 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:22,569 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:22,669 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:22,769 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:22,870 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:22,970 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:23,070 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:23,170 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:23,271 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:23,371 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:23,471 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:23,571 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:23,672 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:23,772 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:23,872 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:23,972 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:24,073 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:24,173 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:24,273 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:24,374 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:24,474 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:24,574 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:24,674 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:24,775 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:24,875 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:24,975 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:25,075 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:25,176 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:25,276 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:25,376 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:25,476 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:25,577 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:25,677 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:25,777 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:25,877 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:25,978 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:26,078 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:26,178 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:26,278 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:26,379 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:26,479 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:26,579 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:26,679 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:26,780 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:26,880 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:26,980 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:27,080 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:27,180 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:27,281 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:27,381 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:27,481 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:27,582 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:27,682 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:27,782 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:27,882 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:27,983 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:28,083 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:28,183 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:28,283 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:28,384 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:28,484 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:28,584 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:28,685 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:28,785 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:28,885 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:28,985 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:29,086 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:29,186 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:29,286 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:29,387 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:29,487 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:29,587 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:29,687 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:29,788 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:29,888 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:29,988 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:30,088 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:30,189 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:30,289 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:30,389 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:30,489 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:30,590 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:30,690 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:30,790 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:30,890 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:30,991 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:31,091 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:31,191 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:31,292 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:31,392 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:31,492 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:31,592 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:31,693 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:31,793 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:31,893 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:31,993 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:32,094 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:32,194 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:32,294 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:32,394 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:32,495 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:32,595 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:32,695 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:32,796 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:32,896 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:32,996 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:33,096 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:33,197 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:33,297 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:33,397 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:33,497 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:33,598 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:33,698 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:33,798 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:33,898 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:33,998 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:34,099 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:34,199 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:34,299 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:34,399 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:34,499 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:34,599 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:34,700 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:34,800 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:34,900 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:35,000 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:35,100 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:35,201 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:35,301 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:35,401 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:35,501 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:35,601 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:35,702 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:35,802 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:35,902 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:36,002 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:36,103 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:36,203 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:36,303 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:36,404 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:36,504 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:36,604 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:36,704 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:36,804 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:36,905 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:37,005 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:37,105 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:37,206 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:37,306 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:37,406 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:37,506 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:37,607 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:37,707 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:37,807 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:37,907 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:38,008 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:38,108 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:38,208 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:38,309 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:38,409 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:38,509 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:38,609 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:38,710 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:38,810 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:38,910 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:39,010 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:39,111 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:39,211 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:39,311 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:39,412 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:39,512 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:39,612 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:39,712 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:39,812 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:39,913 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:40,013 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:40,113 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:40,213 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:40,314 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:40,414 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:40,514 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:40,614 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:40,715 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:40,815 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:40,915 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:41,015 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:41,116 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:41,216 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:41,316 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:41,416 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:41,516 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:41,617 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:41,717 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:41,817 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:41,917 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:42,018 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:42,118 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:42,218 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:42,318 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:42,419 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:42,519 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:42,619 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:42,719 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:42,820 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:42,920 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:43,020 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:43,120 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:43,221 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:43,321 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:43,421 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:43,521 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:43,622 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:43,722 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:43,822 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:43,922 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:44,023 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:44,123 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:44,223 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:44,324 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:44,424 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:44,524 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:44,624 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:44,725 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:44,825 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:44,925 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:45,025 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:45,126 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:45,226 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:45,326 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:45,426 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:45,527 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:45,627 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:45,727 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:45,827 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:45,928 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:46,028 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:46,128 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:46,229 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:46,329 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:46,429 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:46,529 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:46,629 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:46,730 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:46,830 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:46,930 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:47,030 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:47,131 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:47,231 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:47,331 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:47,431 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:47,532 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:47,632 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:47,732 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:47,832 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:47,933 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:48,033 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:48,133 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:48,233 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:48,333 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:48,433 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:48,534 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:48,634 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:48,734 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:48,834 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:48,935 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:49,035 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:49,135 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:49,235 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:49,336 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:49,436 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:49,536 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:49,636 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:49,737 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:49,837 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:49,937 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:50,037 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:50,138 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:50,238 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:50,338 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:50,438 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:50,539 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:50,639 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:50,739 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:50,839 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:50,939 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:51,040 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:51,140 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:51,240 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:51,340 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:51,440 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:51,541 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:51,641 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:51,741 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:51,841 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:51,942 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:52,042 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:52,142 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:52,242 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:52,343 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:52,443 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:52,543 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:52,643 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:52,743 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:52,844 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:52,944 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:53,044 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:53,144 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:53,245 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:53,345 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:53,445 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:53,545 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:53,646 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:53,746 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:53,846 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:53,946 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:54,047 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:54,147 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:54,247 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:54,347 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:54,448 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:54,548 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:54,648 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:54,748 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:54,849 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:54,949 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:55,049 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:55,149 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:55,249 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:55,350 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:55,450 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:55,550 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:55,651 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:55,751 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:55,851 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:55,951 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:56,051 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:56,152 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:56,252 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:56,352 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:56,452 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:56,553 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:56,653 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:56,753 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:56,853 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:56,954 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:57,054 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:57,154 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:57,254 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:57,355 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:57,455 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:57,555 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:57,655 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:57,755 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:57,856 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:57,956 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:58,056 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:58,156 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:58,257 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:58,357 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:58,457 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:58,557 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:58,658 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:58,758 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:58,858 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:58,958 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:59,059 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:59,159 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:59,259 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:59,359 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:59,459 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:59,560 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:59,660 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:59,760 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:59,861 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:01:59,961 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:00,061 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:00,161 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:00,261 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:00,362 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:00,462 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:00,562 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:00,662 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:00,762 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:00,863 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:00,963 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:01,063 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:01,163 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:01,264 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:01,364 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:01,464 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:01,564 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:01,665 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:01,765 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:01,865 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:01,965 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:02,066 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:02,166 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:02,266 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:02,367 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:02,467 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:02,567 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:02,667 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:02,768 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:02,868 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:02,968 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:03,068 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:03,169 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:03,269 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:03,369 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:03,469 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:03,569 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:03,670 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:03,770 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:03,870 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:03,970 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:04,071 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:04,171 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:04,271 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:04,371 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:04,472 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:04,572 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:04,672 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:04,772 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:04,873 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:04,973 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:05,073 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:05,173 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:05,274 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:05,374 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:05,474 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:05,574 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:05,674 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:05,775 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:05,875 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:05,975 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:06,075 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:06,175 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:06,275 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:06,376 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:06,476 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:06,576 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:06,676 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:06,777 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:06,877 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:06,977 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:07,077 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:07,178 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:07,278 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:07,378 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:07,478 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:07,578 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:07,679 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:07,779 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:07,879 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:07,980 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:08,080 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:08,180 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:08,280 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:08,380 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:08,481 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:08,581 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:08,681 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:08,781 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:08,881 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:08,981 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:09,082 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:09,182 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:09,282 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:09,382 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:09,483 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:09,583 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:09,683 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:09,783 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:09,884 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:09,984 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:10,084 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:10,184 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:10,284 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:10,385 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:10,485 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:10,585 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:10,685 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:10,786 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:10,886 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:10,986 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:11,086 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:11,186 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:11,287 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:11,387 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:11,487 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:11,587 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:11,688 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:11,788 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:11,888 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:11,988 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:12,088 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:12,113 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 30 more time(s).
Message: java.net.SocketTimeoutException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on socket timeout exception: java.net.SocketTimeoutException: connect timed out; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
2022-12-15 19:02:12,142 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 30 more time(s).
Message: java.net.SocketTimeoutException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on socket timeout exception: java.net.SocketTimeoutException: connect timed out; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
2022-12-15 19:02:12,188 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :TIMED_WAITING
2022-12-15 19:02:12,288 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :TIMED_WAITING
2022-12-15 19:02:12,389 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :TIMED_WAITING
2022-12-15 19:02:12,489 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :TIMED_WAITING
2022-12-15 19:02:12,589 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :TIMED_WAITING
2022-12-15 19:02:12,689 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :TIMED_WAITING
2022-12-15 19:02:12,789 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :TIMED_WAITING
2022-12-15 19:02:12,890 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :TIMED_WAITING
2022-12-15 19:02:12,990 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :TIMED_WAITING
2022-12-15 19:02:13,090 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :TIMED_WAITING
2022-12-15 19:02:13,190 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:13,291 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:13,391 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:13,491 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:13,591 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:13,691 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:13,792 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:13,892 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:13,992 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:14,092 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:14,193 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:14,293 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:14,393 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:14,493 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:14,594 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:14,694 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:14,794 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:14,894 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:14,995 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:15,095 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:15,195 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:15,295 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:15,395 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:15,496 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:15,596 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:15,696 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:15,796 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:15,897 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:15,997 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:16,097 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:16,197 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:16,297 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:16,398 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:16,498 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:16,598 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:16,698 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:16,798 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:16,898 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:16,999 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:17,099 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:17,199 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:17,299 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:17,400 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:17,500 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:17,600 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:17,700 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:17,801 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:17,901 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:18,001 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:18,101 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:18,202 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:18,302 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:18,402 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:18,502 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:18,602 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:18,703 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:18,803 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:18,903 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:19,003 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:19,103 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:19,204 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:19,304 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:19,404 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:19,504 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:19,605 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:19,705 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:19,805 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:19,905 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:20,006 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:20,106 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:20,206 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:20,306 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:20,407 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:20,507 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:20,607 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:20,707 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:20,807 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:20,908 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:21,008 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:21,108 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:21,208 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:21,309 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:21,409 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:21,509 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:21,609 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:21,710 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:21,810 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:21,910 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:22,010 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:22,111 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:22,211 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:22,311 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:22,411 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:22,512 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:22,612 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:22,712 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:22,812 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:22,913 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:23,013 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:23,113 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:23,213 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:23,314 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:23,414 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:23,514 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:23,614 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:23,715 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:23,815 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:23,915 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:24,015 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:24,116 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:24,216 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:24,316 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:24,416 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:24,516 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:24,617 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:24,717 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:24,817 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:24,917 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:25,018 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:25,118 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:25,218 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:25,319 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:25,419 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:25,519 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:25,619 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:25,719 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:25,820 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:25,920 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:26,020 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:26,120 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:26,221 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:26,321 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:26,421 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:26,521 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:26,621 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:26,722 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:26,822 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:26,922 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:27,022 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:27,123 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:27,223 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:27,323 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:27,423 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:27,524 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:27,624 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:27,724 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:27,824 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:27,924 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:28,025 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:28,125 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:28,225 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:28,326 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:28,426 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:28,526 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:28,626 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:28,726 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:28,827 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:28,927 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:29,027 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:29,127 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:29,228 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:29,328 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:29,428 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:29,528 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:29,629 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:29,729 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:29,829 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:29,929 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:30,029 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:30,130 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:30,230 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:30,330 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:30,430 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:30,531 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:30,631 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:30,731 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:30,831 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:30,931 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:31,032 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:31,132 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:31,232 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:31,332 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:31,432 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:31,532 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:31,633 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:31,733 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:31,833 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:31,933 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:32,033 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:32,133 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:32,234 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:32,334 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:32,434 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:32,534 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:32,634 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:32,735 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:32,835 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:32,935 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:33,035 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:33,136 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:33,236 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:33,336 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:33,436 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:33,537 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:33,637 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:33,737 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:33,837 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:33,937 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:34,038 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:34,138 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:34,238 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:34,338 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:34,439 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:34,539 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:34,639 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:34,739 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:34,840 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:34,940 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:35,040 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:35,140 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:35,240 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:35,341 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:35,441 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:35,541 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:35,641 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:35,742 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:35,842 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:35,942 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:36,042 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:36,142 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:36,243 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:36,343 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:36,443 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:36,543 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:36,644 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:36,744 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:36,844 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:36,944 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:37,044 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:37,144 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:37,245 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:37,345 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:37,445 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:37,545 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:37,645 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:37,746 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:37,846 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:37,946 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:38,046 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:38,147 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:38,247 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:38,347 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:38,448 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:38,548 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:38,648 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:38,749 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:38,849 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:38,949 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:39,049 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:39,150 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:39,250 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:39,350 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:39,450 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:39,551 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:39,651 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:39,751 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:39,851 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:39,951 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:40,051 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:40,152 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:40,252 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:40,352 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:40,452 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:40,552 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:40,653 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:40,753 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:40,853 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:40,953 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:41,053 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:41,154 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:41,254 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:41,354 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:41,454 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:41,555 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:41,655 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:41,755 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:41,855 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:41,956 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:42,056 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:42,156 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:42,256 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:42,356 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:42,457 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:42,557 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:42,657 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:42,757 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:42,858 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:42,958 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:43,058 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:43,158 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:43,259 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:43,359 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:43,459 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:43,559 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:43,660 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:43,760 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:43,860 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:43,960 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:44,060 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:44,161 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:44,261 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:44,361 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:44,461 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:44,562 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:44,662 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:44,762 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:44,862 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:44,962 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:45,063 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:45,163 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:45,263 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:45,363 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:45,464 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:45,564 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:45,664 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:45,764 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:45,865 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:45,965 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:46,065 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:46,165 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:46,265 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:46,366 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:46,466 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:46,566 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:46,666 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:46,766 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:46,866 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:46,967 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:47,067 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:47,167 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:47,267 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:47,367 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:47,468 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:47,568 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:47,668 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:47,768 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:47,869 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:47,969 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:48,069 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:48,169 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:48,270 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:48,370 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:48,470 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:48,570 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:48,671 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:48,771 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:48,871 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:48,971 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:49,072 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:49,172 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:49,272 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:49,372 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:49,472 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:49,573 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:49,673 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:49,773 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:49,873 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:49,974 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:50,074 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:50,174 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:50,274 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:50,375 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:50,475 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:50,575 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:50,675 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:50,775 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:50,876 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:50,976 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:51,076 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:51,176 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:51,277 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:51,377 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:51,477 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:51,577 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:51,678 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:51,778 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:51,878 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:51,978 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:52,079 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:52,179 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:52,279 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:52,379 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:52,480 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:52,580 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:52,680 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:52,780 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:52,880 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:52,981 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:53,081 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:53,181 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:53,281 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:53,382 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:53,482 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:53,582 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:53,682 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:53,782 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:53,883 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:53,983 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:54,083 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:54,184 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:54,284 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:54,384 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:54,484 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:54,584 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:54,685 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:54,785 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:54,885 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:54,985 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:55,086 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:55,186 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:55,286 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:55,386 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:55,486 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:55,587 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:55,687 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:55,787 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:55,887 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:55,987 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:56,087 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:56,188 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:56,288 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:56,388 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:56,488 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:56,589 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:56,689 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:56,789 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:56,889 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:56,990 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:57,090 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:57,190 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:57,290 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:57,390 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:57,491 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:57,591 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:57,691 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:57,792 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:57,892 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:57,992 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:58,092 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:58,193 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:58,293 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:58,393 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:58,493 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:58,594 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:58,694 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:58,794 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:58,895 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:58,995 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:59,095 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:59,195 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:59,296 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:59,396 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:59,496 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:59,596 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:59,697 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:59,797 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:59,897 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:02:59,997 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:00,098 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:00,198 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:00,298 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:00,398 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:00,499 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:00,599 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:00,699 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:00,799 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:00,900 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:01,000 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:01,100 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:01,200 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:01,301 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:01,401 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:01,501 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:01,602 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:01,702 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:01,802 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:01,902 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:02,003 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:02,103 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:02,203 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:02,304 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:02,404 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:02,504 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:02,604 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:02,705 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:02,806 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:02,907 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:03,007 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:03,107 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:03,207 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:03,308 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:03,408 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:03,508 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:03,608 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:03,709 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:03,809 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:03,909 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:04,009 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:04,110 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:04,210 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:04,310 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:04,410 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:04,510 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:04,611 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:04,711 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:04,811 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:04,911 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:05,012 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:05,112 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:05,212 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:05,312 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:05,413 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:05,513 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:05,613 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:05,713 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:05,813 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:05,913 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:06,014 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:06,114 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:06,214 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:06,314 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:06,415 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:06,515 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:06,615 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:06,715 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:06,815 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:06,915 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:07,016 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:07,116 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:07,216 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:07,317 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:07,417 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:07,517 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:07,617 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:07,718 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:07,818 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:07,918 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:08,018 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:08,119 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:08,219 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:08,319 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:08,419 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:08,520 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:08,620 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:08,720 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:08,820 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:08,921 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:09,021 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:09,121 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:09,221 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:09,321 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:09,422 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:09,522 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:09,622 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:09,723 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:09,823 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:09,923 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:10,023 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:10,124 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:10,224 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:10,324 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:10,424 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:10,524 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:10,625 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:10,725 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:10,825 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:10,925 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:11,026 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:11,126 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:11,226 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:11,326 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:11,426 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:11,527 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:11,627 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:11,727 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:11,827 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:11,928 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:12,028 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:12,128 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:12,228 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:12,329 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:12,429 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:12,529 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:12,629 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:12,730 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:12,830 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:12,930 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:13,030 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:13,131 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:13,166 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 29 more time(s).
Message: java.net.SocketTimeoutException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on socket timeout exception: java.net.SocketTimeoutException: connect timed out; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
2022-12-15 19:03:13,197 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 29 more time(s).
Message: java.net.SocketTimeoutException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on socket timeout exception: java.net.SocketTimeoutException: connect timed out; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
2022-12-15 19:03:13,231 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :TIMED_WAITING
2022-12-15 19:03:13,331 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :TIMED_WAITING
2022-12-15 19:03:13,432 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :TIMED_WAITING
2022-12-15 19:03:13,532 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :TIMED_WAITING
2022-12-15 19:03:13,632 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :TIMED_WAITING
2022-12-15 19:03:13,732 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :TIMED_WAITING
2022-12-15 19:03:13,833 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :TIMED_WAITING
2022-12-15 19:03:13,933 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :TIMED_WAITING
2022-12-15 19:03:14,033 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :TIMED_WAITING
2022-12-15 19:03:14,133 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :TIMED_WAITING
2022-12-15 19:03:14,234 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:14,334 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:14,434 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:14,534 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:14,634 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :BLOCKED
2022-12-15 19:03:14,735 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:14,761 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :WAITING
2022-12-15 19:03:14,761 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2022-12-15 19:03:14,762 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Waiting for AsyncDispatcher to drain. Thread state is :RUNNABLE
2022-12-15 19:03:14,762 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2022-12-15 19:03:14,762 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2022-12-15 19:03:14,763 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2022-12-15 19:03:14,763 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2022-12-15 19:03:14,763 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2022-12-15 19:03:14,763 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2022-12-15 19:03:14,763 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2022-12-15 19:03:14,763 INFO org.apache.hadoop.ipc.Server: Stopping server on 8033
2022-12-15 19:03:14,764 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning to standby state
2022-12-15 19:03:14,764 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8033
2022-12-15 19:03:14,764 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioned to standby state
2022-12-15 19:03:14,764 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2022-12-15 19:03:14,764 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error starting ResourceManager
org.apache.hadoop.service.ServiceStateException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /system/yarn/node-labels. Name node is in safe mode.
The reported blocks 0 needs additional 11 blocks to reach the threshold 0.9990 of total blocks 12.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:adh-test-00-mnode1.ru-central1.internal
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1461)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1448)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3145)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1126)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:707)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)

	at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:203)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:869)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1228)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1269)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1265)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1265)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1316)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1501)
Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /system/yarn/node-labels. Name node is in safe mode.
The reported blocks 0 needs additional 11 blocks to reach the threshold 0.9990 of total blocks 12.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:adh-test-00-mnode1.ru-central1.internal
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1461)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1448)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3145)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1126)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:707)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2421)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2395)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1325)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1322)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1339)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1314)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)
	at org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore.init(FileSystemNodeLabelsStore.java:87)
	at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.initNodeLabelStore(CommonNodeLabelsManager.java:253)
	at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.serviceStart(CommonNodeLabelsManager.java:268)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	... 13 more
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /system/yarn/node-labels. Name node is in safe mode.
The reported blocks 0 needs additional 11 blocks to reach the threshold 0.9990 of total blocks 12.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:adh-test-00-mnode1.ru-central1.internal
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1461)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1448)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3145)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1126)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:707)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy89.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:653)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy90.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2419)
	... 24 more
2022-12-15 19:03:14,768 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down ResourceManager at adh-test-00-mnode1.ru-central1.internal/10.130.0.4
************************************************************/
2022-12-28 12:51:25,827 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting ResourceManager
STARTUP_MSG:   host = adh-test-00-mnode1.ru-central1.internal/10.130.0.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.2
STARTUP_MSG:   classpath = /etc/hadoop/conf:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/stax2-api.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jetty-io-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-security-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/jetty-server-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/woodstox-core.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/zookeeper-3.4.13.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jetty-util-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/httpclient-4.5.2.jar:/usr/lib/hadoop/lib/httpcore-4.4.4.jar:/usr/lib/hadoop/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/jackson-annotations-2.7.8.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/jackson-core-2.7.8.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jackson-databind-2.7.8.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-xml-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/jetty-http-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.1.2.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-auth-3.1.2.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-common-3.1.2-tests.jar:/usr/lib/hadoop/.//hadoop-common-3.1.2.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms-3.1.2.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-nfs-3.1.2.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.2.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.1.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.1.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.1.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.1.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-2.8.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.271.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-0.8.2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//lz4-1.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//netty-buffer-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//netty-codec-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//netty-codec-http-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//netty-common-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//netty-handler-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//netty-resolver-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//netty-transport-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.1.2-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-yarn/./:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar
STARTUP_MSG:   build = git@ssh.gitlab.adsw.io:arenadata/infrastructure/code/ci/prj_adh.git -r 15525ac41ebda8bc83c285e9b6ea7fb9921fd7fd; compiled by 'jenkins' on 2022-09-08T14:33Z
STARTUP_MSG:   java = 1.8.0_352
************************************************************/
2022-12-28 12:51:25,836 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]
2022-12-28 12:51:25,997 INFO org.apache.hadoop.conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf/core-site.xml
2022-12-28 12:51:26,097 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found
2022-12-28 12:51:26,098 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.
2022-12-28 12:51:26,181 INFO org.apache.hadoop.conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf/yarn-site.xml
2022-12-28 12:51:26,218 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
2022-12-28 12:51:26,247 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
2022-12-28 12:51:26,251 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
2022-12-28 12:51:26,255 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
2022-12-28 12:51:26,304 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreFactory: Using RMStateStore implementation - class org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore
2022-12-28 12:51:26,305 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
2022-12-28 12:51:26,309 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
2022-12-28 12:51:26,309 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
2022-12-28 12:51:26,338 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher
2022-12-28 12:51:26,339 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher
2022-12-28 12:51:26,340 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher
2022-12-28 12:51:26,340 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher
2022-12-28 12:51:26,431 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-resourcemanager.properties,hadoop-metrics2.properties
2022-12-28 12:51:26,448 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2022-12-28 12:51:26,448 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system started
2022-12-28 12:51:26,461 INFO org.apache.hadoop.yarn.security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.
2022-12-28 12:51:26,463 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
2022-12-28 12:51:26,469 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
2022-12-28 12:51:26,471 INFO org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo: Registered RMNMInfo MBean
2022-12-28 12:51:26,471 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.
2022-12-28 12:51:26,476 INFO org.apache.hadoop.util.HostsFileReader: Refreshing hosts (include/exclude) list
2022-12-28 12:51:26,476 INFO org.apache.hadoop.util.HostsFileReader: Adding a node "adh-test-00-mnode1.ru-central1.internal" to the list of included hosts from /etc/hadoop/conf/include-path
2022-12-28 12:51:26,476 INFO org.apache.hadoop.util.HostsFileReader: Adding a node "adh-test-00-mnode2.ru-central1.internal" to the list of included hosts from /etc/hadoop/conf/include-path
2022-12-28 12:51:26,476 INFO org.apache.hadoop.util.HostsFileReader: Adding a node "adh-test-00-mnode3.ru-central1.internal" to the list of included hosts from /etc/hadoop/conf/include-path
2022-12-28 12:51:26,503 INFO org.apache.hadoop.conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf/capacity-scheduler.xml
2022-12-28 12:51:26,516 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1024, vCores:1>
2022-12-28 12:51:26,516 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Maximum allocation = <memory:4096, vCores:2>
2022-12-28 12:51:26,548 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined
2022-12-28 12:51:26,548 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined
2022-12-28 12:51:26,558 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=ADMINISTER_QUEUE:*SUBMIT_APP:*, labels=*,
, reservationsContinueLooking=true, orderingPolicy=utilization, priority=0
2022-12-28 12:51:26,558 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root
2022-12-28 12:51:26,570 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined
2022-12-28 12:51:26,570 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined
2022-12-28 12:51:26,574 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Initializing default
capacity = 1.0 [= (float) configuredCapacity / 100 ]
absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]
maxCapacity = 1.0 [= configuredMaxCapacity ]
absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
effectiveMinResource=<memory:0, vCores:0>
 , effectiveMaxResource=<memory:0, vCores:0>
userLimit = 100 [= configuredUserLimit ]
userLimitFactor = 1.0 [= configuredUserLimitFactor ]
maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
maxAMResourcePerQueuePercent = 0.2 [= configuredMaximumAMResourcePercent ]
minimumAllocationFactor = 0.75 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
maximumAllocation = <memory:4096, vCores:2> [= configuredMaxAllocation ]
numContainers = 0 [= currentNumContainers ]
state = RUNNING [= configuredState ]
acls = ADMINISTER_QUEUE:*SUBMIT_APP:* [= configuredAcls ]
nodeLocalityDelay = 40
rackLocalityAdditionalDelay = -1
labels=*,
reservationsContinueLooking = true
preemptionDisabled = true
defaultAppPriorityPerQueue = 0
priority = 0
maxLifetime = -1 seconds
defaultLifetime = -1 seconds
2022-12-28 12:51:26,574 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>
2022-12-28 12:51:26,574 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2022-12-28 12:51:26,576 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2022-12-28 12:51:26,576 INFO org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false
2022-12-28 12:51:26,577 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAllocation=<<memory:4096, vCores:2>>, asynchronousScheduling=false, asyncScheduleInterval=5ms
2022-12-28 12:51:26,581 INFO org.apache.hadoop.conf.Configuration: dynamic-resources.xml not found
2022-12-28 12:51:26,582 INFO org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].
2022-12-28 12:51:26,582 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.
2022-12-28 12:51:26,584 INFO org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. 
2022-12-28 12:51:26,750 INFO org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl: Timeline service address: adh-test-00-mnode2.ru-central1.internal:8188
2022-12-28 12:51:26,980 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-28 12:51:26,981 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-28 12:51:26,981 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-28 12:51:26,981 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-28 12:51:26,981 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-28 12:51:26,981 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-28 12:51:26,981 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-28 12:51:26,981 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-28 12:51:26,981 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-28 12:51:26,981 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2022-12-28 12:51:27,021 INFO org.eclipse.jetty.util.log: Logging initialized @1712ms
2022-12-28 12:51:27,065 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Using webapps at: file:/usr/lib/hadoop-yarn/webapps/ui2
2022-12-28 12:51:27,110 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2022-12-28 12:51:27,114 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined
2022-12-28 12:51:27,118 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2022-12-28 12:51:27,122 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster
2022-12-28 12:51:27,122 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs
2022-12-28 12:51:27,122 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static
2022-12-28 12:51:27,123 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster
2022-12-28 12:51:27,123 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2022-12-28 12:51:27,123 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2022-12-28 12:51:27,125 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /cluster/*
2022-12-28 12:51:27,125 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /ws/*
2022-12-28 12:51:27,125 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /app/*
2022-12-28 12:51:27,527 INFO org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules
2022-12-28 12:51:27,535 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 8088
2022-12-28 12:51:27,536 INFO org.eclipse.jetty.server.Server: jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2022-12-28 12:51:27,567 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2022-12-28 12:51:27,577 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2022-12-28 12:51:27,579 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2022-12-28 12:51:27,579 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2022-12-28 12:51:27,581 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@61526469{/logs,file:///var/log/hadoop-yarn/,AVAILABLE}
2022-12-28 12:51:27,582 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c351808{/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.1.2.jar!/webapps/static,AVAILABLE}
2022-12-28 12:51:28,423 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@16c587de{/,file:///tmp/jetty-0.0.0.0-8088-cluster-_-any-662984944794973340.dir/webapp/,AVAILABLE}{/cluster}
2022-12-28 12:51:28,461 INFO org.eclipse.jetty.webapp.StandardDescriptorProcessor: NO JSP Support for /ui2, did not find org.eclipse.jetty.jsp.JettyJspServlet
2022-12-28 12:51:28,468 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@3b1ed14b{/ui2,file:///usr/lib/hadoop-yarn/webapps/ui2/,AVAILABLE}
2022-12-28 12:51:28,474 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@13741d5a{HTTP/1.1,[http/1.1]}{0.0.0.0:8088}
2022-12-28 12:51:28,475 INFO org.eclipse.jetty.server.Server: Started @3167ms
2022-12-28 12:51:28,475 INFO org.apache.hadoop.yarn.webapp.WebApps: Web app cluster started at 8088
2022-12-28 12:51:28,619 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 100 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2022-12-28 12:51:28,630 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8033
2022-12-28 12:51:28,651 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server
2022-12-28 12:51:28,652 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2022-12-28 12:51:28,652 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8033: starting
2022-12-28 12:51:28,655 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning to active state
2022-12-28 12:51:28,729 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Using state database at /srv/hadoop-yarn/leveldb-state-store/yarn-rm-state for recovery
2022-12-28 12:51:28,758 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Recovery started
2022-12-28 12:51:28,763 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Loaded RM state version info 1.1
2022-12-28 12:51:28,773 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 4 RM delegation token master keys
2022-12-28 12:51:28,773 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 0 RM delegation tokens
2022-12-28 12:51:28,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 2 applications and 2 application attempts
2022-12-28 12:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 0 reservations
2022-12-28 12:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: recovering RMDelegationTokenSecretManager.
2022-12-28 12:51:28,845 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager: Recovering 2 applications
2022-12-28 12:51:28,916 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2022-12-28 12:51:28,943 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 30 more time(s).
Message: java.net.ConnectException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on connection exception: java.net.ConnectException: Connection refused (Connection refused); For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2022-12-28 12:51:28,947 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 2 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2022-12-28 12:51:28,948 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1671022279734_0001
2022-12-28 12:51:28,949 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 30 more time(s).
Message: java.net.ConnectException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on connection exception: java.net.ConnectException: Connection refused (Connection refused); For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2022-12-28 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager: Successfully recovered 2 out of 2 applications
2022-12-28 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Recovery ended
2022-12-28 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1671022279734_0001,name=QuasiMonteCarlo,user=yarn,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1671022279734_0001/,appMasterHost=N/A,submitTime=1671022505181,startTime=1671022505266,finishTime=1671022524607,finalStatus=SUCCEEDED,memorySeconds=216125,vcoreSeconds=183,preemptedMemorySeconds=216125,preemptedVcoreSeconds=183,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=216125 MB-seconds\, 183 vcore-seconds,preemptedResourceSeconds=216125 MB-seconds\, 183 vcore-seconds
2022-12-28 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2022-12-28 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1671022279734_0002
2022-12-28 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1671022279734_0002,name=QuasiMonteCarlo,user=hdfs,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1671022279734_0002/,appMasterHost=N/A,submitTime=1671023676043,startTime=1671023676044,finishTime=1671023694897,finalStatus=SUCCEEDED,memorySeconds=228352,vcoreSeconds=193,preemptedMemorySeconds=228352,preemptedVcoreSeconds=193,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=228352 MB-seconds\, 193 vcore-seconds,preemptedResourceSeconds=228352 MB-seconds\, 193 vcore-seconds
2022-12-28 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2022-12-28 12:51:28,951 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2022-12-28 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 5
2022-12-28 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2022-12-28 12:51:28,952 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2022-12-28 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 1
2022-12-28 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2022-12-28 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 2
2022-12-28 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2022-12-28 12:51:28,953 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 3
2022-12-28 12:51:28,953 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2022-12-28 12:51:28,953 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 4
2022-12-28 12:51:28,953 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2022-12-28 12:51:28,953 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2022-12-28 12:51:28,953 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 6
2022-12-28 12:51:28,953 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2022-12-28 12:51:29,485 INFO org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager: No Modified Node label Mapping to replace
2022-12-28 12:51:29,587 INFO org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore: Finished write mirror at:hdfs:/system/yarn/node-labels/nodelabel.mirror
2022-12-28 12:51:29,587 INFO org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore: Finished create editlog file at:hdfs:/system/yarn/node-labels/nodelabel.editlog
2022-12-28 12:51:29,588 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler
2022-12-28 12:51:29,601 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2022-12-28 12:51:29,602 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8031
2022-12-28 12:51:29,604 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server
2022-12-28 12:51:29,604 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2022-12-28 12:51:29,604 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8031: starting
2022-12-28 12:51:29,631 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2022-12-28 12:51:29,640 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2022-12-28 12:51:29,645 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8030
2022-12-28 12:51:29,651 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server
2022-12-28 12:51:29,651 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2022-12-28 12:51:29,652 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8030: starting
2022-12-28 12:51:29,709 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2022-12-28 12:51:29,709 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8032
2022-12-28 12:51:29,711 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server
2022-12-28 12:51:29,712 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2022-12-28 12:51:29,712 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8032: starting
2022-12-28 12:51:29,721 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioned to active state
2022-12-28 12:51:29,945 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 29 more time(s).
Message: java.net.ConnectException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on connection exception: java.net.ConnectException: Connection refused (Connection refused); For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2022-12-28 12:51:29,950 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 29 more time(s).
Message: java.net.ConnectException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on connection exception: java.net.ConnectException: Connection refused (Connection refused); For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2022-12-28 12:51:30,946 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 28 more time(s).
Message: java.net.ConnectException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on connection exception: java.net.ConnectException: Connection refused (Connection refused); For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2022-12-28 12:51:30,951 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 28 more time(s).
Message: java.net.ConnectException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on connection exception: java.net.ConnectException: Connection refused (Connection refused); For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2022-12-28 12:51:31,948 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 27 more time(s).
Message: java.net.ConnectException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on connection exception: java.net.ConnectException: Connection refused (Connection refused); For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2022-12-28 12:51:31,952 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 27 more time(s).
Message: java.net.ConnectException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on connection exception: java.net.ConnectException: Connection refused (Connection refused); For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2022-12-28 12:51:32,949 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 26 more time(s).
Message: java.net.ConnectException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on connection exception: java.net.ConnectException: Connection refused (Connection refused); For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2022-12-28 12:51:32,953 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 26 more time(s).
Message: java.net.ConnectException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on connection exception: java.net.ConnectException: Connection refused (Connection refused); For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2022-12-28 12:51:33,951 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 25 more time(s).
Message: java.net.ConnectException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on connection exception: java.net.ConnectException: Connection refused (Connection refused); For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2022-12-28 12:51:33,954 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 25 more time(s).
Message: java.net.ConnectException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on connection exception: java.net.ConnectException: Connection refused (Connection refused); For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2022-12-28 12:51:34,952 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 24 more time(s).
Message: java.net.ConnectException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on connection exception: java.net.ConnectException: Connection refused (Connection refused); For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2022-12-28 12:51:34,955 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 24 more time(s).
Message: java.net.ConnectException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on connection exception: java.net.ConnectException: Connection refused (Connection refused); For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2022-12-28 12:51:35,954 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 23 more time(s).
Message: java.net.ConnectException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on connection exception: java.net.ConnectException: Connection refused (Connection refused); For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2022-12-28 12:51:35,956 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 23 more time(s).
Message: java.net.ConnectException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on connection exception: java.net.ConnectException: Connection refused (Connection refused); For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2022-12-28 12:51:36,959 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 22 more time(s).
Message: java.net.ConnectException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on connection exception: java.net.ConnectException: Connection refused (Connection refused); For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2022-12-28 12:51:36,959 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 22 more time(s).
Message: java.net.ConnectException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on connection exception: java.net.ConnectException: Connection refused (Connection refused); For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2022-12-28 12:51:37,964 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 21 more time(s).
Message: java.net.ConnectException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on connection exception: java.net.ConnectException: Connection refused (Connection refused); For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2022-12-28 12:51:37,964 INFO org.apache.hadoop.yarn.client.api.impl.TimelineConnector: Exception caught by TimelineClientConnectionRetry, will try 21 more time(s).
Message: java.net.ConnectException: Call From null to adh-test-00-mnode2.ru-central1.internal:8188 failed on connection exception: java.net.ConnectException: Connection refused (Connection refused); For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
2022-12-28 12:52:07,352 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node adh-test-00-mnode2.ru-central1.internal(cmPort: 8041 httpPort: 8042) registered with capability: <memory:7057, vCores:2>, assigned nodeId adh-test-00-mnode2.ru-central1.internal:8041
2022-12-28 12:52:07,358 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: adh-test-00-mnode2.ru-central1.internal:8041 Node Transitioned from NEW to RUNNING
2022-12-28 12:52:07,386 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node adh-test-00-mnode2.ru-central1.internal:8041 clusterResource: <memory:7057, vCores:2>
2022-12-28 12:52:07,418 INFO org.apache.hadoop.yarn.server.resourcemanager.RMActiveServiceContext: Scheduler recovery is done. Start allocating new containers.
2022-12-28 12:52:07,473 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node adh-test-00-mnode1.ru-central1.internal(cmPort: 8041 httpPort: 8042) registered with capability: <memory:7057, vCores:2>, assigned nodeId adh-test-00-mnode1.ru-central1.internal:8041
2022-12-28 12:52:07,473 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: adh-test-00-mnode1.ru-central1.internal:8041 Node Transitioned from NEW to RUNNING
2022-12-28 12:52:07,476 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node adh-test-00-mnode1.ru-central1.internal:8041 clusterResource: <memory:14114, vCores:4>
2022-12-28 12:52:07,557 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node adh-test-00-mnode3.ru-central1.internal(cmPort: 8041 httpPort: 8042) registered with capability: <memory:7057, vCores:2>, assigned nodeId adh-test-00-mnode3.ru-central1.internal:8041
2022-12-28 12:52:07,557 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: adh-test-00-mnode3.ru-central1.internal:8041 Node Transitioned from NEW to RUNNING
2022-12-28 12:52:07,559 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node adh-test-00-mnode3.ru-central1.internal:8041 clusterResource: <memory:21171, vCores:6>
2022-12-28 13:01:26,503 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Release request cache is cleaned up
2022-12-28 13:51:28,758 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-28 13:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2022-12-28 14:51:28,758 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-28 14:51:28,763 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2022-12-28 15:51:28,758 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-28 15:51:28,763 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2022-12-28 16:51:28,758 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-28 16:51:28,769 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2022-12-28 17:51:28,758 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-28 17:51:28,772 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2022-12-28 18:51:28,758 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-28 18:51:28,764 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2022-12-28 19:51:28,758 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-28 19:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2022-12-28 20:51:28,758 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-28 20:51:28,762 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2022-12-28 21:51:28,758 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-28 21:51:28,762 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2022-12-28 22:51:28,759 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-28 22:51:28,763 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2022-12-28 23:51:28,759 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-28 23:51:28,767 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2022-12-29 00:51:28,759 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 00:51:28,764 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2022-12-29 01:51:28,759 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 01:51:28,767 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2022-12-29 02:51:28,759 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 02:51:28,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 28 msec
2022-12-29 03:51:28,759 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 03:51:28,765 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2022-12-29 04:51:28,759 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 04:51:28,762 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2022-12-29 05:51:28,760 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 05:51:28,770 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2022-12-29 06:51:28,759 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 06:51:28,764 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2022-12-29 07:17:22,604 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 07:51:28,759 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 07:51:28,769 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2022-12-29 08:51:28,759 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 08:51:28,764 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2022-12-29 09:51:28,759 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 09:51:28,766 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2022-12-29 10:51:28,759 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 10:51:28,765 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2022-12-29 11:51:28,759 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 11:51:28,762 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2022-12-29 11:53:47,452 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 11:54:21,371 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 11:54:22,538 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 11:54:25,239 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 11:54:26,250 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 11:56:39,239 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 11:56:41,057 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 11:56:41,332 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 11:56:47,030 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 11:57:06,342 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 11:57:08,023 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 11:57:10,254 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 11:57:12,257 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 11:57:14,696 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 11:57:18,552 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 12:00:43,219 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 12:00:47,231 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 12:00:49,134 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 12:00:51,503 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 12:00:55,437 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 12:10:04,277 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 12:51:28,759 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 12:51:28,782 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2022-12-29 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2022-12-29 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2022-12-29 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2022-12-29 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220253 in 900000ms
2022-12-29 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2022-12-29 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892553 in 900000ms
2022-12-29 12:51:30,183 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2022-12-29 12:51:31,447 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2022-12-29 12:51:31,448 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 7
2022-12-29 12:51:31,448 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2022-12-29 13:06:03,958 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 13:06:05,498 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 13:06:09,255 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 13:06:11,738 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 13:06:13,068 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 13:06:20,582 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 13:06:21,625 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 13:06:22,728 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 13:06:23,980 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 13:06:25,057 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626873
2022-12-29 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2022-12-29 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220253
2022-12-29 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892553
2022-12-29 13:51:28,759 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 13:51:28,784 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2022-12-29 13:54:17,574 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-29 14:51:28,759 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 14:51:28,767 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2022-12-29 15:51:28,759 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 15:51:28,769 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2022-12-29 16:51:28,759 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 16:51:28,768 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2022-12-29 17:51:28,759 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 17:51:28,773 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2022-12-29 18:51:28,760 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 18:51:28,763 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2022-12-29 19:51:28,761 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 19:51:28,775 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2022-12-29 20:51:28,761 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 20:51:28,772 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2022-12-29 21:51:28,761 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 21:51:28,764 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2022-12-29 22:51:28,761 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 22:51:28,775 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2022-12-29 23:51:28,761 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-29 23:51:28,770 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2022-12-30 00:51:28,761 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 00:51:28,772 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2022-12-30 01:51:28,761 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 01:51:28,766 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2022-12-30 02:51:28,761 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 02:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2022-12-30 03:51:28,762 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 03:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2022-12-30 04:44:21,129 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-30 04:45:19,638 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://adh-test-00-mnode1.ru-central1.internal:19888/jobhistory/job/job_1671022279734_0002 which is the app master GUI of application_1671022279734_0002 owned by hdfs
2022-12-30 04:45:43,681 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-30 04:45:45,599 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-30 04:45:47,359 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-30 04:45:48,982 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-30 04:45:50,023 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-30 04:45:50,910 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-30 04:45:51,920 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-30 04:45:55,742 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-30 04:45:56,933 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-30 04:45:58,149 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-30 04:45:59,569 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-30 04:46:04,800 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-30 04:51:28,762 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 04:51:28,776 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2022-12-30 05:20:15,418 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-30 05:20:31,436 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-30 05:34:05,147 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 1
2022-12-30 05:34:05,838 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1672231888656_0001' is submitted without priority hence considering default queue/cluster priority: 0
2022-12-30 05:34:05,838 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1672231888656_0001
2022-12-30 05:34:05,839 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 1 submitted by user root
2022-12-30 05:34:05,839 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=root	IP=10.130.0.4	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1672231888656_0001	QUEUENAME=default
2022-12-30 05:34:05,839 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1672231888656_0001
2022-12-30 05:34:05,843 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0001 State change from NEW to NEW_SAVING on event = START
2022-12-30 05:34:05,843 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1672231888656_0001
2022-12-30 05:34:05,970 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0001 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2022-12-30 05:34:05,971 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1672231888656_0001 user: root leaf-queue of parent: root #applications: 1
2022-12-30 05:34:05,972 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1672231888656_0001 from user: root, in queue: default
2022-12-30 05:34:05,972 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0001 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2022-12-30 05:34:06,000 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1672231888656_0001_000001
2022-12-30 05:34:06,002 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0001_000001 State change from NEW to SUBMITTED on event = START
2022-12-30 05:34:06,019 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1672231888656_0001 from user: root activated in queue: default
2022-12-30 05:34:06,019 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1672231888656_0001 user: root, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2022-12-30 05:34:06,019 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1672231888656_0001_000001 to scheduler from user root in queue default
2022-12-30 05:34:06,026 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0001_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2022-12-30 05:34:06,264 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0001_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2022-12-30 05:34:06,268 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0001_01_000001 Container Transitioned from NEW to ALLOCATED
2022-12-30 05:34:06,269 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0001_01_000001 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2022-12-30 05:34:06,269 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0001	CONTAINERID=container_e02_1672231888656_0001_01_000001	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2022-12-30 05:34:06,289 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode1.ru-central1.internal:8041 for container : container_e02_1672231888656_0001_01_000001
2022-12-30 05:34:06,298 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-30 05:34:06,299 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1672231888656_0001_000001
2022-12-30 05:34:06,299 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1672231888656_0001 AttemptId: appattempt_1672231888656_0001_000001 MasterContainer: Container: [ContainerId: container_e02_1672231888656_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode1.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode1.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.4:8041 }, ExecutionType: GUARANTEED, ]
2022-12-30 05:34:06,299 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.0967361 absoluteUsedCapacity=0.0967361 used=<memory:2048, vCores:1> cluster=<memory:21171, vCores:6>
2022-12-30 05:34:06,299 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-30 05:34:06,302 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2022-12-30 05:34:06,307 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2022-12-30 05:34:06,309 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1672231888656_0001_000001
2022-12-30 05:34:06,365 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e02_1672231888656_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode1.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode1.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.4:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1672231888656_0001_000001
2022-12-30 05:34:06,365 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1672231888656_0001_000001
2022-12-30 05:34:06,369 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1672231888656_0001_000001
2022-12-30 05:34:06,608 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e02_1672231888656_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode1.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode1.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.4:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1672231888656_0001_000001
2022-12-30 05:34:06,609 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0001_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2022-12-30 05:34:07,292 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING
2022-12-30 05:34:10,925 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-30 05:34:11,525 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1672231888656_0001_000001 (auth:SIMPLE)
2022-12-30 05:34:11,550 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1672231888656_0001_000001
2022-12-30 05:34:11,550 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=root	IP=10.130.0.4	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1672231888656_0001	APPATTEMPTID=appattempt_1672231888656_0001_000001
2022-12-30 05:34:11,552 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0001_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2022-12-30 05:34:11,552 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0001 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2022-12-30 05:34:12,169 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1672231888656_0001_000001 with final state: FINISHING, and exit status: -1000
2022-12-30 05:34:12,169 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0001_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2022-12-30 05:34:12,169 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1672231888656_0001 with final state: FINISHING
2022-12-30 05:34:12,170 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0001 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2022-12-30 05:34:12,170 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1672231888656_0001
2022-12-30 05:34:12,170 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0001_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2022-12-30 05:34:12,170 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0001 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2022-12-30 05:34:13,170 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1672231888656_0001 unregistered successfully. 
2022-12-30 05:34:18,560 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0001_01_000001 Container Transitioned from RUNNING to COMPLETED
2022-12-30 05:34:18,560 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1672231888656_0001_000001
2022-12-30 05:34:18,560 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0001	CONTAINERID=container_e02_1672231888656_0001_01_000001	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2022-12-30 05:34:18,561 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1672231888656_0001_000001
2022-12-30 05:34:18,563 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0001_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2022-12-30 05:34:18,565 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0001 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2022-12-30 05:34:18,565 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1672231888656_0001_000001 is done. finalState=FINISHED
2022-12-30 05:34:18,565 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=root	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0001
2022-12-30 05:34:18,565 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1672231888656_0001 requests cleared
2022-12-30 05:34:18,565 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1672231888656_0001_000001
2022-12-30 05:34:18,566 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1672231888656_0001 user: root queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2022-12-30 05:34:18,566 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1672231888656_0001 user: root leaf-queue of parent: root #applications: 0
2022-12-30 05:34:18,567 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0001,name=word count,user=root,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0001/,appMasterHost=adh-test-00-mnode1.ru-central1.internal,submitTime=1672378445838,startTime=1672378445838,finishTime=1672378452169,finalStatus=FAILED,memorySeconds=25184,vcoreSeconds=12,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=25184 MB-seconds\, 12 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2022-12-30 05:35:03,145 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 2
2022-12-30 05:35:03,808 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1672231888656_0002' is submitted without priority hence considering default queue/cluster priority: 0
2022-12-30 05:35:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1672231888656_0002
2022-12-30 05:35:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 2 submitted by user student48
2022-12-30 05:35:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	IP=10.130.0.4	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1672231888656_0002	QUEUENAME=default
2022-12-30 05:35:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1672231888656_0002
2022-12-30 05:35:03,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1672231888656_0002
2022-12-30 05:35:03,810 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0002 State change from NEW to NEW_SAVING on event = START
2022-12-30 05:35:03,811 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0002 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2022-12-30 05:35:03,811 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1672231888656_0002 user: student48 leaf-queue of parent: root #applications: 1
2022-12-30 05:35:03,811 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1672231888656_0002 from user: student48, in queue: default
2022-12-30 05:35:03,812 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0002 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2022-12-30 05:35:03,812 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1672231888656_0002_000001
2022-12-30 05:35:03,812 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0002_000001 State change from NEW to SUBMITTED on event = START
2022-12-30 05:35:03,812 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1672231888656_0002 from user: student48 activated in queue: default
2022-12-30 05:35:03,812 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1672231888656_0002 user: student48, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2022-12-30 05:35:03,812 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1672231888656_0002_000001 to scheduler from user student48 in queue default
2022-12-30 05:35:03,813 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0002_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2022-12-30 05:35:04,456 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2022-12-30 05:35:04,456 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0002_01_000001 Container Transitioned from NEW to ALLOCATED
2022-12-30 05:35:04,456 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0002_01_000001 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2022-12-30 05:35:04,456 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0002	CONTAINERID=container_e02_1672231888656_0002_01_000001	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2022-12-30 05:35:04,457 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_e02_1672231888656_0002_01_000001
2022-12-30 05:35:04,458 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0002_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-30 05:35:04,459 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1672231888656_0002_000001
2022-12-30 05:35:04,459 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1672231888656_0002 AttemptId: appattempt_1672231888656_0002_000001 MasterContainer: Container: [ContainerId: container_e02_1672231888656_0002_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ]
2022-12-30 05:35:04,459 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.0967361 absoluteUsedCapacity=0.0967361 used=<memory:2048, vCores:1> cluster=<memory:21171, vCores:6>
2022-12-30 05:35:04,459 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-30 05:35:04,459 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0002_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2022-12-30 05:35:04,460 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0002_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2022-12-30 05:35:04,461 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1672231888656_0002_000001
2022-12-30 05:35:04,462 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e02_1672231888656_0002_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1672231888656_0002_000001
2022-12-30 05:35:04,462 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1672231888656_0002_000001
2022-12-30 05:35:04,462 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1672231888656_0002_000001
2022-12-30 05:35:04,686 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e02_1672231888656_0002_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1672231888656_0002_000001
2022-12-30 05:35:04,686 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0002_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2022-12-30 05:35:05,249 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-30 05:35:05,468 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0002_01_000001 Container Transitioned from ACQUIRED to RUNNING
2022-12-30 05:35:09,759 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1672231888656_0002_000001 (auth:SIMPLE)
2022-12-30 05:35:09,765 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1672231888656_0002_000001
2022-12-30 05:35:09,765 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	IP=10.130.0.6	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1672231888656_0002	APPATTEMPTID=appattempt_1672231888656_0002_000001
2022-12-30 05:35:09,765 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0002_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2022-12-30 05:35:09,766 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0002 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2022-12-30 05:35:11,509 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2022-12-30 05:35:11,510 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0002_01_000002 Container Transitioned from NEW to ALLOCATED
2022-12-30 05:35:11,510 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0002_01_000002 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 2 containers, <memory:3072, vCores:2> used and <memory:3985, vCores:0> available after allocation
2022-12-30 05:35:11,510 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0002	CONTAINERID=container_e02_1672231888656_0002_01_000002	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-30 05:35:11,510 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.14510415 absoluteUsedCapacity=0.14510415 used=<memory:3072, vCores:2> cluster=<memory:21171, vCores:6>
2022-12-30 05:35:11,510 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-30 05:35:11,846 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_e02_1672231888656_0002_01_000002
2022-12-30 05:35:11,847 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0002_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-30 05:35:12,513 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0002_01_000002 Container Transitioned from ACQUIRED to RUNNING
2022-12-30 05:35:12,864 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1672231888656_0002
2022-12-30 05:35:15,068 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0002_01_000002 Container Transitioned from RUNNING to COMPLETED
2022-12-30 05:35:15,068 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0002	CONTAINERID=container_e02_1672231888656_0002_01_000002	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-30 05:35:16,070 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0002_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2022-12-30 05:35:16,070 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0002_01_000003 Container Transitioned from NEW to ALLOCATED
2022-12-30 05:35:16,070 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0002_01_000003 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 2 containers, <memory:3072, vCores:2> used and <memory:3985, vCores:0> available after allocation
2022-12-30 05:35:16,070 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0002	CONTAINERID=container_e02_1672231888656_0002_01_000003	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-30 05:35:16,071 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.14510415 absoluteUsedCapacity=0.14510415 used=<memory:3072, vCores:2> cluster=<memory:21171, vCores:6>
2022-12-30 05:35:16,071 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2022-12-30 05:35:16,895 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0002_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2022-12-30 05:35:17,072 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0002_01_000003 Container Transitioned from ACQUIRED to RUNNING
2022-12-30 05:35:17,900 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1672231888656_0002
2022-12-30 05:35:19,495 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1672231888656_0002_000001 with final state: FINISHING, and exit status: -1000
2022-12-30 05:35:19,495 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0002_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2022-12-30 05:35:19,495 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1672231888656_0002 with final state: FINISHING
2022-12-30 05:35:19,495 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0002 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2022-12-30 05:35:19,495 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1672231888656_0002
2022-12-30 05:35:19,495 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0002_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2022-12-30 05:35:19,496 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0002 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2022-12-30 05:35:19,591 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0002_01_000003 Container Transitioned from RUNNING to COMPLETED
2022-12-30 05:35:19,591 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0002	CONTAINERID=container_e02_1672231888656_0002_01_000003	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2022-12-30 05:35:20,498 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1672231888656_0002 unregistered successfully. 
2022-12-30 05:35:25,876 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1672231888656_0002_000001
2022-12-30 05:35:25,876 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0002_01_000001 Container Transitioned from RUNNING to COMPLETED
2022-12-30 05:35:25,876 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1672231888656_0002_000001
2022-12-30 05:35:25,876 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0002	CONTAINERID=container_e02_1672231888656_0002_01_000001	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2022-12-30 05:35:25,876 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0002_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2022-12-30 05:35:25,877 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0002 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2022-12-30 05:35:25,877 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1672231888656_0002_000001 is done. finalState=FINISHED
2022-12-30 05:35:25,878 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0002
2022-12-30 05:35:25,878 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1672231888656_0002 requests cleared
2022-12-30 05:35:25,878 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1672231888656_0002 user: student48 queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2022-12-30 05:35:25,878 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1672231888656_0002 user: student48 leaf-queue of parent: root #applications: 0
2022-12-30 05:35:25,878 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0002,name=word count,user=student48,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0002/,appMasterHost=adh-test-00-mnode3.ru-central1.internal,submitTime=1672378503807,startTime=1672378503809,finishTime=1672378519495,finalStatus=SUCCEEDED,memorySeconds=51118,vcoreSeconds=27,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=51118 MB-seconds\, 27 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2022-12-30 05:35:25,878 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1672231888656_0002_000001
2022-12-30 05:35:45,410 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-30 05:37:02,662 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://adh-test-00-mnode1.ru-central1.internal:19888/jobhistory/job/job_1672231888656_0002 which is the app master GUI of application_1672231888656_0002 owned by student48
2022-12-30 05:44:34,675 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-30 05:46:06,476 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://adh-test-00-mnode1.ru-central1.internal:19888/jobhistory/job/job_1672231888656_0002 which is the app master GUI of application_1672231888656_0002 owned by student48
2022-12-30 05:51:00,763 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://adh-test-00-mnode1.ru-central1.internal:19888/jobhistory/job/job_1672231888656_0002 which is the app master GUI of application_1672231888656_0002 owned by student48
2022-12-30 05:51:28,762 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 05:51:28,777 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2022-12-30 05:51:52,353 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-30 05:52:21,442 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-30 05:52:26,901 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-30 05:53:39,773 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://adh-test-00-mnode1.ru-central1.internal:19888/jobhistory/job/job_1672231888656_0002 which is the app master GUI of application_1672231888656_0002 owned by student48
2022-12-30 06:27:50,804 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://adh-test-00-mnode1.ru-central1.internal:19888/jobhistory/job/job_1672231888656_0002 which is the app master GUI of application_1672231888656_0002 owned by student48
2022-12-30 06:51:28,763 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 06:51:28,766 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2022-12-30 07:51:28,763 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 07:51:28,766 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2022-12-30 08:51:28,763 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 08:51:28,772 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2022-12-30 09:51:28,763 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 09:51:28,778 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2022-12-30 10:51:28,763 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 10:51:28,766 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2022-12-30 11:51:28,763 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 11:51:28,767 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2022-12-30 12:51:28,763 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 12:51:28,766 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2022-12-30 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2022-12-30 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2022-12-30 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2022-12-30 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220252 in 900000ms
2022-12-30 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2022-12-30 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892554 in 900000ms
2022-12-30 12:51:32,742 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2022-12-30 12:51:34,000 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2022-12-30 12:51:34,000 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 8
2022-12-30 12:51:34,000 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2022-12-30 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220252
2022-12-30 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892554
2022-12-30 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626874
2022-12-30 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2022-12-30 13:51:28,763 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 13:51:28,777 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2022-12-30 14:51:28,764 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 14:51:28,767 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2022-12-30 15:51:28,764 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 15:51:28,773 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2022-12-30 16:51:28,764 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 16:51:28,781 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2022-12-30 17:51:28,764 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 17:51:28,776 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2022-12-30 18:51:28,764 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 18:51:28,768 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2022-12-30 19:51:28,765 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 19:51:28,776 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2022-12-30 20:51:28,765 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 20:51:28,777 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2022-12-30 21:51:28,765 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 21:51:28,768 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2022-12-30 22:51:28,766 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 22:51:28,780 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2022-12-30 23:51:28,766 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-30 23:51:28,771 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2022-12-31 00:51:28,766 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 00:51:28,770 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2022-12-31 01:51:28,766 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 01:51:28,769 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2022-12-31 02:51:28,766 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 02:51:28,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 27 msec
2022-12-31 03:19:32,762 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-31 03:25:08,449 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://adh-test-00-mnode1.ru-central1.internal:19888/jobhistory/job/job_1672231888656_0002 which is the app master GUI of application_1672231888656_0002 owned by student48
2022-12-31 03:51:28,767 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 03:51:28,781 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2022-12-31 04:06:46,348 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2022-12-31 04:51:28,767 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 04:51:28,771 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2022-12-31 05:51:28,767 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 05:51:28,771 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2022-12-31 06:51:28,768 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 06:51:28,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2022-12-31 07:51:28,768 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 07:51:28,784 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2022-12-31 08:51:28,768 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 08:51:28,772 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2022-12-31 09:51:28,768 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 09:51:28,777 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2022-12-31 10:51:28,769 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 10:51:28,786 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2022-12-31 11:51:28,769 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 11:51:28,786 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2022-12-31 12:51:28,769 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 12:51:28,773 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2022-12-31 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2022-12-31 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2022-12-31 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2022-12-31 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2022-12-31 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892555 in 900000ms
2022-12-31 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220251 in 900000ms
2022-12-31 12:51:35,290 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2022-12-31 12:51:36,418 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2022-12-31 12:51:36,418 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 9
2022-12-31 12:51:36,418 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2022-12-31 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626875
2022-12-31 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2022-12-31 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892555
2022-12-31 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220251
2022-12-31 13:51:28,769 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 13:51:28,777 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2022-12-31 14:51:28,770 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 14:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2022-12-31 15:51:28,770 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 15:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2022-12-31 16:51:28,770 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 16:51:28,789 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2022-12-31 17:51:28,770 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 17:51:28,775 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2022-12-31 18:51:28,770 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 18:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2022-12-31 19:51:28,770 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 19:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2022-12-31 20:51:28,770 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 20:51:28,773 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2022-12-31 21:51:28,771 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 21:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2022-12-31 22:51:28,771 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 22:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2022-12-31 23:51:28,772 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2022-12-31 23:51:28,781 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-01 00:51:28,772 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 00:51:28,776 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-01 01:51:28,773 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 01:51:28,784 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-01 02:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 02:51:28,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-01-01 03:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 03:51:28,778 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-01 04:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 04:51:28,781 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-01 05:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 05:51:28,780 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-01 06:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 06:51:28,784 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-01 07:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 07:51:28,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-01 08:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 08:51:28,785 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-01 09:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 09:51:28,786 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-01 10:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 10:51:28,779 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-01 11:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 11:51:28,778 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-01 12:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 12:51:28,779 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-01 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-01 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-01 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-01 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892556 in 900000ms
2023-01-01 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-01 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220250 in 900000ms
2023-01-01 12:51:37,730 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-01 12:51:38,893 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-01 12:51:38,893 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 10
2023-01-01 12:51:38,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-01 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892556
2023-01-01 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626876
2023-01-01 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220250
2023-01-01 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-01 13:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 13:51:28,785 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-01 14:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 14:51:28,778 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-01 15:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 15:51:28,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-01 16:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 16:51:28,787 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-01 17:51:28,774 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 17:51:28,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-01 18:51:28,775 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 18:51:28,785 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-01 19:51:28,775 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 19:51:28,778 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-01 20:51:28,776 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 20:51:28,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-01 21:51:28,777 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 21:51:28,781 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-01 22:51:28,777 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 22:51:28,791 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-01 23:51:28,777 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-01 23:51:28,794 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-01-02 00:51:28,777 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 00:51:28,782 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-02 01:51:28,777 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 01:51:28,779 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-02 02:51:28,778 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 02:51:28,791 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-02 03:51:28,778 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 03:51:28,782 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-02 04:51:28,778 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 04:51:28,781 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-02 05:51:28,778 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 05:51:28,790 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-02 06:51:28,778 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 06:51:28,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-02 07:51:28,778 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 07:51:28,782 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-02 08:51:28,779 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 08:51:28,794 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-02 09:51:28,780 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 09:51:28,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-02 10:51:28,780 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 10:51:28,784 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-02 11:51:28,781 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 11:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-02 12:51:28,780 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 12:51:28,791 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-02 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-02 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-02 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-02 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-02 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892557 in 900000ms
2023-01-02 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220249 in 900000ms
2023-01-02 12:51:40,171 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-02 12:51:41,248 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-02 12:51:41,248 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 11
2023-01-02 12:51:41,248 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-02 13:06:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626877
2023-01-02 13:06:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-02 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892557
2023-01-02 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220249
2023-01-02 13:51:28,780 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 13:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-02 14:51:28,781 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 14:51:28,784 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-02 15:51:28,781 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 15:51:28,789 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-02 16:51:28,782 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 16:51:28,786 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-02 17:51:28,782 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 17:51:28,786 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-02 18:51:28,782 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 18:51:28,794 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-02 19:51:28,783 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 19:51:28,787 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-02 20:51:28,783 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 20:51:28,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-02 21:51:28,783 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 21:51:28,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-02 22:51:28,783 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 22:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-02 23:51:28,783 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-02 23:51:28,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-03 00:51:28,783 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 00:51:28,787 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-03 01:51:28,783 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 01:51:28,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-03 02:51:28,783 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 02:51:28,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-03 03:51:28,784 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 03:51:28,787 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-03 04:51:28,784 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 04:51:28,797 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-03 05:51:28,784 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 05:51:28,801 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-03 06:51:28,784 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 06:51:28,803 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-01-03 07:51:28,784 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 07:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-03 08:51:28,785 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 08:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-03 09:51:28,785 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 09:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-01-03 10:51:28,785 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 10:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-03 11:51:28,785 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 11:51:28,791 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-03 12:51:28,785 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 12:51:28,791 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-03 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-03 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-03 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-03 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892558 in 900000ms
2023-01-03 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-03 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220248 in 900000ms
2023-01-03 12:51:42,599 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-03 12:51:43,718 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-03 12:51:43,718 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 12
2023-01-03 12:51:43,718 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-03 13:06:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626878
2023-01-03 13:06:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-03 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892558
2023-01-03 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220248
2023-01-03 13:51:28,785 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 13:51:28,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-01-03 14:51:28,785 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 14:51:28,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-03 15:51:28,785 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 15:51:28,789 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-03 16:51:28,786 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 16:51:28,794 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-03 17:51:28,786 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 17:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-03 18:51:28,786 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 18:51:28,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-01-03 19:51:28,787 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 19:51:28,791 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-03 20:51:28,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 20:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-03 21:51:28,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 21:51:28,791 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-03 22:51:28,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 22:51:28,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-01-03 23:51:28,789 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-03 23:51:28,793 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-04 00:51:28,789 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 00:51:28,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-04 01:51:28,789 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 01:51:28,794 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-04 02:51:28,789 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 02:51:28,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-04 03:51:28,789 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 03:51:28,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-04 04:51:28,790 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 04:51:28,793 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-04 05:51:28,790 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 05:51:28,800 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-04 06:51:28,790 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 06:51:28,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-04 07:51:28,790 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 07:51:28,793 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-04 08:51:28,790 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 08:51:28,794 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-04 09:51:28,790 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 09:51:28,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-04 10:51:28,791 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 10:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-01-04 11:51:28,791 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 11:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-04 12:51:28,791 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 12:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-01-04 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-04 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-04 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-04 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892559 in 900000ms
2023-01-04 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-04 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220247 in 900000ms
2023-01-04 12:51:45,019 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-04 12:51:46,123 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 5
2023-01-04 12:51:46,123 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-04 12:51:46,123 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-04 12:51:46,123 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 13
2023-01-04 12:51:46,123 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-04 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626879
2023-01-04 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-04 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892559
2023-01-04 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220247
2023-01-04 13:51:28,791 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 13:51:28,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-04 14:51:28,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 14:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-04 15:51:28,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 15:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-04 16:51:28,793 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 16:51:28,829 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 37 msec
2023-01-04 17:51:28,793 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 17:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-04 18:51:28,793 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 18:51:28,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-04 19:51:28,794 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 19:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-04 20:51:28,794 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 20:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-04 21:51:28,794 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 21:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-04 22:51:28,794 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 22:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-04 23:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-04 23:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-05 00:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 00:51:28,839 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 44 msec
2023-01-05 01:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 01:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-05 02:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 02:51:28,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-05 03:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 03:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-05 04:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 04:51:28,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-05 05:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 05:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-05 06:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 06:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-05 07:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 07:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-05 08:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 08:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-05 09:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 09:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-05 10:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 10:51:28,800 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-05 11:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 11:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-05 12:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 12:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-05 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-05 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-05 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-05 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220246 in 900000ms
2023-01-05 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-05 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892560 in 900000ms
2023-01-05 12:51:47,402 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-05 12:51:48,570 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 6
2023-01-05 12:51:48,570 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-05 12:51:48,570 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-05 12:51:48,570 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 14
2023-01-05 12:51:48,570 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-05 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220246
2023-01-05 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626880
2023-01-05 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-05 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892560
2023-01-05 13:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 13:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 38 msec
2023-01-05 14:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 14:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-05 15:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 15:51:28,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-05 16:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 16:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-05 17:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 17:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-05 18:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 18:51:28,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-05 19:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 19:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-05 20:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 20:51:28,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-05 21:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 21:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-05 22:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 22:51:28,803 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-05 23:51:28,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-05 23:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-06 00:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 00:51:28,801 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-06 01:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 01:51:28,803 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-06 02:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 02:51:28,800 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-06 03:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 03:51:28,803 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-01-06 04:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 04:51:28,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-06 05:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 05:51:28,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-06 06:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 06:51:28,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-06 07:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 07:51:28,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 34 msec
2023-01-06 08:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 08:51:28,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-06 09:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 09:51:28,803 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-01-06 10:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 10:51:28,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-06 11:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 11:51:28,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-06 12:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 12:51:28,800 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-06 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-06 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-06 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892561 in 900000ms
2023-01-06 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-06 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-06 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220245 in 900000ms
2023-01-06 12:51:49,839 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-06 12:51:50,902 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 7
2023-01-06 12:51:50,902 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-06 12:51:50,902 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-06 12:51:50,902 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 15
2023-01-06 12:51:50,902 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-06 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892561
2023-01-06 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626881
2023-01-06 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-06 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220245
2023-01-06 13:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 13:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-06 14:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 14:51:28,800 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-06 15:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 15:51:28,800 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-06 16:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 16:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-06 17:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 17:51:28,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-01-06 18:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 18:51:28,800 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-06 19:51:28,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 19:51:28,800 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-06 20:51:28,797 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 20:51:28,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-06 21:51:28,797 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 21:51:28,801 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-06 22:51:28,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 22:51:28,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-06 23:51:28,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-06 23:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-07 00:51:28,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 00:51:28,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-07 01:51:28,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 01:51:28,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-01-07 02:51:28,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 02:51:28,812 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-07 03:51:28,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 03:51:28,803 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-07 04:51:28,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 04:51:28,801 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-07 05:51:28,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 05:51:28,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-07 06:51:28,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 06:51:28,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-07 07:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 07:51:28,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-07 08:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 08:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-07 09:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 09:51:28,803 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-07 10:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 10:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-07 11:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 11:51:28,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-07 12:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 12:51:28,803 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-07 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-07 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-07 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-07 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220244 in 900000ms
2023-01-07 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-07 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892562 in 900000ms
2023-01-07 12:51:52,239 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-07 12:51:53,362 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 8
2023-01-07 12:51:53,363 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-07 12:51:53,363 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-07 12:51:53,363 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 16
2023-01-07 12:51:53,363 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-07 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626882
2023-01-07 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220244
2023-01-07 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-07 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892562
2023-01-07 13:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 13:51:28,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-07 14:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 14:51:28,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-07 15:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 15:51:28,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-07 16:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 16:51:28,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-07 17:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 17:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-07 18:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 18:51:28,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-07 19:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 19:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-07 20:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 20:51:28,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-01-07 21:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 21:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-07 22:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 22:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-07 23:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-07 23:51:28,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-01-08 00:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 00:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-08 01:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 01:51:28,803 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-08 02:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 02:51:28,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-08 03:51:28,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 03:51:28,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-08 04:51:28,800 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 04:51:28,812 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-08 05:51:28,801 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 05:51:28,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-08 06:51:28,801 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 06:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-08 07:51:28,801 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 07:51:28,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-08 08:51:28,801 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 08:51:28,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-08 09:51:28,801 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 09:51:28,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-08 10:51:28,801 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 10:51:28,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-08 11:51:28,801 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 11:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-08 12:51:28,801 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 12:51:28,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-08 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-08 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-08 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-08 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220243 in 900000ms
2023-01-08 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-08 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892563 in 900000ms
2023-01-08 12:51:54,692 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-08 12:51:55,730 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 9
2023-01-08 12:51:55,730 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-08 12:51:55,730 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-08 12:51:55,730 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 17
2023-01-08 12:51:55,730 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-08 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626883
2023-01-08 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-08 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220243
2023-01-08 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892563
2023-01-08 13:51:28,801 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 13:51:28,823 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-01-08 14:51:28,801 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 14:51:28,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-08 15:51:28,801 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 15:51:28,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-08 16:51:28,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 16:51:28,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-08 17:51:28,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 17:51:28,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-08 18:51:28,803 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 18:51:28,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-01-08 19:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 19:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-08 20:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 20:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-08 21:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 21:51:28,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-08 22:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 22:51:28,822 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-01-08 23:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-08 23:51:28,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-09 00:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 00:51:28,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-09 01:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 01:51:28,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-01-09 02:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 02:51:28,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-09 03:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 03:51:28,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-09 04:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 04:51:28,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-09 05:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 05:51:28,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-09 06:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 06:51:28,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-09 07:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 07:51:28,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-09 08:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 08:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-09 09:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 09:51:28,812 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-01-09 10:51:28,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 10:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-09 11:51:28,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 11:51:28,823 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-01-09 12:51:28,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 12:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-09 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-09 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-09 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-09 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892564 in 900000ms
2023-01-09 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-09 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220242 in 900000ms
2023-01-09 12:51:57,085 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-09 12:51:58,190 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 10
2023-01-09 12:51:58,190 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-09 12:51:58,190 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-09 12:51:58,190 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 18
2023-01-09 12:51:58,190 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-09 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626884
2023-01-09 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892564
2023-01-09 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-09 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220242
2023-01-09 13:51:28,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 13:51:28,840 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 35 msec
2023-01-09 14:51:28,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 14:51:28,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-09 15:51:28,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 15:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-09 16:51:28,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 16:51:28,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-09 17:51:28,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 17:51:28,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-09 18:51:28,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 18:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-09 19:51:28,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 19:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-09 20:51:28,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 20:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-09 21:51:28,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 21:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 2 msec
2023-01-09 22:51:28,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 22:51:28,824 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-01-09 23:51:28,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-09 23:51:28,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-10 00:51:28,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 00:51:28,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-10 01:51:28,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 01:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-10 02:51:28,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 02:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-10 03:51:28,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 03:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-10 04:51:28,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 04:51:28,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-10 05:51:28,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 05:51:28,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-10 06:51:28,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 06:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-10 07:51:28,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 07:51:28,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-10 08:51:28,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 08:51:28,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-10 09:51:28,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 09:51:28,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-10 10:51:28,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 10:51:28,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-10 11:51:28,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 11:51:28,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-10 12:51:28,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 12:51:28,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-10 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-10 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-10 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-10 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220241 in 900000ms
2023-01-10 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-10 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892565 in 900000ms
2023-01-10 12:51:59,541 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-10 12:52:00,565 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 11
2023-01-10 12:52:00,565 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-10 12:52:00,565 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-10 12:52:00,565 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 19
2023-01-10 12:52:00,565 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-10 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626885
2023-01-10 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-10 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220241
2023-01-10 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892565
2023-01-10 13:51:28,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 13:51:28,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-10 14:51:28,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 14:51:28,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-10 15:51:28,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 15:51:28,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-10 16:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 16:51:28,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-01-10 17:51:28,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 17:51:28,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 26 msec
2023-01-10 18:51:28,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 18:51:28,826 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-01-10 19:51:28,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 19:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-10 20:51:28,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 20:51:28,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-01-10 21:51:28,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 21:51:28,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-10 22:51:28,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 22:51:28,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-10 23:51:28,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-10 23:51:28,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-11 00:51:28,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 00:51:28,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-11 01:51:28,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 01:51:28,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-11 02:51:28,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 02:51:28,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-01-11 03:51:28,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 03:51:28,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-11 04:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 04:51:28,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-11 05:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 05:51:28,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-11 06:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 06:51:28,848 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 40 msec
2023-01-11 07:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 07:51:28,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-11 08:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 08:51:28,812 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-11 09:51:28,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 09:51:28,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-11 10:51:28,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 10:51:28,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-01-11 11:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 11:51:28,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-11 12:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 12:51:28,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-11 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-11 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-11 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-11 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-11 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220240 in 900000ms
2023-01-11 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892566 in 900000ms
2023-01-11 12:52:01,839 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-11 12:52:03,008 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 12
2023-01-11 12:52:03,008 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-11 12:52:03,009 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-11 12:52:03,009 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 20
2023-01-11 12:52:03,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-11 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626886
2023-01-11 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-11 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892566
2023-01-11 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220240
2023-01-11 13:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 13:51:28,942 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 131 msec
2023-01-11 14:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 14:51:28,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-11 15:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 15:51:28,824 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-11 16:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 16:51:28,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-11 17:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 17:51:28,824 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-11 18:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 18:51:28,823 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-11 19:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 19:51:28,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-11 20:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 20:51:28,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 31 msec
2023-01-11 21:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 21:51:28,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-11 22:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 22:51:28,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-11 23:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-11 23:51:28,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-12 00:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 00:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-12 01:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 01:51:28,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-12 02:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 02:51:28,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-12 03:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 03:51:28,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-12 04:51:28,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 04:51:28,822 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-12 05:51:28,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 05:51:28,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-01-12 06:51:28,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 06:51:28,824 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-12 07:51:28,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 07:51:28,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-01-12 08:51:28,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 08:51:28,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-12 09:51:28,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 09:51:28,858 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 46 msec
2023-01-12 10:51:28,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 10:51:28,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-01-12 11:51:28,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 11:51:28,831 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-01-12 12:51:28,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 12:51:28,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-12 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-12 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-12 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-12 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892567 in 900000ms
2023-01-12 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-12 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220239 in 900000ms
2023-01-12 12:52:04,277 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-12 12:52:05,399 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 13
2023-01-12 12:52:05,399 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-12 12:52:05,399 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-12 12:52:05,399 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 21
2023-01-12 12:52:05,399 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-12 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626887
2023-01-12 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-12 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892567
2023-01-12 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220239
2023-01-12 13:51:28,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 13:51:28,831 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-01-12 14:51:28,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 14:51:28,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-01-12 15:51:28,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 15:51:28,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-12 16:51:28,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 16:51:28,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-12 17:51:28,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 17:51:28,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-12 18:51:28,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 18:51:28,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-12 19:51:28,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 19:51:28,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-12 20:51:28,812 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 20:51:28,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-12 21:51:28,812 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 21:51:28,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-12 22:51:28,812 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 22:51:28,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-12 23:51:28,812 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-12 23:51:28,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-13 00:51:28,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 00:51:28,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-13 01:51:28,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 01:51:28,826 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-13 02:51:28,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 02:51:28,824 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-13 03:51:28,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 03:51:28,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-13 04:51:28,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 04:51:28,823 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-13 05:51:28,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 05:51:28,836 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-01-13 06:51:28,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 06:51:28,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 27 msec
2023-01-13 07:51:28,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 07:51:28,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-13 08:51:28,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 08:51:28,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-01-13 09:51:28,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 09:51:28,823 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-13 10:51:28,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 10:51:28,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-13 11:51:28,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 11:51:28,831 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-13 12:51:28,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 12:51:28,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-13 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-13 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-13 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-13 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892568 in 900000ms
2023-01-13 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-13 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220238 in 900000ms
2023-01-13 12:52:06,649 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-13 12:52:07,757 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 14
2023-01-13 12:52:07,757 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-13 12:52:07,757 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-13 12:52:07,757 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 22
2023-01-13 12:52:07,757 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-13 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626888
2023-01-13 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-13 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892568
2023-01-13 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220238
2023-01-13 13:51:28,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 13:51:28,824 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-13 14:51:28,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 14:51:28,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-13 15:51:28,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 15:51:28,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-13 16:51:28,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 16:51:28,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-13 17:51:28,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 17:51:28,822 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-13 18:51:28,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 18:51:28,824 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-13 19:51:28,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 19:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 27 msec
2023-01-13 20:51:28,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 20:51:28,836 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-01-13 21:51:28,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 21:51:28,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-13 22:51:28,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 22:51:28,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-13 23:51:28,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-13 23:51:28,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-14 00:51:28,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 00:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-14 01:51:28,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 01:51:28,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-14 02:51:28,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 02:51:28,826 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-14 03:51:28,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 03:51:28,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-14 04:51:28,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 04:51:28,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-14 05:51:28,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 05:51:28,826 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-14 06:51:28,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 06:51:28,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-14 07:51:28,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 07:51:28,829 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-14 08:51:28,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 08:51:28,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-14 09:51:28,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 09:51:28,824 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-14 10:51:28,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 10:51:28,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-14 11:51:28,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 11:51:28,829 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-14 12:51:28,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 12:51:28,826 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-14 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-14 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-14 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-14 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220237 in 900000ms
2023-01-14 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-14 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892569 in 900000ms
2023-01-14 12:52:09,110 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-14 12:52:10,163 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 15
2023-01-14 12:52:10,163 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-14 12:52:10,163 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-14 12:52:10,163 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 23
2023-01-14 12:52:10,163 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-14 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626889
2023-01-14 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220237
2023-01-14 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-14 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892569
2023-01-14 13:51:28,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 13:51:28,839 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-01-14 14:51:28,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 14:51:28,835 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-01-14 15:51:28,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 15:51:28,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-14 16:51:28,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 16:51:28,831 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-14 17:51:28,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 17:51:28,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-14 18:51:28,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 18:51:28,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-14 19:51:28,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 19:51:28,822 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-14 20:51:28,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 20:51:28,822 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-14 21:51:28,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 21:51:28,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-14 22:51:28,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 22:51:28,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-14 23:51:28,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-14 23:51:28,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-15 00:51:28,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 00:51:28,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-15 01:51:28,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 01:51:28,838 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-01-15 02:51:28,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 02:51:28,822 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-15 03:51:28,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 03:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-15 04:51:28,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 04:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-15 05:51:28,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 05:51:28,831 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-15 06:51:28,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 06:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-01-15 07:51:28,822 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 07:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-01-15 08:51:28,823 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 08:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-15 09:51:28,823 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 09:51:28,829 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-15 10:51:28,823 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 10:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-15 11:51:28,823 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 11:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-15 12:51:28,823 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 12:51:28,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-15 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-15 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-15 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-15 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220236 in 900000ms
2023-01-15 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-15 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892570 in 900000ms
2023-01-15 12:52:11,441 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-15 12:52:12,565 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 16
2023-01-15 12:52:12,565 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-15 12:52:12,565 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-15 12:52:12,566 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 24
2023-01-15 12:52:12,566 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-15 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626890
2023-01-15 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-15 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220236
2023-01-15 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892570
2023-01-15 13:51:28,824 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 13:51:28,844 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-01-15 14:51:28,824 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 14:51:28,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-15 15:51:28,824 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 15:51:28,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-15 16:51:28,824 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 16:51:28,838 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-15 17:51:28,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 17:51:28,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-15 18:51:28,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 18:51:28,829 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-15 19:51:28,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 19:51:28,829 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-15 20:51:28,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 20:51:28,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-15 21:51:28,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 21:51:28,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-15 22:51:28,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 22:51:28,836 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-15 23:51:28,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-15 23:51:28,843 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-01-16 00:51:28,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 00:51:28,831 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-16 01:51:28,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 01:51:28,837 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-16 02:51:28,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 02:51:28,838 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-16 03:51:28,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 03:51:28,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 26 msec
2023-01-16 04:51:28,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 04:51:28,846 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-01-16 05:51:28,826 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 05:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-16 06:51:28,826 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 06:51:28,829 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-16 07:51:28,826 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 07:51:28,840 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-16 08:51:28,826 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 08:51:28,831 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-16 09:51:28,826 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 09:51:28,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-16 10:51:28,826 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 10:51:28,829 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-16 11:51:28,826 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 11:51:28,839 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-16 12:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 12:51:28,846 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-01-16 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-16 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-16 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-16 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892571 in 900000ms
2023-01-16 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-16 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220235 in 900000ms
2023-01-16 12:52:13,884 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-16 12:52:15,009 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 17
2023-01-16 12:52:15,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-16 12:52:15,009 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-16 12:52:15,009 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 25
2023-01-16 12:52:15,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-16 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626891
2023-01-16 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-16 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892571
2023-01-16 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220235
2023-01-16 13:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 13:51:28,840 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-16 14:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 14:51:28,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-16 15:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 15:51:28,836 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-16 16:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 16:51:28,831 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-16 17:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 17:51:28,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-16 18:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 18:51:28,836 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-16 19:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 19:51:28,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-16 20:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 20:51:28,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-16 21:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 21:51:28,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-16 22:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 22:51:28,831 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-16 23:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-16 23:51:28,836 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-17 00:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 00:51:28,863 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 36 msec
2023-01-17 01:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 01:51:28,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-17 02:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 02:51:28,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-17 03:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 03:51:28,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-17 04:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 04:51:28,831 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-17 05:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 05:51:28,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-17 06:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 06:51:28,839 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-17 07:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 07:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-17 08:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 08:51:28,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-17 09:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 09:51:28,836 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-17 10:51:28,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 10:51:28,831 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-17 11:51:28,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 11:51:28,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-17 12:51:28,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 12:51:28,840 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-17 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-17 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-17 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-17 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892572 in 900000ms
2023-01-17 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-17 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220234 in 900000ms
2023-01-17 12:52:16,287 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-17 12:52:17,385 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 18
2023-01-17 12:52:17,386 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-17 12:52:17,386 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-17 12:52:17,386 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 26
2023-01-17 12:52:17,386 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-17 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626892
2023-01-17 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-17 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892572
2023-01-17 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220234
2023-01-17 13:51:28,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 13:51:28,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-01-17 14:51:28,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 14:51:28,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-17 15:51:28,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 15:51:28,840 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-17 16:51:28,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 16:51:28,838 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-17 17:51:28,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 17:51:28,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-17 18:51:28,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 18:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-17 19:51:28,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 19:51:28,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-17 20:51:28,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 20:51:28,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-17 21:51:28,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 21:51:28,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-17 22:51:28,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 22:51:28,836 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-01-17 23:51:28,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-17 23:51:28,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-18 00:51:28,829 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 00:51:28,848 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-01-18 01:51:28,829 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 01:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-01-18 02:51:28,829 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 02:51:28,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-18 03:51:28,829 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 03:51:28,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-18 04:51:28,829 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 04:51:28,839 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-18 05:51:28,829 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 05:51:28,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-18 06:51:28,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 06:51:28,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-18 07:51:28,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 07:51:28,840 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-18 08:51:28,831 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 08:51:28,852 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-01-18 09:51:28,831 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 09:51:28,835 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-18 10:51:28,831 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 10:51:28,836 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-18 11:51:28,831 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 11:51:28,861 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 30 msec
2023-01-18 12:51:28,831 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 12:51:28,849 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-01-18 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-18 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-18 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-18 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220233 in 900000ms
2023-01-18 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-18 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892573 in 900000ms
2023-01-18 12:52:18,732 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-18 12:52:19,821 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 19
2023-01-18 12:52:19,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-18 12:52:19,821 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-18 12:52:19,821 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 27
2023-01-18 12:52:19,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-18 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626893
2023-01-18 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-18 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220233
2023-01-18 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892573
2023-01-18 13:51:28,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 13:51:28,845 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-18 14:51:28,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 14:51:28,849 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-18 15:51:28,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 15:51:28,837 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-18 16:51:28,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 16:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-18 17:51:28,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 17:51:28,837 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-18 18:51:28,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 18:51:28,836 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-18 19:51:28,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 19:51:28,836 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-18 20:51:28,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 20:51:28,837 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-18 21:51:28,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 21:51:28,837 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-18 22:51:28,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 22:51:28,843 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-18 23:51:28,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-18 23:51:28,846 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-19 00:51:28,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 00:51:28,837 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-19 01:51:28,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 01:51:28,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-19 02:51:28,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 02:51:28,868 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 34 msec
2023-01-19 03:51:28,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 03:51:28,837 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-19 04:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 04:51:28,838 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-19 05:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 05:51:28,857 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-01-19 06:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 06:51:28,846 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-19 07:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 07:51:28,844 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-19 08:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 08:51:28,839 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-19 09:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 09:51:28,854 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-01-19 10:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 10:51:28,840 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-01-19 11:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 11:51:28,843 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-19 12:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 12:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-19 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-19 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-19 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-19 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220232 in 900000ms
2023-01-19 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-19 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892574 in 900000ms
2023-01-19 12:52:21,078 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-19 12:52:22,227 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 20
2023-01-19 12:52:22,227 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-19 12:52:22,227 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-19 12:52:22,227 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 28
2023-01-19 12:52:22,227 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-19 13:06:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626894
2023-01-19 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-19 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220232
2023-01-19 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892574
2023-01-19 13:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 13:51:28,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-19 14:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 14:51:28,843 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-19 15:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 15:51:28,856 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-01-19 16:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 16:51:28,843 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-19 17:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 17:51:28,854 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-01-19 18:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 18:51:28,844 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-19 19:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 19:51:28,838 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-19 20:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 20:51:28,843 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-19 21:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 21:51:28,839 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-19 22:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 22:51:28,869 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 35 msec
2023-01-19 23:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-19 23:51:28,839 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-20 00:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 00:51:28,848 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-20 01:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 01:51:28,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-20 02:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 02:51:28,847 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-20 03:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 03:51:28,858 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-01-20 04:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 04:51:28,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-20 05:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 05:51:28,838 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-20 06:51:28,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 06:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-20 07:51:28,835 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 07:51:28,864 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 29 msec
2023-01-20 08:51:28,835 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 08:51:28,839 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-20 09:51:28,835 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 09:51:28,848 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-20 10:51:28,835 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 10:51:28,855 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-01-20 11:51:28,835 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 11:51:28,840 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-20 12:51:28,836 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 12:51:28,849 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-20 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-20 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-20 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-20 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892575 in 900000ms
2023-01-20 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-20 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220231 in 900000ms
2023-01-20 12:52:23,496 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-20 12:52:24,668 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 21
2023-01-20 12:52:24,668 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-20 12:52:24,668 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-20 12:52:24,668 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 29
2023-01-20 12:52:24,669 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-20 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626895
2023-01-20 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-20 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892575
2023-01-20 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220231
2023-01-20 13:51:28,836 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 13:51:28,859 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-01-20 14:51:28,836 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 14:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-20 15:51:28,836 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 15:51:28,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-20 16:51:28,837 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 16:51:28,859 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-01-20 17:51:28,838 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 17:51:28,843 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-20 18:51:28,838 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 18:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-20 19:51:28,838 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 19:51:28,844 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-20 20:51:28,839 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 20:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-20 21:51:28,840 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 21:51:28,844 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-20 22:51:28,840 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 22:51:28,854 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-20 23:51:28,840 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-20 23:51:28,858 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-01-21 00:51:28,840 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 00:51:28,848 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-21 01:51:28,840 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 01:51:28,857 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-21 02:51:28,840 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 02:51:28,855 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-21 03:51:28,840 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 03:51:28,844 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-21 04:51:28,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 04:51:28,856 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-21 05:51:28,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 05:51:28,845 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-21 06:51:28,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 06:51:28,845 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-21 07:51:28,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 07:51:28,859 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-01-21 08:51:28,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 08:51:28,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-21 09:51:28,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 09:51:28,847 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-21 10:51:28,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 10:51:28,856 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-21 11:51:28,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 11:51:28,845 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-21 12:51:28,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 12:51:28,855 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-21 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-21 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-21 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-21 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892576 in 900000ms
2023-01-21 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-21 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220230 in 900000ms
2023-01-21 12:52:25,866 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-21 12:52:27,003 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 22
2023-01-21 12:52:27,003 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-21 12:52:27,004 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-21 12:52:27,004 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 30
2023-01-21 12:52:27,004 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-21 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626896
2023-01-21 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-21 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892576
2023-01-21 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220230
2023-01-21 13:51:28,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 13:51:28,861 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-01-21 14:51:28,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 14:51:28,856 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-21 15:51:28,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 15:51:28,855 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-21 16:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 16:51:28,857 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-21 17:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 17:51:28,863 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-01-21 18:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 18:51:28,845 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-21 19:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 19:51:28,846 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-21 20:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 20:51:28,864 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-01-21 21:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 21:51:28,883 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 41 msec
2023-01-21 22:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 22:51:28,845 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-21 23:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-21 23:51:28,846 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-22 00:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 00:51:28,891 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 48 msec
2023-01-22 01:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 01:51:28,845 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-22 02:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 02:51:28,856 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-22 03:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 03:51:28,855 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-22 04:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 04:51:28,846 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-22 05:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 05:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-22 06:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 06:51:28,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-22 07:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 07:51:28,846 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-22 08:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 08:51:28,860 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-01-22 09:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 09:51:28,847 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-22 10:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 10:51:28,854 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-22 11:51:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 11:51:28,857 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-22 12:51:28,843 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 12:51:28,847 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-22 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-22 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-22 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-22 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-22 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220229 in 900000ms
2023-01-22 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892577 in 900000ms
2023-01-22 12:52:28,296 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-22 12:52:29,435 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 23
2023-01-22 12:52:29,435 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-22 12:52:29,435 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-22 12:52:29,435 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 31
2023-01-22 12:52:29,435 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-22 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892577
2023-01-22 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626897
2023-01-22 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220229
2023-01-22 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-22 13:51:28,843 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 13:51:28,867 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-01-22 14:51:28,843 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 14:51:28,846 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-22 15:51:28,843 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 15:51:28,849 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-22 16:51:28,843 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 16:51:28,847 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-22 17:51:28,843 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 17:51:28,852 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-22 18:51:28,843 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 18:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-22 19:51:28,843 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 19:51:28,868 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-01-22 20:51:28,843 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 20:51:28,858 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-22 21:51:28,843 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 21:51:28,855 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-22 22:51:28,843 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 22:51:28,858 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-22 23:51:28,843 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-22 23:51:28,860 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-01-23 00:51:28,844 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 00:51:28,855 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-23 01:51:28,844 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 01:51:28,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-23 02:51:28,844 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 02:51:28,847 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-23 03:51:28,844 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 03:51:28,859 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-23 04:51:28,844 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 04:51:28,863 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-01-23 05:51:28,844 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 05:51:28,847 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-23 06:51:28,845 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 06:51:28,868 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-01-23 07:51:28,845 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 07:51:28,857 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-23 08:51:28,845 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 08:51:28,858 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-23 09:31:27,848 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2023-01-23 09:51:28,845 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 09:51:28,858 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-23 10:51:28,845 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 10:51:28,866 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-01-23 11:51:28,846 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 11:51:28,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-23 12:02:20,810 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2023-01-23 12:02:45,517 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://adh-test-00-mnode1.ru-central1.internal:19888/jobhistory/job/job_1671022279734_0002 which is the app master GUI of application_1671022279734_0002 owned by hdfs
2023-01-23 12:02:54,352 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://adh-test-00-mnode1.ru-central1.internal:19888/jobhistory/job/job_1672231888656_0002 which is the app master GUI of application_1672231888656_0002 owned by student48
2023-01-23 12:04:26,614 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2023-01-23 12:51:28,846 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 12:51:28,864 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-01-23 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-23 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-23 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-23 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-23 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892578 in 900000ms
2023-01-23 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220228 in 900000ms
2023-01-23 12:52:30,646 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-23 12:52:31,851 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 24
2023-01-23 12:52:31,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-23 12:52:31,851 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-23 12:52:31,851 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 32
2023-01-23 12:52:31,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-23 13:06:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626898
2023-01-23 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-23 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220228
2023-01-23 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892578
2023-01-23 13:51:28,846 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 13:51:28,865 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-01-23 14:51:28,846 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 14:51:28,862 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-23 15:51:28,847 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 15:51:28,858 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-23 16:51:28,847 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 16:51:28,868 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-01-23 17:51:28,847 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 17:51:28,873 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 26 msec
2023-01-23 18:51:28,847 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 18:51:28,857 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-23 19:51:28,847 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 19:51:28,864 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-01-23 20:51:28,847 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 20:51:28,858 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-23 21:51:28,848 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 21:51:28,864 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-01-23 22:51:28,848 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 22:51:28,865 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-23 23:51:28,848 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-23 23:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-24 00:51:28,848 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 00:51:28,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-24 01:51:28,848 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 01:51:28,878 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 30 msec
2023-01-24 02:51:28,849 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 02:51:28,854 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-24 03:51:28,849 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 03:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-24 04:51:28,849 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 04:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-24 05:51:28,849 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 05:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-24 06:51:28,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 06:51:28,854 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-24 07:51:28,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 07:51:28,855 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-24 08:51:28,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 08:51:28,863 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-24 09:51:28,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 09:51:28,857 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-01-24 10:51:28,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 10:51:28,872 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-01-24 11:51:28,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 11:51:28,868 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-01-24 12:51:28,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 12:51:28,865 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-24 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-24 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-24 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-24 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-24 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220227 in 900000ms
2023-01-24 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892579 in 900000ms
2023-01-24 12:52:33,062 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-24 12:52:34,263 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 25
2023-01-24 12:52:34,263 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-24 12:52:34,263 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-24 12:52:34,263 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 33
2023-01-24 12:52:34,263 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-24 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626899
2023-01-24 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-24 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220227
2023-01-24 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892579
2023-01-24 13:08:01,916 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2023-01-24 13:09:04,347 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://adh-test-00-mnode1.ru-central1.internal:19888/jobhistory/job/job_1672231888656_0002 which is the app master GUI of application_1672231888656_0002 owned by student48
2023-01-24 13:09:34,490 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://adh-test-00-mnode1.ru-central1.internal:19888/jobhistory/job/job_1672231888656_0002 which is the app master GUI of application_1672231888656_0002 owned by student48
2023-01-24 13:24:44,320 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2023-01-24 13:51:28,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 13:51:28,871 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-01-24 14:03:41,556 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2023-01-24 14:03:46,756 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2023-01-24 14:13:24,030 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 3
2023-01-24 14:14:36,711 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 4
2023-01-24 14:15:01,195 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 5
2023-01-24 14:15:01,970 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1672231888656_0005' is submitted without priority hence considering default queue/cluster priority: 0
2023-01-24 14:15:01,970 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1672231888656_0005
2023-01-24 14:15:01,970 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 5 submitted by user student48
2023-01-24 14:15:01,970 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	IP=10.130.0.4	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1672231888656_0005	QUEUENAME=default
2023-01-24 14:15:01,971 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1672231888656_0005
2023-01-24 14:15:01,971 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1672231888656_0005
2023-01-24 14:15:01,971 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0005 State change from NEW to NEW_SAVING on event = START
2023-01-24 14:15:01,972 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0005 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2023-01-24 14:15:01,972 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1672231888656_0005 user: student48 leaf-queue of parent: root #applications: 1
2023-01-24 14:15:01,973 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1672231888656_0005 from user: student48, in queue: default
2023-01-24 14:15:01,973 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0005 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2023-01-24 14:15:01,973 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1672231888656_0005_000001
2023-01-24 14:15:01,973 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0005_000001 State change from NEW to SUBMITTED on event = START
2023-01-24 14:15:01,974 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1672231888656_0005 from user: student48 activated in queue: default
2023-01-24 14:15:01,974 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1672231888656_0005 user: student48, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-01-24 14:15:01,974 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1672231888656_0005_000001 to scheduler from user student48 in queue default
2023-01-24 14:15:01,975 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0005_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-01-24 14:15:02,216 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0005_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-01-24 14:15:02,217 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0005_01_000001 Container Transitioned from NEW to ALLOCATED
2023-01-24 14:15:02,217 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0005_01_000001 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2023-01-24 14:15:02,217 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0005	CONTAINERID=container_e02_1672231888656_0005_01_000001	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-01-24 14:15:02,219 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e02_1672231888656_0005_01_000001
2023-01-24 14:15:02,222 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0005_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-01-24 14:15:02,222 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1672231888656_0005_000001
2023-01-24 14:15:02,222 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1672231888656_0005 AttemptId: appattempt_1672231888656_0005_000001 MasterContainer: Container: [ContainerId: container_e02_1672231888656_0005_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ]
2023-01-24 14:15:02,223 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0005_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-01-24 14:15:02,223 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.0967361 absoluteUsedCapacity=0.0967361 used=<memory:2048, vCores:1> cluster=<memory:21171, vCores:6>
2023-01-24 14:15:02,225 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-01-24 14:15:02,225 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0005_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-01-24 14:15:02,228 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1672231888656_0005_000001
2023-01-24 14:15:02,230 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e02_1672231888656_0005_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1672231888656_0005_000001
2023-01-24 14:15:02,230 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1672231888656_0005_000001
2023-01-24 14:15:02,231 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1672231888656_0005_000001
2023-01-24 14:15:02,694 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e02_1672231888656_0005_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1672231888656_0005_000001
2023-01-24 14:15:02,694 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0005_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-01-24 14:15:03,230 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0005_01_000001 Container Transitioned from ACQUIRED to RUNNING
2023-01-24 14:15:10,504 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1672231888656_0005_000001 (auth:SIMPLE)
2023-01-24 14:15:10,511 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1672231888656_0005_000001
2023-01-24 14:15:10,511 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	IP=10.130.0.5	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1672231888656_0005	APPATTEMPTID=appattempt_1672231888656_0005_000001
2023-01-24 14:15:10,512 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0005_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-01-24 14:15:10,512 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0005 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-01-24 14:15:12,186 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1672231888656_0005_000001 with final state: FINISHING, and exit status: -1000
2023-01-24 14:15:12,186 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0005_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2023-01-24 14:15:12,186 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1672231888656_0005 with final state: FINISHING
2023-01-24 14:15:12,186 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0005 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2023-01-24 14:15:12,186 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1672231888656_0005
2023-01-24 14:15:12,186 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0005_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2023-01-24 14:15:12,186 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0005 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2023-01-24 14:15:13,188 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1672231888656_0005 unregistered successfully. 
2023-01-24 14:15:18,589 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1672231888656_0005_000001
2023-01-24 14:15:18,589 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0005_01_000001 Container Transitioned from RUNNING to COMPLETED
2023-01-24 14:15:18,589 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1672231888656_0005_000001
2023-01-24 14:15:18,589 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0005	CONTAINERID=container_e02_1672231888656_0005_01_000001	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-01-24 14:15:18,590 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0005_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2023-01-24 14:15:18,591 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0005 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2023-01-24 14:15:18,591 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1672231888656_0005_000001 is done. finalState=FINISHED
2023-01-24 14:15:18,591 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0005
2023-01-24 14:15:18,591 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1672231888656_0005 requests cleared
2023-01-24 14:15:18,591 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1672231888656_0005 user: student48 queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-01-24 14:15:18,591 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1672231888656_0005 user: student48 leaf-queue of parent: root #applications: 0
2023-01-24 14:15:18,591 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1672231888656_0005_000001
2023-01-24 14:15:18,592 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0005,name=word count,user=student48,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0005/,appMasterHost=adh-test-00-mnode2.ru-central1.internal,submitTime=1674569701969,startTime=1674569701970,finishTime=1674569712186,finalStatus=FAILED,memorySeconds=33531,vcoreSeconds=16,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=33531 MB-seconds\, 16 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2023-01-24 14:15:20,898 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2023-01-24 14:16:00,396 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 6
2023-01-24 14:16:01,159 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1672231888656_0006' is submitted without priority hence considering default queue/cluster priority: 0
2023-01-24 14:16:01,159 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1672231888656_0006
2023-01-24 14:16:01,159 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 6 submitted by user student48
2023-01-24 14:16:01,159 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	IP=10.130.0.4	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1672231888656_0006	QUEUENAME=default
2023-01-24 14:16:01,160 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1672231888656_0006
2023-01-24 14:16:01,160 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0006 State change from NEW to NEW_SAVING on event = START
2023-01-24 14:16:01,160 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1672231888656_0006
2023-01-24 14:16:01,161 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0006 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2023-01-24 14:16:01,161 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1672231888656_0006 user: student48 leaf-queue of parent: root #applications: 1
2023-01-24 14:16:01,161 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1672231888656_0006 from user: student48, in queue: default
2023-01-24 14:16:01,161 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0006 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2023-01-24 14:16:01,161 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1672231888656_0006_000001
2023-01-24 14:16:01,161 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0006_000001 State change from NEW to SUBMITTED on event = START
2023-01-24 14:16:01,162 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1672231888656_0006 from user: student48 activated in queue: default
2023-01-24 14:16:01,162 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1672231888656_0006 user: student48, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-01-24 14:16:01,162 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1672231888656_0006_000001 to scheduler from user student48 in queue default
2023-01-24 14:16:01,162 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0006_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-01-24 14:16:01,655 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0006_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-01-24 14:16:01,655 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0006_01_000001 Container Transitioned from NEW to ALLOCATED
2023-01-24 14:16:01,655 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0006_01_000001 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2023-01-24 14:16:01,655 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0006	CONTAINERID=container_e02_1672231888656_0006_01_000001	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-01-24 14:16:01,657 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e02_1672231888656_0006_01_000001
2023-01-24 14:16:01,658 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0006_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-01-24 14:16:01,658 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1672231888656_0006_000001
2023-01-24 14:16:01,658 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1672231888656_0006 AttemptId: appattempt_1672231888656_0006_000001 MasterContainer: Container: [ContainerId: container_e02_1672231888656_0006_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ]
2023-01-24 14:16:01,658 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.0967361 absoluteUsedCapacity=0.0967361 used=<memory:2048, vCores:1> cluster=<memory:21171, vCores:6>
2023-01-24 14:16:01,658 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-01-24 14:16:01,658 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0006_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-01-24 14:16:01,659 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0006_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-01-24 14:16:01,659 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1672231888656_0006_000001
2023-01-24 14:16:01,661 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e02_1672231888656_0006_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1672231888656_0006_000001
2023-01-24 14:16:01,661 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1672231888656_0006_000001
2023-01-24 14:16:01,661 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1672231888656_0006_000001
2023-01-24 14:16:01,678 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e02_1672231888656_0006_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1672231888656_0006_000001
2023-01-24 14:16:01,678 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0006_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-01-24 14:16:02,658 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0006_01_000001 Container Transitioned from ACQUIRED to RUNNING
2023-01-24 14:16:05,558 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1672231888656_0006_000001 (auth:SIMPLE)
2023-01-24 14:16:05,561 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1672231888656_0006_000001
2023-01-24 14:16:05,562 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	IP=10.130.0.5	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1672231888656_0006	APPATTEMPTID=appattempt_1672231888656_0006_000001
2023-01-24 14:16:05,562 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0006_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-01-24 14:16:05,562 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0006 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-01-24 14:16:06,674 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0006_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2023-01-24 14:16:06,675 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0006_01_000002 Container Transitioned from NEW to ALLOCATED
2023-01-24 14:16:06,675 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0006_01_000002 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 2 containers, <memory:3072, vCores:2> used and <memory:3985, vCores:0> available after allocation
2023-01-24 14:16:06,675 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0006	CONTAINERID=container_e02_1672231888656_0006_01_000002	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-01-24 14:16:06,675 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.14510415 absoluteUsedCapacity=0.14510415 used=<memory:3072, vCores:2> cluster=<memory:21171, vCores:6>
2023-01-24 14:16:06,675 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-01-24 14:16:07,629 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e02_1672231888656_0006_01_000002
2023-01-24 14:16:07,630 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0006_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2023-01-24 14:16:08,645 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1672231888656_0006
2023-01-24 14:16:08,681 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0006_01_000002 Container Transitioned from ACQUIRED to RUNNING
2023-01-24 14:16:13,661 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0006_01_000002 Container Transitioned from RUNNING to COMPLETED
2023-01-24 14:16:13,661 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0006	CONTAINERID=container_e02_1672231888656_0006_01_000002	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-01-24 14:16:14,911 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0006_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-01-24 14:16:14,911 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0006_01_000003 Container Transitioned from NEW to ALLOCATED
2023-01-24 14:16:14,911 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0006_01_000003 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-01-24 14:16:14,911 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0006	CONTAINERID=container_e02_1672231888656_0006_01_000003	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-01-24 14:16:14,912 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.14510415 absoluteUsedCapacity=0.14510415 used=<memory:3072, vCores:2> cluster=<memory:21171, vCores:6>
2023-01-24 14:16:14,912 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-01-24 14:16:15,685 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_e02_1672231888656_0006_01_000003
2023-01-24 14:16:15,686 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0006_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2023-01-24 14:16:15,912 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0006_01_000003 Container Transitioned from ACQUIRED to RUNNING
2023-01-24 14:16:16,692 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1672231888656_0006
2023-01-24 14:16:18,485 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1672231888656_0006_000001 with final state: FINISHING, and exit status: -1000
2023-01-24 14:16:18,485 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0006_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2023-01-24 14:16:18,485 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1672231888656_0006 with final state: FINISHING
2023-01-24 14:16:18,485 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0006 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2023-01-24 14:16:18,485 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1672231888656_0006
2023-01-24 14:16:18,485 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0006_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2023-01-24 14:16:18,486 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0006 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2023-01-24 14:16:18,625 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0006_01_000003 Container Transitioned from RUNNING to COMPLETED
2023-01-24 14:16:18,625 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0006	CONTAINERID=container_e02_1672231888656_0006_01_000003	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-01-24 14:16:19,488 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1672231888656_0006 unregistered successfully. 
2023-01-24 14:16:24,870 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1672231888656_0006_000001
2023-01-24 14:16:24,870 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0006_01_000001 Container Transitioned from RUNNING to COMPLETED
2023-01-24 14:16:24,870 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1672231888656_0006_000001
2023-01-24 14:16:24,870 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0006	CONTAINERID=container_e02_1672231888656_0006_01_000001	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-01-24 14:16:24,870 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0006_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2023-01-24 14:16:24,871 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0006 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2023-01-24 14:16:24,871 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0006
2023-01-24 14:16:24,871 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1672231888656_0006_000001 is done. finalState=FINISHED
2023-01-24 14:16:24,871 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1672231888656_0006 requests cleared
2023-01-24 14:16:24,871 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1672231888656_0006_000001
2023-01-24 14:16:24,871 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1672231888656_0006 user: student48 queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-01-24 14:16:24,871 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1672231888656_0006 user: student48 leaf-queue of parent: root #applications: 0
2023-01-24 14:16:24,871 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0006,name=word count,user=student48,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0006/,appMasterHost=adh-test-00-mnode2.ru-central1.internal,submitTime=1674569761159,startTime=1674569761159,finishTime=1674569778485,finalStatus=SUCCEEDED,memorySeconds=58501,vcoreSeconds=32,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=58501 MB-seconds\, 32 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2023-01-24 14:17:19,922 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 7
2023-01-24 14:17:20,580 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1672231888656_0007' is submitted without priority hence considering default queue/cluster priority: 0
2023-01-24 14:17:20,580 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1672231888656_0007
2023-01-24 14:17:20,581 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 7 submitted by user student48
2023-01-24 14:17:20,581 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	IP=10.130.0.4	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1672231888656_0007	QUEUENAME=default
2023-01-24 14:17:20,581 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1672231888656_0007
2023-01-24 14:17:20,581 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1672231888656_0007
2023-01-24 14:17:20,581 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0007 State change from NEW to NEW_SAVING on event = START
2023-01-24 14:17:20,582 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0007 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2023-01-24 14:17:20,582 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1672231888656_0007 user: student48 leaf-queue of parent: root #applications: 1
2023-01-24 14:17:20,582 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1672231888656_0007 from user: student48, in queue: default
2023-01-24 14:17:20,583 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0007 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2023-01-24 14:17:20,583 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1672231888656_0007_000001
2023-01-24 14:17:20,583 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0007_000001 State change from NEW to SUBMITTED on event = START
2023-01-24 14:17:20,583 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1672231888656_0007 from user: student48 activated in queue: default
2023-01-24 14:17:20,583 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1672231888656_0007 user: student48, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-01-24 14:17:20,583 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1672231888656_0007_000001 to scheduler from user student48 in queue default
2023-01-24 14:17:20,584 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0007_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-01-24 14:17:20,709 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0007_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-01-24 14:17:20,709 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0007_01_000001 Container Transitioned from NEW to ALLOCATED
2023-01-24 14:17:20,709 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0007_01_000001 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2023-01-24 14:17:20,709 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0007	CONTAINERID=container_e02_1672231888656_0007_01_000001	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-01-24 14:17:20,710 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_e02_1672231888656_0007_01_000001
2023-01-24 14:17:20,711 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0007_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-01-24 14:17:20,711 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1672231888656_0007_000001
2023-01-24 14:17:20,711 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1672231888656_0007 AttemptId: appattempt_1672231888656_0007_000001 MasterContainer: Container: [ContainerId: container_e02_1672231888656_0007_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ]
2023-01-24 14:17:20,711 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.0967361 absoluteUsedCapacity=0.0967361 used=<memory:2048, vCores:1> cluster=<memory:21171, vCores:6>
2023-01-24 14:17:20,711 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-01-24 14:17:20,712 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0007_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-01-24 14:17:20,713 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0007_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-01-24 14:17:20,713 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1672231888656_0007_000001
2023-01-24 14:17:20,714 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e02_1672231888656_0007_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1672231888656_0007_000001
2023-01-24 14:17:20,714 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1672231888656_0007_000001
2023-01-24 14:17:20,714 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1672231888656_0007_000001
2023-01-24 14:17:20,728 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e02_1672231888656_0007_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1672231888656_0007_000001
2023-01-24 14:17:20,728 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0007_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-01-24 14:17:21,710 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0007_01_000001 Container Transitioned from ACQUIRED to RUNNING
2023-01-24 14:17:24,830 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1672231888656_0007_000001 (auth:SIMPLE)
2023-01-24 14:17:24,834 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1672231888656_0007_000001
2023-01-24 14:17:24,834 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	IP=10.130.0.6	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1672231888656_0007	APPATTEMPTID=appattempt_1672231888656_0007_000001
2023-01-24 14:17:24,834 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0007_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-01-24 14:17:24,834 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0007 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-01-24 14:17:25,954 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0007_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=NODE_LOCAL requestedPartition=
2023-01-24 14:17:25,955 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0007_01_000002 Container Transitioned from NEW to ALLOCATED
2023-01-24 14:17:25,955 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0007_01_000002 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-01-24 14:17:25,955 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0007	CONTAINERID=container_e02_1672231888656_0007_01_000002	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-01-24 14:17:25,955 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.14510415 absoluteUsedCapacity=0.14510415 used=<memory:3072, vCores:2> cluster=<memory:21171, vCores:6>
2023-01-24 14:17:25,955 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-01-24 14:17:26,899 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e02_1672231888656_0007_01_000002
2023-01-24 14:17:26,900 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0007_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2023-01-24 14:17:27,915 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1672231888656_0007
2023-01-24 14:17:27,960 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0007_01_000002 Container Transitioned from ACQUIRED to RUNNING
2023-01-24 14:17:30,266 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0007_01_000002 Container Transitioned from RUNNING to COMPLETED
2023-01-24 14:17:30,266 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0007	CONTAINERID=container_e02_1672231888656_0007_01_000002	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-01-24 14:17:31,025 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0007_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-01-24 14:17:31,026 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0007_01_000003 Container Transitioned from NEW to ALLOCATED
2023-01-24 14:17:31,026 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0007_01_000003 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-01-24 14:17:31,026 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0007	CONTAINERID=container_e02_1672231888656_0007_01_000003	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-01-24 14:17:31,026 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.14510415 absoluteUsedCapacity=0.14510415 used=<memory:3072, vCores:2> cluster=<memory:21171, vCores:6>
2023-01-24 14:17:31,026 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-01-24 14:17:31,939 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode1.ru-central1.internal:8041 for container : container_e02_1672231888656_0007_01_000003
2023-01-24 14:17:31,940 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0007_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2023-01-24 14:17:32,028 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0007_01_000003 Container Transitioned from ACQUIRED to RUNNING
2023-01-24 14:17:32,945 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1672231888656_0007
2023-01-24 14:17:34,533 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1672231888656_0007_000001 with final state: FINISHING, and exit status: -1000
2023-01-24 14:17:34,534 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0007_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2023-01-24 14:17:34,534 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1672231888656_0007 with final state: FINISHING
2023-01-24 14:17:34,534 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0007 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2023-01-24 14:17:34,534 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1672231888656_0007
2023-01-24 14:17:34,534 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0007_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2023-01-24 14:17:34,534 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0007 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2023-01-24 14:17:34,562 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0007_01_000003 Container Transitioned from RUNNING to COMPLETED
2023-01-24 14:17:34,562 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0007	CONTAINERID=container_e02_1672231888656_0007_01_000003	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-01-24 14:17:35,536 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1672231888656_0007 unregistered successfully. 
2023-01-24 14:17:40,913 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1672231888656_0007_000001
2023-01-24 14:17:40,913 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0007_01_000001 Container Transitioned from RUNNING to COMPLETED
2023-01-24 14:17:40,913 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1672231888656_0007_000001
2023-01-24 14:17:40,914 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0007	CONTAINERID=container_e02_1672231888656_0007_01_000001	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-01-24 14:17:40,914 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0007_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2023-01-24 14:17:40,915 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0007 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2023-01-24 14:17:40,915 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1672231888656_0007_000001 is done. finalState=FINISHED
2023-01-24 14:17:40,915 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0007
2023-01-24 14:17:40,915 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1672231888656_0007 requests cleared
2023-01-24 14:17:40,915 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1672231888656_0007 user: student48 queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-01-24 14:17:40,915 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1672231888656_0007 user: student48 leaf-queue of parent: root #applications: 0
2023-01-24 14:17:40,915 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1672231888656_0007_000001
2023-01-24 14:17:40,915 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0007,name=word count,user=student48,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0007/,appMasterHost=adh-test-00-mnode3.ru-central1.internal,submitTime=1674569840580,startTime=1674569840580,finishTime=1674569854534,finalStatus=SUCCEEDED,memorySeconds=49413,vcoreSeconds=27,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=49413 MB-seconds\, 27 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2023-01-24 14:17:53,462 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2023-01-24 14:18:20,355 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://adh-test-00-mnode1.ru-central1.internal:19888/jobhistory/job/job_1672231888656_0006 which is the app master GUI of application_1672231888656_0006 owned by student48
2023-01-24 14:18:42,543 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2023-01-24 14:32:46,000 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2023-01-24 14:51:28,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 14:51:28,867 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-01-24 15:51:28,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 15:51:28,855 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-24 16:51:28,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 16:51:28,854 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-24 17:51:28,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 17:51:28,865 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-24 18:51:28,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 18:51:28,854 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-24 19:51:28,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 19:51:28,861 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-24 20:51:28,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 20:51:28,894 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 44 msec
2023-01-24 21:51:28,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 21:51:28,861 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-24 22:51:28,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 22:51:28,855 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-24 23:51:28,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-24 23:51:28,866 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-25 00:51:28,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 00:51:28,856 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-25 01:51:28,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 01:51:28,854 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-25 02:51:28,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 02:51:28,871 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-01-25 03:51:28,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 03:51:28,865 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-25 04:51:28,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 04:51:28,856 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-25 05:51:28,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 05:51:28,866 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-25 06:51:28,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 06:51:28,859 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-25 07:30:50,743 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2023-01-25 07:36:59,778 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://adh-test-00-mnode1.ru-central1.internal:19888/jobhistory/job/job_1672231888656_0007 which is the app master GUI of application_1672231888656_0007 owned by student48
2023-01-25 07:40:01,109 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://adh-test-00-mnode1.ru-central1.internal:19888/jobhistory/job/job_1672231888656_0007 which is the app master GUI of application_1672231888656_0007 owned by student48
2023-01-25 07:51:28,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 07:51:28,856 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-25 08:03:19,234 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://adh-test-00-mnode1.ru-central1.internal:19888/jobhistory/job/job_1672231888656_0007 which is the app master GUI of application_1672231888656_0007 owned by student48
2023-01-25 08:51:28,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 08:51:28,864 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-25 08:58:31,177 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://adh-test-00-mnode1.ru-central1.internal:19888/jobhistory/job/job_1672231888656_0007 which is the app master GUI of application_1672231888656_0007 owned by student48
2023-01-25 09:51:28,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 09:51:28,867 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-25 10:51:28,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 10:51:28,861 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-25 11:51:28,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 11:51:28,880 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 29 msec
2023-01-25 12:51:28,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 12:51:28,861 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-25 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-25 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-25 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-25 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220226 in 900000ms
2023-01-25 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-25 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892580 in 900000ms
2023-01-25 12:52:35,404 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-25 12:52:36,552 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 26
2023-01-25 12:52:36,552 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-25 12:52:36,552 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-25 12:52:36,552 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 34
2023-01-25 12:52:36,552 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-25 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626900
2023-01-25 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-25 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220226
2023-01-25 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892580
2023-01-25 13:10:33,845 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://adh-test-00-mnode1.ru-central1.internal:19888/jobhistory/job/job_1672231888656_0002 which is the app master GUI of application_1672231888656_0002 owned by student48
2023-01-25 13:10:41,606 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2023-01-25 13:11:26,673 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2023-01-25 13:12:25,687 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://adh-test-00-mnode1.ru-central1.internal:19888/jobhistory/job/job_1672231888656_0007 which is the app master GUI of application_1672231888656_0007 owned by student48
2023-01-25 13:13:56,513 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://adh-test-00-mnode1.ru-central1.internal:19888/jobhistory/job/job_1672231888656_0007 which is the app master GUI of application_1672231888656_0007 owned by student48
2023-01-25 13:30:53,852 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://adh-test-00-mnode1.ru-central1.internal:19888/jobhistory/job/job_1672231888656_0007 which is the app master GUI of application_1672231888656_0007 owned by student48
2023-01-25 13:51:28,852 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 13:51:28,859 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-25 14:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 14:51:28,863 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-25 15:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 15:51:28,877 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-01-25 16:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 16:51:28,862 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-25 17:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 17:51:28,856 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-25 18:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 18:51:28,865 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-25 19:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 19:51:28,856 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-25 20:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 20:51:28,861 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-25 21:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 21:51:28,865 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-25 22:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 22:51:28,868 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-25 23:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-25 23:51:28,858 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-26 00:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 00:51:28,863 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-26 01:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 01:51:28,867 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-26 02:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 02:51:28,861 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-01-26 03:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 03:51:28,863 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-26 04:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 04:51:28,858 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-26 05:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 05:51:28,858 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-26 06:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 06:51:28,857 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-26 07:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 07:51:28,885 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 31 msec
2023-01-26 08:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 08:51:28,868 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-26 09:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 09:51:28,867 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-26 10:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 10:51:28,857 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-26 11:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 11:51:28,859 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-26 12:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 12:51:28,867 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-26 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-26 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-26 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-26 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892581 in 900000ms
2023-01-26 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-26 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220225 in 900000ms
2023-01-26 12:52:37,857 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-26 12:52:39,011 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 27
2023-01-26 12:52:39,011 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-26 12:52:39,011 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-26 12:52:39,011 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 35
2023-01-26 12:52:39,011 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-26 13:06:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626901
2023-01-26 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-26 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892581
2023-01-26 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220225
2023-01-26 13:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 13:51:28,880 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 27 msec
2023-01-26 14:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 14:51:28,857 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-26 15:06:08,047 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2023-01-26 15:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 15:51:28,868 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-26 16:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 16:51:28,859 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-26 17:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 17:51:28,857 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-26 18:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 18:51:28,857 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-26 19:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 19:51:28,863 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-26 20:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 20:51:28,874 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-01-26 21:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 21:51:28,867 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-26 22:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 22:51:28,862 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-26 23:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-26 23:51:28,867 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-27 00:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 00:51:28,857 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-27 01:51:28,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 01:51:28,857 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-27 02:51:28,854 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 02:51:28,859 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-27 03:51:28,854 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 03:51:28,858 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-27 04:51:28,854 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 04:51:28,868 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-27 05:51:28,855 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 05:51:28,858 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-27 06:51:28,855 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 06:51:28,858 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-27 07:51:28,856 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 07:51:28,868 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-27 08:51:28,856 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 08:51:28,867 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-27 09:51:28,857 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 09:51:28,860 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-27 10:51:28,857 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 10:51:28,863 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-27 11:51:28,858 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 11:51:28,875 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-01-27 12:51:28,859 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 12:51:28,863 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-27 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-27 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-27 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-27 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220224 in 900000ms
2023-01-27 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-27 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892582 in 900000ms
2023-01-27 12:52:40,357 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-27 12:52:41,403 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 28
2023-01-27 12:52:41,403 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-27 12:52:41,404 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-27 12:52:41,404 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 36
2023-01-27 12:52:41,404 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-27 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220224
2023-01-27 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626902
2023-01-27 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-27 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892582
2023-01-27 13:51:28,859 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 13:51:28,873 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-27 14:51:28,860 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 14:51:28,874 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-27 15:51:28,860 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 15:51:28,873 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-27 16:51:28,860 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 16:51:28,877 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-01-27 17:51:28,860 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 17:51:28,872 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-27 18:51:28,861 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 18:51:28,865 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-27 19:51:28,861 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 19:51:28,881 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-01-27 20:51:28,861 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 20:51:28,864 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 2 msec
2023-01-27 21:51:28,861 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 21:51:28,866 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-27 22:51:28,862 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 22:51:28,876 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-27 23:51:28,862 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-27 23:51:28,875 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-28 00:51:28,863 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 00:51:28,868 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-28 01:51:28,863 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 01:51:28,878 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-28 02:51:28,863 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 02:51:28,874 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-28 03:51:28,864 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 03:51:28,874 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-28 04:51:28,864 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 04:51:28,874 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-28 05:51:28,864 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 05:51:28,891 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 27 msec
2023-01-28 06:51:28,864 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 06:51:28,891 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 27 msec
2023-01-28 07:51:28,864 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 07:51:28,881 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-01-28 08:51:28,864 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 08:51:28,881 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-01-28 09:51:28,864 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 09:51:28,874 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-28 10:51:28,864 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 10:51:28,869 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-28 11:51:28,864 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 11:51:28,867 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-28 12:51:28,864 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 12:51:28,886 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-01-28 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-28 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-28 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-28 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892583 in 900000ms
2023-01-28 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-28 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220223 in 900000ms
2023-01-28 12:52:42,786 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-28 12:52:43,893 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 29
2023-01-28 12:52:43,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-28 12:52:43,893 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-28 12:52:43,893 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 37
2023-01-28 12:52:43,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-28 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626903
2023-01-28 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-28 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892583
2023-01-28 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220223
2023-01-28 13:51:28,865 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 13:51:28,892 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 27 msec
2023-01-28 14:51:28,865 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 14:51:28,873 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-28 15:51:28,865 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 15:51:28,901 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 36 msec
2023-01-28 16:51:28,866 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 16:51:28,874 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-28 17:51:28,866 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 17:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 48 msec
2023-01-28 18:51:28,867 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 18:51:28,880 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-28 19:51:28,866 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 19:51:28,871 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-28 20:51:28,866 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 20:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-01-28 21:51:28,866 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 21:51:28,878 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-28 22:51:28,866 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 22:51:28,869 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-28 23:51:28,866 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-28 23:51:28,869 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-29 00:51:28,866 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 00:51:28,880 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-29 01:51:28,866 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 01:51:28,876 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-01-29 02:51:28,867 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 02:51:28,871 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-29 03:51:28,868 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 03:51:28,882 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-29 04:51:28,868 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 04:51:28,879 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-29 05:51:28,868 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 05:51:28,872 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-29 06:51:28,868 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 06:51:28,873 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-29 07:51:28,868 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 07:51:28,885 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-29 08:51:28,868 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 08:51:28,874 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-29 09:51:28,868 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 09:51:28,876 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-29 10:51:28,868 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 10:51:28,884 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-29 11:51:28,868 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 11:51:28,884 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-29 12:51:28,868 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 12:51:28,882 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-29 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-29 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-29 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-29 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892584 in 900000ms
2023-01-29 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-29 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220222 in 900000ms
2023-01-29 12:52:45,231 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-29 12:52:46,318 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 30
2023-01-29 12:52:46,319 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-29 12:52:46,319 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-29 12:52:46,319 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 38
2023-01-29 12:52:46,319 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-29 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626904
2023-01-29 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-29 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892584
2023-01-29 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220222
2023-01-29 13:51:28,869 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 13:51:28,890 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-01-29 14:51:28,869 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 14:51:28,873 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-29 15:51:28,869 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 15:51:28,885 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-29 16:51:28,869 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 16:51:28,876 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-01-29 17:51:28,869 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 17:51:28,874 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-29 18:51:28,869 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 18:51:28,885 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-29 19:51:28,869 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 19:51:28,881 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-29 20:51:28,870 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 20:51:28,874 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-29 21:51:28,870 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 21:51:28,892 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-01-29 22:51:28,870 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 22:51:28,884 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-29 23:51:28,870 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-29 23:51:28,874 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-30 00:51:28,870 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 00:51:28,874 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-30 01:51:28,870 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 01:51:28,877 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-01-30 02:51:28,870 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 02:51:28,883 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-30 03:51:28,870 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 03:51:28,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-01-30 04:51:28,870 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 04:51:28,883 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-30 05:51:28,870 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 05:51:28,882 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-30 06:51:28,870 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 06:51:28,874 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-30 07:51:28,870 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 07:51:28,890 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-01-30 08:51:28,870 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 08:51:28,875 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-30 09:51:28,870 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 09:51:28,875 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-30 10:51:28,870 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 10:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-01-30 11:51:28,870 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 11:51:28,874 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-30 12:24:20,305 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2023-01-30 12:51:28,870 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 12:51:28,876 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-30 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-30 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-30 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-30 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892585 in 900000ms
2023-01-30 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-30 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220221 in 900000ms
2023-01-30 12:52:47,677 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-30 12:52:48,805 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 31
2023-01-30 12:52:48,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-30 12:52:48,805 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-30 12:52:48,805 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 39
2023-01-30 12:52:48,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-30 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892585
2023-01-30 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220221
2023-01-30 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626905
2023-01-30 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-30 13:51:28,870 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 13:51:28,895 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-01-30 14:51:28,870 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 14:51:28,874 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-30 15:51:28,871 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 15:51:28,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-30 16:51:28,871 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 16:51:28,875 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-30 17:51:28,871 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 17:51:28,876 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-30 18:51:28,871 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 18:51:28,882 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-30 19:51:28,871 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 19:51:28,882 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-01-30 20:51:28,871 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 20:51:28,894 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-01-30 21:51:28,872 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 21:51:28,876 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-30 22:51:28,872 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 22:51:28,877 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-30 23:51:28,872 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-30 23:51:28,878 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-01-31 00:51:28,872 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 00:51:28,886 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-31 01:51:28,872 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 01:51:28,875 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-31 02:51:28,872 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 02:51:28,875 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-01-31 03:51:28,872 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 03:51:28,877 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-31 04:51:28,872 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 04:51:28,885 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-31 05:51:28,872 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 05:51:28,889 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-01-31 06:51:28,872 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 06:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-01-31 07:51:28,872 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 07:51:28,875 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-31 08:51:28,873 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 08:51:28,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-31 09:51:28,872 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 09:51:28,892 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-01-31 10:51:28,872 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 10:51:28,884 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-31 11:51:28,872 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 11:51:28,876 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-01-31 12:51:28,873 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 12:51:28,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-01-31 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-01-31 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-31 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-01-31 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-01-31 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220220 in 900000ms
2023-01-31 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892586 in 900000ms
2023-01-31 12:52:50,138 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-31 12:52:51,107 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 32
2023-01-31 12:52:51,107 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-01-31 12:52:51,107 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-01-31 12:52:51,107 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 40
2023-01-31 12:52:51,107 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-01-31 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626906
2023-01-31 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-01-31 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220220
2023-01-31 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892586
2023-01-31 13:51:28,873 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 13:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-01-31 14:51:28,873 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 14:51:28,889 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-01-31 15:51:28,873 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 15:51:28,880 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-01-31 16:51:28,873 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 16:51:28,882 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-01-31 17:51:28,874 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 17:51:28,879 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-31 18:51:28,874 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 18:51:28,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-01-31 19:51:28,874 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 19:51:28,879 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-01-31 20:51:28,874 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 20:51:28,886 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-01-31 21:51:28,874 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 21:51:28,882 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-01-31 22:51:28,874 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 22:51:28,890 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-01-31 23:51:28,875 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-01-31 23:51:28,878 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-01 00:51:28,875 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 00:51:28,886 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-01 01:51:28,875 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 01:51:28,886 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-01 02:51:28,875 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 02:51:28,879 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-01 03:51:28,875 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 03:51:28,898 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-02-01 04:51:28,875 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 04:51:28,878 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-01 05:51:28,876 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 05:51:28,879 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-01 06:51:28,876 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 06:51:28,886 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-01 07:51:28,877 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 07:51:28,882 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-01 08:51:28,877 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 08:51:28,883 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-01 09:51:28,878 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 09:51:28,881 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-01 10:51:28,877 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 10:51:28,894 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-02-01 11:51:28,877 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 11:51:28,882 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-01 12:51:28,877 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 12:51:28,880 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-01 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-01 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-01 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-01 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220219 in 900000ms
2023-02-01 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-01 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892587 in 900000ms
2023-02-01 12:52:52,531 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-01 12:52:53,562 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 33
2023-02-01 12:52:53,562 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-01 12:52:53,562 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-01 12:52:53,562 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 41
2023-02-01 12:52:53,562 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-01 13:06:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626907
2023-02-01 13:06:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-01 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220219
2023-02-01 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892587
2023-02-01 13:51:28,877 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 13:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-02-01 14:51:28,877 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 14:51:28,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-01 15:51:28,877 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 15:51:28,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-01 16:51:28,877 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 16:51:28,886 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-01 17:51:28,878 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 17:51:28,889 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-01 18:51:28,878 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 18:51:28,885 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-02-01 19:51:28,878 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 19:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-02-01 20:51:28,878 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 20:51:28,885 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-02-01 21:51:28,878 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 21:51:28,881 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-01 22:51:28,878 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 22:51:28,891 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-01 23:51:28,878 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-01 23:51:28,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-02 00:51:28,878 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 00:51:28,883 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-02 01:51:28,878 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 01:51:28,881 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-02 02:51:28,879 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 02:51:28,884 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-02 03:51:28,879 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 03:51:28,883 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-02 04:51:28,879 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 04:51:28,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-02 05:51:28,879 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 05:51:28,883 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-02 06:51:28,880 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 06:51:28,889 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-02 07:51:28,880 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 07:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-02-02 08:51:28,880 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 08:51:28,894 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-02-02 09:51:28,880 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 09:51:28,884 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-02 10:51:28,880 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 10:51:28,884 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-02 11:51:28,880 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 11:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-02 12:51:28,880 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 12:51:28,895 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-02-02 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-02 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-02 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-02 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220218 in 900000ms
2023-02-02 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-02 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892588 in 900000ms
2023-02-02 12:52:54,978 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-02 12:52:55,969 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 34
2023-02-02 12:52:55,969 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-02 12:52:55,969 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-02 12:52:55,969 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 42
2023-02-02 12:52:55,969 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-02 13:06:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626908
2023-02-02 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-02 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220218
2023-02-02 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892588
2023-02-02 13:51:28,880 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 13:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-02-02 14:51:28,881 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 14:51:28,894 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-02-02 15:51:28,881 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 15:51:28,892 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-02 16:51:28,881 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 16:51:28,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-02 17:51:28,881 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 17:51:28,901 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-02-02 18:51:28,881 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 18:51:28,886 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-02 19:51:28,881 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 19:51:28,895 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-02 20:51:28,881 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 20:51:28,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-02 21:51:28,881 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 21:51:28,903 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-02-02 22:51:28,881 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 22:51:28,885 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-02 23:51:28,881 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-02 23:51:28,892 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-03 00:51:28,881 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 00:51:28,889 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-03 01:51:28,881 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 01:51:28,901 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-02-03 02:51:28,881 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 02:51:28,886 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-03 03:51:28,881 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 03:51:28,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-03 04:51:28,882 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 04:51:28,886 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-03 05:51:28,883 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 05:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-03 06:51:28,883 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 06:51:28,892 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-03 07:51:28,883 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 07:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-03 08:51:28,883 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 08:51:28,894 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-03 09:51:28,883 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 09:51:28,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-03 10:51:28,884 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 10:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-03 11:51:28,885 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 11:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-03 12:51:28,885 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 12:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-03 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-03 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-03 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-03 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892589 in 900000ms
2023-02-03 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-03 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220217 in 900000ms
2023-02-03 12:52:57,325 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-03 12:52:58,434 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 35
2023-02-03 12:52:58,434 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-03 12:52:58,434 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-03 12:52:58,434 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 43
2023-02-03 12:52:58,434 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-03 13:06:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626909
2023-02-03 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-03 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892589
2023-02-03 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220217
2023-02-03 13:51:28,885 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 13:51:28,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-03 14:51:28,885 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 14:51:28,889 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-03 15:51:28,886 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 15:51:28,897 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-03 16:51:28,886 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 16:51:28,891 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-03 17:51:28,886 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 17:51:28,908 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-02-03 18:51:28,886 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 18:51:28,890 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-03 19:51:28,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 19:51:28,891 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-03 20:51:28,887 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 20:51:28,891 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-03 21:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 21:51:28,898 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-03 22:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 22:51:28,892 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-03 23:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-03 23:51:28,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-04 00:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 00:51:28,902 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-02-04 01:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 01:51:28,897 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-04 02:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 02:51:28,892 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-04 03:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 03:51:28,925 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 38 msec
2023-02-04 04:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 04:51:28,901 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-04 05:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 05:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-02-04 06:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 06:51:28,903 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-04 07:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 07:51:28,895 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-04 08:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 08:51:28,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-04 09:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 09:51:28,901 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-04 10:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 10:51:28,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-04 11:51:28,888 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 11:51:28,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-04 12:51:28,889 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 12:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-04 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-04 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-04 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-04 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-04 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220216 in 900000ms
2023-02-04 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892590 in 900000ms
2023-02-04 12:52:59,799 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-04 12:53:00,768 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 36
2023-02-04 12:53:00,768 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-04 12:53:00,768 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-04 12:53:00,768 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 44
2023-02-04 12:53:00,768 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-04 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626910
2023-02-04 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-04 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220216
2023-02-04 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892590
2023-02-04 13:51:28,890 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 13:51:28,906 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-04 14:51:28,890 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 14:51:28,895 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-04 15:51:28,890 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 15:51:28,918 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 28 msec
2023-02-04 16:51:28,890 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 16:51:28,908 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-02-04 17:51:28,890 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 17:51:28,898 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-04 18:51:28,890 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 18:51:28,895 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-04 19:51:28,890 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 19:51:28,905 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-04 20:51:28,890 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 20:51:28,895 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-04 21:51:28,890 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 21:51:28,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-04 22:51:28,891 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 22:51:28,895 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-04 23:51:28,891 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-04 23:51:28,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-05 00:51:28,891 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 00:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-05 01:51:28,891 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 01:51:28,898 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-05 02:51:28,891 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 02:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-05 03:51:28,891 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 03:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-05 04:51:28,891 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 04:51:28,895 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-05 05:51:28,891 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 05:51:28,894 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-05 06:51:28,891 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 06:51:28,912 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-02-05 07:51:28,891 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 07:51:28,894 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-05 08:51:28,892 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 08:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-05 09:51:28,892 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 09:51:28,917 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-02-05 10:51:28,892 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 10:51:28,908 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-05 11:51:28,892 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 11:51:28,903 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-05 12:51:28,892 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 12:51:28,897 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-05 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-05 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-05 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-05 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220215 in 900000ms
2023-02-05 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-05 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892591 in 900000ms
2023-02-05 12:53:02,193 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-05 12:53:03,228 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 37
2023-02-05 12:53:03,228 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-05 12:53:03,228 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-05 12:53:03,228 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 45
2023-02-05 12:53:03,228 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-05 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626911
2023-02-05 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220215
2023-02-05 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-05 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892591
2023-02-05 13:51:28,892 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 13:51:28,931 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 39 msec
2023-02-05 14:51:28,892 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 14:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-05 15:51:28,892 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 15:51:28,903 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-05 16:51:28,892 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 16:51:28,897 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-05 17:51:28,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 17:51:28,902 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-05 18:51:28,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 18:51:28,897 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-05 19:51:28,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 19:51:28,898 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-05 20:51:28,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 20:51:28,897 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-05 21:51:28,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 21:51:28,897 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-05 22:51:28,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 22:51:28,897 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-05 23:51:28,894 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-05 23:51:28,914 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-02-06 00:51:28,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 00:51:28,898 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-06 01:51:28,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 01:51:28,897 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-06 02:51:28,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 02:51:28,906 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-06 03:51:28,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 03:51:28,902 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-06 04:51:28,894 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 04:51:28,898 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-06 05:51:28,894 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 05:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-06 06:51:28,895 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 06:51:28,905 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-06 07:51:28,895 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 07:51:28,911 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-02-06 08:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 08:51:28,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-06 09:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 09:51:28,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-06 10:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 10:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-06 11:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 11:51:28,901 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-06 12:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 12:51:28,923 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 27 msec
2023-02-06 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-06 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-06 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-06 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220214 in 900000ms
2023-02-06 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-06 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892592 in 900000ms
2023-02-06 12:53:04,661 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-06 12:53:05,598 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 38
2023-02-06 12:53:05,598 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-06 12:53:05,598 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-06 12:53:05,598 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 46
2023-02-06 12:53:05,598 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-06 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626912
2023-02-06 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-06 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220214
2023-02-06 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892592
2023-02-06 13:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 13:51:28,906 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-06 14:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 14:51:28,910 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-02-06 15:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 15:51:28,903 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-02-06 16:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 16:51:28,901 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-06 17:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 17:51:28,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-06 18:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 18:51:28,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-06 19:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 19:51:28,905 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-06 20:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 20:51:28,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-06 21:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 21:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-02-06 22:51:28,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 22:51:28,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-06 23:51:28,897 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-06 23:51:28,907 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-07 00:51:28,898 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 00:51:28,906 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-07 01:51:28,898 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 01:51:28,901 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-07 02:51:28,898 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 02:51:28,903 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-07 03:51:28,898 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 03:51:28,903 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-07 04:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 04:51:28,911 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-07 05:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 05:51:28,904 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-07 06:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 06:51:28,907 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-07 07:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 07:51:28,908 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-07 08:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 08:51:28,904 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-07 09:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 09:51:28,916 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-02-07 10:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 10:51:28,923 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-02-07 11:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 11:51:28,903 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-07 12:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 12:51:28,921 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-02-07 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-07 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-07 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-07 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892593 in 900000ms
2023-02-07 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-07 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220213 in 900000ms
2023-02-07 12:53:07,058 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-07 12:53:08,044 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 39
2023-02-07 12:53:08,044 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-07 12:53:08,044 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-07 12:53:08,044 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 47
2023-02-07 12:53:08,044 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-07 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892593
2023-02-07 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626913
2023-02-07 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-07 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220213
2023-02-07 13:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 13:51:28,916 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-02-07 14:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 14:51:28,912 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-07 15:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 15:51:28,906 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-07 16:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 16:51:28,915 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-07 17:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 17:51:28,911 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-07 18:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 18:51:28,903 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-07 19:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 19:51:28,923 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-02-07 20:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 20:51:28,904 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-07 21:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 21:51:28,915 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-07 22:51:28,899 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 22:51:28,903 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-07 23:51:28,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-07 23:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-08 00:51:28,901 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 00:51:28,906 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-08 01:51:28,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 01:51:28,915 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-08 02:51:28,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 02:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 50 msec
2023-02-08 03:51:28,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 03:51:28,912 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-08 04:51:28,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 04:51:28,904 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-08 05:51:28,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 05:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-08 06:51:28,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 06:51:28,905 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-08 07:51:28,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 07:51:28,910 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-08 08:51:28,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 08:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-02-08 09:51:28,900 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 09:51:28,904 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-08 10:51:28,901 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 10:51:28,910 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-08 11:51:28,901 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 11:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-08 12:51:28,901 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 12:51:28,915 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-08 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-08 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-08 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-08 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220212 in 900000ms
2023-02-08 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-08 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892594 in 900000ms
2023-02-08 12:53:09,466 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-08 12:53:10,485 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 40
2023-02-08 12:53:10,485 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-08 12:53:10,485 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-08 12:53:10,485 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 48
2023-02-08 12:53:10,485 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-08 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220212
2023-02-08 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626914
2023-02-08 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-08 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892594
2023-02-08 13:51:28,901 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 13:51:28,923 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-02-08 14:51:28,901 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 14:51:28,914 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-08 15:51:28,901 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 15:51:28,942 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 41 msec
2023-02-08 16:51:28,901 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 16:51:28,911 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-08 17:51:28,901 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 17:51:28,911 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-08 18:51:28,901 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 18:51:28,906 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-08 19:51:28,902 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 19:51:28,905 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-08 20:51:28,902 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 20:51:28,915 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-08 21:51:28,903 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 21:51:28,907 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-08 22:51:28,902 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 22:51:28,907 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-08 23:51:28,902 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-08 23:51:28,906 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-09 00:51:28,902 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 00:51:28,907 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-09 01:51:28,902 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 01:51:28,906 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-09 02:51:28,902 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 02:51:28,921 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-02-09 03:51:28,902 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 03:51:28,916 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-02-09 04:51:28,902 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 04:51:28,929 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 27 msec
2023-02-09 05:51:28,902 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 05:51:28,905 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-09 06:51:28,903 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 06:51:28,907 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-09 07:51:28,904 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 07:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-09 08:51:28,904 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 08:51:28,912 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-09 09:51:28,904 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 09:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-09 10:51:28,904 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 10:51:28,908 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-09 11:51:28,905 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 11:51:28,914 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-09 12:51:28,905 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 12:51:28,918 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-09 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-09 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-09 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892595 in 900000ms
2023-02-09 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-09 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-09 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220211 in 900000ms
2023-02-09 12:53:11,803 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-09 12:53:12,890 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 41
2023-02-09 12:53:12,890 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-09 12:53:12,890 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-09 12:53:12,890 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 49
2023-02-09 12:53:12,890 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-09 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892595
2023-02-09 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626915
2023-02-09 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-09 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220211
2023-02-09 13:51:28,905 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 13:51:28,920 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-09 14:51:28,906 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 14:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-02-09 15:51:28,906 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 15:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-02-09 16:51:28,906 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 16:51:28,915 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-09 17:51:28,906 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 17:51:28,919 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-09 18:51:28,906 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 18:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-09 19:51:28,907 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 19:51:28,927 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-02-09 20:51:28,907 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 20:51:28,911 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-09 21:51:28,907 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 21:51:28,916 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-09 22:51:28,907 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 22:51:28,911 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-09 23:51:28,907 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-09 23:51:28,912 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-10 00:51:28,907 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 00:51:28,922 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-10 01:51:28,907 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 01:51:28,944 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 37 msec
2023-02-10 02:51:28,907 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 02:51:28,911 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-10 03:51:28,908 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 03:51:28,911 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-10 04:51:28,908 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 04:51:28,915 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-02-10 05:51:28,908 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 05:51:28,924 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-02-10 06:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 06:51:28,922 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-10 07:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 07:51:28,912 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-10 08:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 08:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-10 09:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 09:51:28,926 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-02-10 10:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 10:51:28,914 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-10 11:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 11:51:28,912 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-10 12:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 12:51:28,912 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-10 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-10 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-10 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-10 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892596 in 900000ms
2023-02-10 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-10 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220210 in 900000ms
2023-02-10 12:53:14,260 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-10 12:53:15,339 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 42
2023-02-10 12:53:15,339 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-10 12:53:15,339 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-10 12:53:15,339 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 50
2023-02-10 12:53:15,339 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-10 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626916
2023-02-10 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-10 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892596
2023-02-10 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220210
2023-02-10 13:51:28,910 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 13:51:28,953 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 44 msec
2023-02-10 14:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 14:51:28,912 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-10 15:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 15:51:28,927 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-02-10 16:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 16:51:28,930 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-02-10 17:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 17:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-10 18:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 18:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-10 19:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 19:51:28,919 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-10 20:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 20:51:28,961 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 52 msec
2023-02-10 21:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 21:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-10 22:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 22:51:28,919 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-10 23:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-10 23:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 37 msec
2023-02-11 00:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 00:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-11 01:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 01:51:28,920 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-11 02:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 02:51:28,917 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-11 03:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 03:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-11 04:51:28,909 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 04:51:28,929 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-02-11 05:51:28,910 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 05:51:28,917 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-11 06:51:28,911 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 06:51:28,919 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-11 07:51:28,911 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 07:51:28,919 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-11 08:51:28,911 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 08:51:28,927 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-11 09:51:28,911 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 09:51:28,936 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-02-11 10:51:28,912 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 10:51:28,945 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 33 msec
2023-02-11 11:51:28,912 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 11:51:28,930 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-02-11 12:51:28,912 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 12:51:28,917 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-11 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-11 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-11 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-11 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-11 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892597 in 900000ms
2023-02-11 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220209 in 900000ms
2023-02-11 12:53:16,637 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-11 12:53:17,719 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 43
2023-02-11 12:53:17,719 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-11 12:53:17,719 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-11 12:53:17,719 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 51
2023-02-11 12:53:17,719 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-11 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626917
2023-02-11 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-11 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892597
2023-02-11 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220209
2023-02-11 13:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 13:51:28,920 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-02-11 14:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 14:51:28,925 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-11 15:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 15:51:28,918 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-11 16:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 16:51:28,916 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-11 17:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 17:51:28,918 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-11 18:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 18:51:28,924 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-11 19:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 19:51:28,923 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-11 20:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 20:51:28,916 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-11 21:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 21:51:28,935 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-02-11 22:51:28,913 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 22:51:28,935 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-02-11 23:51:28,914 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-11 23:51:28,922 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-12 00:51:28,914 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 00:51:28,919 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-12 01:51:28,914 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 01:51:28,918 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-12 02:51:28,914 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 02:51:28,923 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-12 03:51:28,914 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 03:51:28,919 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-12 04:51:28,914 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 04:51:28,918 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-12 05:51:28,915 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 05:51:28,931 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-12 06:51:28,915 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 06:51:28,930 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-12 07:51:28,915 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 07:51:28,919 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-12 08:51:28,915 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 08:51:28,928 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-12 09:51:28,915 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 09:51:28,937 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-02-12 10:51:28,915 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 10:51:28,930 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-12 11:51:28,916 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 11:51:28,923 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-12 12:51:28,917 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 12:51:28,928 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-12 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-12 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-12 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-12 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-12 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892598 in 900000ms
2023-02-12 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220208 in 900000ms
2023-02-12 12:53:19,092 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-12 12:53:20,155 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 44
2023-02-12 12:53:20,155 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-12 12:53:20,155 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-12 12:53:20,155 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 52
2023-02-12 12:53:20,155 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-12 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626918
2023-02-12 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-12 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892598
2023-02-12 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220208
2023-02-12 13:51:28,918 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 13:51:28,927 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-12 14:51:28,919 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 14:51:28,922 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-12 15:51:28,920 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 15:51:28,924 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-12 16:51:28,920 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 16:51:28,933 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-12 17:51:28,920 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 17:51:28,925 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-12 18:51:28,920 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 18:51:28,931 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-12 19:51:28,920 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 19:51:28,939 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-02-12 20:51:28,920 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 20:51:28,930 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-12 21:51:28,921 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 21:51:28,929 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-12 22:51:28,921 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 22:51:28,967 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 46 msec
2023-02-12 23:51:28,921 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-12 23:51:28,925 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-13 00:51:28,921 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 00:51:28,924 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-13 01:51:28,922 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 01:51:28,947 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 26 msec
2023-02-13 02:51:28,923 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 02:51:28,927 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-13 03:51:28,923 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 03:51:28,927 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-13 04:51:28,923 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 04:51:28,929 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-13 05:51:28,923 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 05:51:28,929 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-13 06:51:28,923 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 06:51:28,935 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-13 07:51:28,924 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 07:51:28,936 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-13 08:51:28,925 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 08:51:28,928 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-13 09:33:48,955 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2023-02-13 09:51:28,925 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 09:51:28,944 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-02-13 10:51:28,925 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 10:51:28,940 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-13 11:49:49,031 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2023-02-13 11:51:28,925 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 11:51:28,944 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-02-13 11:54:06,415 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2023-02-13 11:58:27,099 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2023-02-13 11:59:29,138 INFO org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: dr.who is accessing unchecked http://adh-test-00-mnode1.ru-central1.internal:19888/jobhistory/job/job_1672231888656_0007 which is the app master GUI of application_1672231888656_0007 owned by student48
2023-02-13 12:51:28,925 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-13 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-13 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-13 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892599 in 900000ms
2023-02-13 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-13 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220207 in 900000ms
2023-02-13 12:51:28,955 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 30 msec
2023-02-13 12:53:21,473 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-13 12:53:22,521 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 45
2023-02-13 12:53:22,521 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-13 12:53:22,521 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-13 12:53:22,521 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 53
2023-02-13 12:53:22,521 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-13 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626919
2023-02-13 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-13 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892599
2023-02-13 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220207
2023-02-13 13:51:28,926 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 13:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-02-13 14:51:28,926 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 14:51:28,941 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-13 15:51:28,926 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 15:51:28,931 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-13 16:51:28,926 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 16:51:28,931 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-13 17:51:28,927 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 17:51:28,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-02-13 18:51:28,927 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 18:51:28,933 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-13 19:51:28,927 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 19:51:28,938 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-13 20:51:28,927 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 20:51:28,932 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-13 21:51:28,927 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 21:51:28,931 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-13 22:51:28,927 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 22:51:28,931 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-13 23:51:28,928 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-13 23:51:28,935 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-14 00:51:28,929 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 00:51:28,932 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-14 01:51:28,929 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 01:51:28,943 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-14 02:51:28,929 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 02:51:28,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-14 03:51:28,929 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 03:51:28,944 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-14 04:51:28,929 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 04:51:28,942 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-14 05:51:28,929 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 05:51:28,940 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-14 06:51:28,929 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 06:51:28,932 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-14 07:51:28,930 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 07:51:28,942 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-14 08:51:28,930 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 08:51:28,940 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-14 09:51:28,931 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 09:51:28,943 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-14 10:51:28,931 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 10:51:28,943 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-14 11:51:28,931 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 11:51:28,936 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-14 12:51:28,931 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 12:51:28,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-14 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-14 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-14 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-14 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892600 in 900000ms
2023-02-14 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-14 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220206 in 900000ms
2023-02-14 12:53:23,918 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-14 12:53:24,953 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 46
2023-02-14 12:53:24,953 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-14 12:53:24,953 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-14 12:53:24,953 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 54
2023-02-14 12:53:24,953 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-14 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626920
2023-02-14 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892600
2023-02-14 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-14 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220206
2023-02-14 13:51:28,931 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 13:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-02-14 14:51:28,931 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 14:51:28,945 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-02-14 15:51:28,931 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 15:51:28,939 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-14 16:51:28,932 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 16:51:28,935 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-14 17:51:28,932 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 17:51:28,949 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-14 18:51:28,932 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 18:51:28,942 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-14 19:51:28,932 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 19:51:28,947 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-14 20:51:28,932 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 20:51:28,944 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-14 21:51:28,932 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 21:51:28,942 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-14 22:51:28,932 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 22:51:28,937 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-14 23:51:28,932 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-14 23:51:28,943 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-15 00:51:28,932 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 00:51:28,942 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-15 01:51:28,932 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 01:51:28,944 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-15 02:51:28,932 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 02:51:28,958 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 27 msec
2023-02-15 03:51:28,933 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 03:51:28,948 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-15 04:51:28,933 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 04:51:28,948 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-02-15 05:51:28,933 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 05:51:28,957 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-02-15 06:51:28,933 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 06:51:28,958 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-02-15 07:51:28,933 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 07:51:28,937 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-15 08:51:28,933 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 08:51:28,937 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-15 09:51:28,933 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 09:51:28,944 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-15 10:51:28,933 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 10:51:28,948 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-02-15 11:51:28,933 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 11:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-02-15 12:51:28,933 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 12:51:28,937 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-15 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-15 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-15 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-15 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892601 in 900000ms
2023-02-15 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-15 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220205 in 900000ms
2023-02-15 12:53:26,248 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-15 12:53:27,348 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 47
2023-02-15 12:53:27,349 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-15 12:53:27,349 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-15 12:53:27,349 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 55
2023-02-15 12:53:27,349 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-15 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626921
2023-02-15 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-15 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892601
2023-02-15 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220205
2023-02-15 13:51:28,933 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 13:51:28,956 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-02-15 14:51:28,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 14:51:28,937 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-15 15:51:28,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 15:51:28,943 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-15 16:51:28,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 16:51:28,944 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-15 17:51:28,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 17:51:28,942 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-15 18:51:28,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 18:51:28,941 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-02-15 19:51:28,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 19:51:28,943 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-15 20:51:28,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 20:51:28,939 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-15 21:51:28,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 21:51:28,943 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-15 22:51:28,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 22:51:28,940 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-15 23:51:28,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-15 23:51:28,941 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-16 00:51:28,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 00:51:28,938 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-16 01:51:28,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 01:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-16 02:51:28,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 02:51:28,944 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-16 03:51:28,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 03:51:28,943 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-16 04:51:28,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 04:51:28,939 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-16 05:51:28,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 05:51:28,938 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-16 06:51:28,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 06:51:28,947 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-16 07:51:28,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 07:51:28,949 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-02-16 08:51:28,934 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 08:51:28,939 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-16 09:51:28,935 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 09:51:28,939 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-16 10:51:28,935 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 10:51:28,948 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-16 11:51:28,935 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 11:51:28,939 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-16 12:51:28,935 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 12:51:28,943 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-02-16 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-16 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-16 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-16 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220204 in 900000ms
2023-02-16 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-16 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892602 in 900000ms
2023-02-16 12:53:28,671 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-16 12:53:29,784 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 48
2023-02-16 12:53:29,784 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-16 12:53:29,784 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-16 12:53:29,784 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 56
2023-02-16 12:53:29,784 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-16 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220204
2023-02-16 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626922
2023-02-16 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-16 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892602
2023-02-16 13:51:28,935 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 13:51:28,955 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-02-16 14:51:28,935 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 14:51:28,947 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-16 15:51:28,936 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 15:51:28,939 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-16 16:51:28,936 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 16:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-16 17:51:28,936 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 17:51:28,942 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-02-16 18:51:28,937 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 18:51:28,941 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-16 19:51:28,937 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 19:51:28,941 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-16 20:51:28,937 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 20:51:28,941 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-16 21:51:28,937 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 21:51:28,949 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-16 22:51:28,937 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 22:51:28,945 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-16 23:51:28,938 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-16 23:51:28,948 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-17 00:51:28,939 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 00:51:28,942 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-17 01:51:28,939 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 01:51:28,942 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-17 02:51:28,939 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 02:51:28,943 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-17 03:51:28,939 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 03:51:28,955 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-02-17 04:51:28,940 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 04:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-17 05:51:28,940 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 05:51:28,953 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-17 06:51:28,940 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 06:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-17 07:51:28,940 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 07:51:28,944 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-17 08:51:28,940 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 08:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-17 09:51:28,940 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 09:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-17 10:51:28,941 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 10:51:28,961 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-02-17 11:51:28,942 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 11:51:28,945 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-17 12:51:28,942 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 12:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-17 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-17 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-17 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-17 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892603 in 900000ms
2023-02-17 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-17 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220203 in 900000ms
2023-02-17 12:53:31,001 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-17 12:53:32,105 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 49
2023-02-17 12:53:32,105 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-17 12:53:32,105 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-17 12:53:32,105 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 57
2023-02-17 12:53:32,106 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-17 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892603
2023-02-17 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626923
2023-02-17 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-17 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220203
2023-02-17 13:51:28,942 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 13:51:28,954 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-17 14:51:28,942 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 14:51:28,956 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-17 15:51:28,942 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 15:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-17 16:51:28,942 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 16:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-17 17:51:28,942 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 17:51:28,949 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-02-17 18:51:28,942 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 18:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-17 19:51:28,942 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 19:51:28,973 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 31 msec
2023-02-17 20:51:28,943 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 20:51:28,957 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-17 21:51:28,944 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 21:51:28,958 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-17 22:51:28,945 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 22:51:28,949 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-17 23:51:28,945 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-17 23:51:28,971 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 26 msec
2023-02-18 00:51:28,944 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 00:51:28,964 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-02-18 01:51:28,944 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 01:51:28,949 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-18 02:51:28,944 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 02:51:28,949 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-18 03:51:28,944 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 03:51:28,955 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-18 04:51:28,945 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 04:51:28,953 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-18 05:51:28,945 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 05:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-02-18 06:51:28,945 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 06:51:28,957 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-18 07:51:28,945 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 07:51:28,958 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-18 08:51:28,945 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 08:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-18 09:51:28,945 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 09:51:28,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-02-18 10:51:28,945 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 10:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-02-18 11:51:28,945 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 11:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-02-18 12:51:28,945 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-18 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-18 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-18 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892604 in 900000ms
2023-02-18 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-18 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220202 in 900000ms
2023-02-18 12:51:28,999 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 54 msec
2023-02-18 12:53:33,446 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-18 12:53:34,530 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 50
2023-02-18 12:53:34,530 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-18 12:53:34,530 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-18 12:53:34,530 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 58
2023-02-18 12:53:34,530 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-18 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626924
2023-02-18 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-18 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892604
2023-02-18 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220202
2023-02-18 13:51:28,945 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 13:51:28,957 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-18 14:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 14:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-18 15:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 15:51:28,961 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-18 16:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 16:51:28,959 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-18 17:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 17:51:28,960 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-18 18:49:59,223 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 8
2023-02-18 18:50:01,438 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1672231888656_0008' is submitted without priority hence considering default queue/cluster priority: 0
2023-02-18 18:50:01,438 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1672231888656_0008
2023-02-18 18:50:01,438 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 8 submitted by user student48
2023-02-18 18:50:01,438 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	IP=10.130.0.4	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1672231888656_0008	CALLERCONTEXT=CLI	QUEUENAME=default
2023-02-18 18:50:01,439 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1672231888656_0008
2023-02-18 18:50:01,441 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1672231888656_0008
2023-02-18 18:50:01,441 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0008 State change from NEW to NEW_SAVING on event = START
2023-02-18 18:50:01,442 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0008 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2023-02-18 18:50:01,442 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1672231888656_0008 user: student48 leaf-queue of parent: root #applications: 1
2023-02-18 18:50:01,442 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1672231888656_0008 from user: student48, in queue: default
2023-02-18 18:50:01,443 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0008 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2023-02-18 18:50:01,443 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1672231888656_0008_000001
2023-02-18 18:50:01,443 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0008_000001 State change from NEW to SUBMITTED on event = START
2023-02-18 18:50:01,443 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1672231888656_0008 from user: student48 activated in queue: default
2023-02-18 18:50:01,443 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1672231888656_0008 user: student48, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-02-18 18:50:01,443 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1672231888656_0008_000001 to scheduler from user student48 in queue default
2023-02-18 18:50:01,444 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0008_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-02-18 18:50:01,555 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0008_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-02-18 18:50:01,555 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0008_01_000001 Container Transitioned from NEW to ALLOCATED
2023-02-18 18:50:01,555 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0008_01_000001 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2023-02-18 18:50:01,555 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0008	CONTAINERID=container_e02_1672231888656_0008_01_000001	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-02-18 18:50:01,556 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e02_1672231888656_0008_01_000001
2023-02-18 18:50:01,557 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0008_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-02-18 18:50:01,557 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1672231888656_0008_000001
2023-02-18 18:50:01,557 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1672231888656_0008 AttemptId: appattempt_1672231888656_0008_000001 MasterContainer: Container: [ContainerId: container_e02_1672231888656_0008_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ]
2023-02-18 18:50:01,557 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.0967361 absoluteUsedCapacity=0.0967361 used=<memory:2048, vCores:1> cluster=<memory:21171, vCores:6>
2023-02-18 18:50:01,557 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-02-18 18:50:01,558 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0008_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-02-18 18:50:01,559 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0008_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-02-18 18:50:01,560 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1672231888656_0008_000001
2023-02-18 18:50:01,561 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e02_1672231888656_0008_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1672231888656_0008_000001
2023-02-18 18:50:01,561 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1672231888656_0008_000001
2023-02-18 18:50:01,562 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1672231888656_0008_000001
2023-02-18 18:50:01,578 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e02_1672231888656_0008_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1672231888656_0008_000001
2023-02-18 18:50:01,578 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0008_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-02-18 18:50:02,557 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0008_01_000001 Container Transitioned from ACQUIRED to RUNNING
2023-02-18 18:50:05,883 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1672231888656_0008_000001 (auth:SIMPLE)
2023-02-18 18:50:05,889 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1672231888656_0008_000001
2023-02-18 18:50:05,889 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	IP=10.130.0.5	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1672231888656_0008	APPATTEMPTID=appattempt_1672231888656_0008_000001
2023-02-18 18:50:05,889 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0008_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-02-18 18:50:05,890 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0008 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-02-18 18:50:06,408 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1672231888656_0008_000001 with final state: FINISHING, and exit status: -1000
2023-02-18 18:50:06,409 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0008_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2023-02-18 18:50:06,409 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1672231888656_0008 with final state: FINISHING
2023-02-18 18:50:06,409 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0008 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2023-02-18 18:50:06,409 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1672231888656_0008
2023-02-18 18:50:06,409 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0008_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2023-02-18 18:50:06,409 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0008 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2023-02-18 18:50:07,414 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1672231888656_0008 unregistered successfully. 
2023-02-18 18:50:12,797 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1672231888656_0008_000001
2023-02-18 18:50:12,797 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0008_01_000001 Container Transitioned from RUNNING to COMPLETED
2023-02-18 18:50:12,797 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1672231888656_0008_000001
2023-02-18 18:50:12,797 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0008	CONTAINERID=container_e02_1672231888656_0008_01_000001	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-02-18 18:50:12,797 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0008_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2023-02-18 18:50:12,798 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0008 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2023-02-18 18:50:12,798 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1672231888656_0008_000001 is done. finalState=FINISHED
2023-02-18 18:50:12,798 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1672231888656_0008 requests cleared
2023-02-18 18:50:12,798 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0008
2023-02-18 18:50:12,798 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1672231888656_0008 user: student48 queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-02-18 18:50:12,798 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1672231888656_0008 user: student48 leaf-queue of parent: root #applications: 0
2023-02-18 18:50:12,798 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0008,name=authors.jar,user=student48,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0008/,appMasterHost=adh-test-00-mnode2.ru-central1.internal,submitTime=1676746201437,startTime=1676746201438,finishTime=1676746206409,finalStatus=FAILED,memorySeconds=23021,vcoreSeconds=11,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=23021 MB-seconds\, 11 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2023-02-18 18:50:12,799 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1672231888656_0008_000001
2023-02-18 18:51:06,282 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=dr.who	OPERATION=Get Applications Request	TARGET=ClientRMService	RESULT=SUCCESS
2023-02-18 18:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 18:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-02-18 18:53:12,559 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 9
2023-02-18 18:53:14,493 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1672231888656_0009' is submitted without priority hence considering default queue/cluster priority: 0
2023-02-18 18:53:14,493 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1672231888656_0009
2023-02-18 18:53:14,493 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 9 submitted by user student48
2023-02-18 18:53:14,493 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	IP=10.130.0.4	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1672231888656_0009	CALLERCONTEXT=CLI	QUEUENAME=default
2023-02-18 18:53:14,494 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1672231888656_0009
2023-02-18 18:53:14,494 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0009 State change from NEW to NEW_SAVING on event = START
2023-02-18 18:53:14,494 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1672231888656_0009
2023-02-18 18:53:14,494 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0009 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2023-02-18 18:53:14,495 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1672231888656_0009 user: student48 leaf-queue of parent: root #applications: 1
2023-02-18 18:53:14,495 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1672231888656_0009 from user: student48, in queue: default
2023-02-18 18:53:14,495 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0009 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2023-02-18 18:53:14,495 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1672231888656_0009_000001
2023-02-18 18:53:14,495 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0009_000001 State change from NEW to SUBMITTED on event = START
2023-02-18 18:53:14,495 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1672231888656_0009 from user: student48 activated in queue: default
2023-02-18 18:53:14,495 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1672231888656_0009 user: student48, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-02-18 18:53:14,495 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1672231888656_0009_000001 to scheduler from user student48 in queue default
2023-02-18 18:53:14,497 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0009_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-02-18 18:53:15,048 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0009_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-02-18 18:53:15,048 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000001 Container Transitioned from NEW to ALLOCATED
2023-02-18 18:53:15,048 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0009_01_000001 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2023-02-18 18:53:15,048 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000001	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-02-18 18:53:15,049 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e02_1672231888656_0009_01_000001
2023-02-18 18:53:15,050 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-02-18 18:53:15,050 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1672231888656_0009_000001
2023-02-18 18:53:15,050 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1672231888656_0009 AttemptId: appattempt_1672231888656_0009_000001 MasterContainer: Container: [ContainerId: container_e02_1672231888656_0009_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ]
2023-02-18 18:53:15,050 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.0967361 absoluteUsedCapacity=0.0967361 used=<memory:2048, vCores:1> cluster=<memory:21171, vCores:6>
2023-02-18 18:53:15,050 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-02-18 18:53:15,050 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0009_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-02-18 18:53:15,054 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0009_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-02-18 18:53:15,054 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1672231888656_0009_000001
2023-02-18 18:53:15,055 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e02_1672231888656_0009_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1672231888656_0009_000001
2023-02-18 18:53:15,055 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1672231888656_0009_000001
2023-02-18 18:53:15,055 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1672231888656_0009_000001
2023-02-18 18:53:15,080 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e02_1672231888656_0009_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:2048, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1672231888656_0009_000001
2023-02-18 18:53:15,080 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0009_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-02-18 18:53:16,049 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000001 Container Transitioned from ACQUIRED to RUNNING
2023-02-18 18:53:19,269 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1672231888656_0009_000001 (auth:SIMPLE)
2023-02-18 18:53:19,274 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1672231888656_0009_000001
2023-02-18 18:53:19,274 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	IP=10.130.0.5	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1672231888656_0009	APPATTEMPTID=appattempt_1672231888656_0009_000001
2023-02-18 18:53:19,274 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0009_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-02-18 18:53:19,274 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0009 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-02-18 18:53:21,055 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0009_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-02-18 18:53:21,055 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000002 Container Transitioned from NEW to ALLOCATED
2023-02-18 18:53:21,055 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0009_01_000002 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-02-18 18:53:21,055 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000002	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:21,055 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.14510415 absoluteUsedCapacity=0.14510415 used=<memory:3072, vCores:2> cluster=<memory:21171, vCores:6>
2023-02-18 18:53:21,055 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-02-18 18:53:21,068 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0009_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-02-18 18:53:21,068 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000003 Container Transitioned from NEW to ALLOCATED
2023-02-18 18:53:21,068 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0009_01_000003 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 2 containers, <memory:3072, vCores:2> used and <memory:3985, vCores:0> available after allocation
2023-02-18 18:53:21,068 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000003	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:21,069 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.1934722 absoluteUsedCapacity=0.1934722 used=<memory:4096, vCores:3> cluster=<memory:21171, vCores:6>
2023-02-18 18:53:21,069 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-02-18 18:53:21,173 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0009_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-02-18 18:53:21,173 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000004 Container Transitioned from NEW to ALLOCATED
2023-02-18 18:53:21,173 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0009_01_000004 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-02-18 18:53:21,174 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000004	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:21,174 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.24184026 absoluteUsedCapacity=0.24184026 used=<memory:5120, vCores:4> cluster=<memory:21171, vCores:6>
2023-02-18 18:53:21,174 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-02-18 18:53:21,345 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_e02_1672231888656_0009_01_000002
2023-02-18 18:53:21,346 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2023-02-18 18:53:21,347 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e02_1672231888656_0009_01_000003
2023-02-18 18:53:21,348 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2023-02-18 18:53:21,348 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode1.ru-central1.internal:8041 for container : container_e02_1672231888656_0009_01_000004
2023-02-18 18:53:21,349 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000004 Container Transitioned from ALLOCATED to ACQUIRED
2023-02-18 18:53:22,055 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000002 Container Transitioned from ACQUIRED to RUNNING
2023-02-18 18:53:22,056 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0009_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-02-18 18:53:22,056 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000005 Container Transitioned from NEW to ALLOCATED
2023-02-18 18:53:22,056 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0009_01_000005 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 2 containers, <memory:2048, vCores:2> used and <memory:5009, vCores:0> available after allocation
2023-02-18 18:53:22,056 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000005	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:22,056 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.2902083 absoluteUsedCapacity=0.2902083 used=<memory:6144, vCores:5> cluster=<memory:21171, vCores:6>
2023-02-18 18:53:22,056 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-02-18 18:53:22,075 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000003 Container Transitioned from ACQUIRED to RUNNING
2023-02-18 18:53:22,174 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000004 Container Transitioned from ACQUIRED to RUNNING
2023-02-18 18:53:22,373 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000005 Container Transitioned from ALLOCATED to ACQUIRED
2023-02-18 18:53:23,059 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000005 Container Transitioned from ACQUIRED to RUNNING
2023-02-18 18:53:23,060 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0009_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-02-18 18:53:23,061 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000006 Container Transitioned from NEW to ALLOCATED
2023-02-18 18:53:23,061 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0009_01_000006 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 3 containers, <memory:3072, vCores:3> used and <memory:3985, vCores:-1> available after allocation
2023-02-18 18:53:23,061 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000006	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:23,061 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.33857635 absoluteUsedCapacity=0.33857635 used=<memory:7168, vCores:6> cluster=<memory:21171, vCores:6>
2023-02-18 18:53:23,061 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-02-18 18:53:23,380 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1672231888656_0009
2023-02-18 18:53:23,382 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000006 Container Transitioned from ALLOCATED to ACQUIRED
2023-02-18 18:53:24,282 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000003 Container Transitioned from RUNNING to COMPLETED
2023-02-18 18:53:24,282 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000003	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:24,406 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000006 Container Transitioned from ACQUIRED to RELEASED
2023-02-18 18:53:24,406 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	IP=10.130.0.5	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000006	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:25,072 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0009_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-02-18 18:53:25,072 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000007 Container Transitioned from NEW to ALLOCATED
2023-02-18 18:53:25,072 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0009_01_000007 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 3 containers, <memory:3072, vCores:3> used and <memory:3985, vCores:-1> available after allocation
2023-02-18 18:53:25,072 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000007	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:25,073 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.2902083 absoluteUsedCapacity=0.2902083 used=<memory:6144, vCores:5> cluster=<memory:21171, vCores:6>
2023-02-18 18:53:25,073 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-02-18 18:53:25,219 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000004 Container Transitioned from RUNNING to COMPLETED
2023-02-18 18:53:25,219 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000004	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:25,285 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000002 Container Transitioned from RUNNING to COMPLETED
2023-02-18 18:53:25,286 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000002	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:25,418 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000007 Container Transitioned from ALLOCATED to ACQUIRED
2023-02-18 18:53:25,764 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000007 Container Transitioned from ACQUIRED to RUNNING
2023-02-18 18:53:25,765 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000005 Container Transitioned from RUNNING to COMPLETED
2023-02-18 18:53:25,765 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000005	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:25,767 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0009_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-02-18 18:53:25,767 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000008 Container Transitioned from NEW to ALLOCATED
2023-02-18 18:53:25,767 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0009_01_000008 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 2 containers, <memory:2048, vCores:2> used and <memory:5009, vCores:0> available after allocation
2023-02-18 18:53:25,768 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000008	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:25,768 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.1934722 absoluteUsedCapacity=0.1934722 used=<memory:4096, vCores:3> cluster=<memory:21171, vCores:6>
2023-02-18 18:53:25,768 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-02-18 18:53:26,219 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0009_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-02-18 18:53:26,219 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000009 Container Transitioned from NEW to ALLOCATED
2023-02-18 18:53:26,219 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0009_01_000009 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-02-18 18:53:26,219 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000009	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:26,220 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.24184026 absoluteUsedCapacity=0.24184026 used=<memory:5120, vCores:4> cluster=<memory:21171, vCores:6>
2023-02-18 18:53:26,220 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-02-18 18:53:26,284 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0009_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-02-18 18:53:26,284 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000010 Container Transitioned from NEW to ALLOCATED
2023-02-18 18:53:26,284 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0009_01_000010 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 2 containers, <memory:3072, vCores:2> used and <memory:3985, vCores:0> available after allocation
2023-02-18 18:53:26,284 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000010	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:26,285 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.2902083 absoluteUsedCapacity=0.2902083 used=<memory:6144, vCores:5> cluster=<memory:21171, vCores:6>
2023-02-18 18:53:26,285 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-02-18 18:53:26,428 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000008 Container Transitioned from ALLOCATED to ACQUIRED
2023-02-18 18:53:26,430 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000009 Container Transitioned from ALLOCATED to ACQUIRED
2023-02-18 18:53:26,431 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000010 Container Transitioned from ALLOCATED to ACQUIRED
2023-02-18 18:53:26,775 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000008 Container Transitioned from ACQUIRED to RUNNING
2023-02-18 18:53:26,777 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0009_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-02-18 18:53:26,777 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000011 Container Transitioned from NEW to ALLOCATED
2023-02-18 18:53:26,777 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0009_01_000011 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 3 containers, <memory:3072, vCores:3> used and <memory:3985, vCores:-1> available after allocation
2023-02-18 18:53:26,777 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000011	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:26,778 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.33857635 absoluteUsedCapacity=0.33857635 used=<memory:7168, vCores:6> cluster=<memory:21171, vCores:6>
2023-02-18 18:53:26,778 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-02-18 18:53:27,225 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000009 Container Transitioned from ACQUIRED to RUNNING
2023-02-18 18:53:27,226 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0009_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-02-18 18:53:27,226 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000012 Container Transitioned from NEW to ALLOCATED
2023-02-18 18:53:27,226 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0009_01_000012 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 2 containers, <memory:2048, vCores:2> used and <memory:5009, vCores:0> available after allocation
2023-02-18 18:53:27,226 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000012	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:27,227 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.3869444 absoluteUsedCapacity=0.3869444 used=<memory:8192, vCores:7> cluster=<memory:21171, vCores:6>
2023-02-18 18:53:27,227 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-02-18 18:53:27,443 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000010 Container Transitioned from ACQUIRED to RELEASED
2023-02-18 18:53:27,443 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	IP=10.130.0.5	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000010	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:27,444 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1672231888656_0009
2023-02-18 18:53:27,447 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000011 Container Transitioned from ALLOCATED to ACQUIRED
2023-02-18 18:53:27,450 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000012 Container Transitioned from ALLOCATED to ACQUIRED
2023-02-18 18:53:28,456 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000011 Container Transitioned from ACQUIRED to RELEASED
2023-02-18 18:53:28,457 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	IP=10.130.0.5	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000011	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:28,458 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000012 Container Transitioned from ACQUIRED to RELEASED
2023-02-18 18:53:28,458 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	IP=10.130.0.5	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000012	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:28,459 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: blacklist are updated in Scheduler.blacklistAdditions: [adh-test-00-mnode3.ru-central1.internal], blacklistRemovals: []
2023-02-18 18:53:28,563 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000007 Container Transitioned from RUNNING to COMPLETED
2023-02-18 18:53:28,563 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000007	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:29,225 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0009_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-02-18 18:53:29,225 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000013 Container Transitioned from NEW to ALLOCATED
2023-02-18 18:53:29,225 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0009_01_000013 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 2 containers, <memory:2048, vCores:2> used and <memory:5009, vCores:0> available after allocation
2023-02-18 18:53:29,226 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000013	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:29,226 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.24184026 absoluteUsedCapacity=0.24184026 used=<memory:5120, vCores:4> cluster=<memory:21171, vCores:6>
2023-02-18 18:53:29,226 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-02-18 18:53:29,464 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000013 Container Transitioned from ALLOCATED to ACQUIRED
2023-02-18 18:53:29,464 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: blacklist are updated in Scheduler.blacklistAdditions: [], blacklistRemovals: [adh-test-00-mnode3.ru-central1.internal]
2023-02-18 18:53:29,469 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000008 Container Transitioned from RUNNING to COMPLETED
2023-02-18 18:53:29,469 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000008	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:29,471 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0009_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-02-18 18:53:29,471 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000014 Container Transitioned from NEW to ALLOCATED
2023-02-18 18:53:29,471 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0009_01_000014 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-02-18 18:53:29,471 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000014	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:29,471 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.24184026 absoluteUsedCapacity=0.24184026 used=<memory:5120, vCores:4> cluster=<memory:21171, vCores:6>
2023-02-18 18:53:29,471 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-02-18 18:53:29,646 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000013 Container Transitioned from ACQUIRED to RUNNING
2023-02-18 18:53:29,647 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000009 Container Transitioned from RUNNING to COMPLETED
2023-02-18 18:53:29,647 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000009	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:29,648 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0009_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-02-18 18:53:29,648 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000015 Container Transitioned from NEW to ALLOCATED
2023-02-18 18:53:29,648 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0009_01_000015 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 2 containers, <memory:2048, vCores:2> used and <memory:5009, vCores:0> available after allocation
2023-02-18 18:53:29,648 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000015	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:29,648 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.24184026 absoluteUsedCapacity=0.24184026 used=<memory:5120, vCores:4> cluster=<memory:21171, vCores:6>
2023-02-18 18:53:29,648 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-02-18 18:53:30,475 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000014 Container Transitioned from ALLOCATED to ACQUIRED
2023-02-18 18:53:30,479 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000015 Container Transitioned from ALLOCATED to ACQUIRED
2023-02-18 18:53:30,648 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0009_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-02-18 18:53:30,648 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000016 Container Transitioned from NEW to ALLOCATED
2023-02-18 18:53:30,648 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0009_01_000016 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 3 containers, <memory:3072, vCores:3> used and <memory:3985, vCores:-1> available after allocation
2023-02-18 18:53:30,648 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000016	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:30,648 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.2902083 absoluteUsedCapacity=0.2902083 used=<memory:6144, vCores:5> cluster=<memory:21171, vCores:6>
2023-02-18 18:53:30,648 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-02-18 18:53:31,475 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000014 Container Transitioned from ACQUIRED to RUNNING
2023-02-18 18:53:31,489 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000015 Container Transitioned from ACQUIRED to RELEASED
2023-02-18 18:53:31,489 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	IP=10.130.0.5	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000015	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:31,489 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1672231888656_0009
2023-02-18 18:53:31,491 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000016 Container Transitioned from ALLOCATED to ACQUIRED
2023-02-18 18:53:32,497 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000016 Container Transitioned from ACQUIRED to RELEASED
2023-02-18 18:53:32,497 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	IP=10.130.0.5	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000016	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:32,744 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000013 Container Transitioned from RUNNING to COMPLETED
2023-02-18 18:53:32,744 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000013	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:33,164 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000014 Container Transitioned from RUNNING to COMPLETED
2023-02-18 18:53:33,164 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000014	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:33,745 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0009_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-02-18 18:53:33,746 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000017 Container Transitioned from NEW to ALLOCATED
2023-02-18 18:53:33,746 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0009_01_000017 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-02-18 18:53:33,746 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000017	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:33,746 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.14510415 absoluteUsedCapacity=0.14510415 used=<memory:3072, vCores:2> cluster=<memory:21171, vCores:6>
2023-02-18 18:53:33,746 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-02-18 18:53:34,506 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000017 Container Transitioned from ALLOCATED to ACQUIRED
2023-02-18 18:53:34,747 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000017 Container Transitioned from ACQUIRED to RUNNING
2023-02-18 18:53:35,514 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1672231888656_0009
2023-02-18 18:53:37,277 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1672231888656_0009_000001 with final state: FINISHING, and exit status: -1000
2023-02-18 18:53:37,277 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0009_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2023-02-18 18:53:37,277 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1672231888656_0009 with final state: FINISHING
2023-02-18 18:53:37,277 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0009 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2023-02-18 18:53:37,278 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1672231888656_0009
2023-02-18 18:53:37,278 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0009_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2023-02-18 18:53:37,278 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0009 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2023-02-18 18:53:37,296 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000017 Container Transitioned from RUNNING to COMPLETED
2023-02-18 18:53:37,296 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000017	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-02-18 18:53:38,281 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1672231888656_0009 unregistered successfully. 
2023-02-18 18:53:43,656 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1672231888656_0009_000001
2023-02-18 18:53:43,656 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0009_01_000001 Container Transitioned from RUNNING to COMPLETED
2023-02-18 18:53:43,656 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1672231888656_0009_000001
2023-02-18 18:53:43,656 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0009	CONTAINERID=container_e02_1672231888656_0009_01_000001	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-02-18 18:53:43,656 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0009_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2023-02-18 18:53:43,657 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0009 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2023-02-18 18:53:43,657 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1672231888656_0009_000001 is done. finalState=FINISHED
2023-02-18 18:53:43,657 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0009
2023-02-18 18:53:43,657 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1672231888656_0009 requests cleared
2023-02-18 18:53:43,657 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1672231888656_0009 user: student48 queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-02-18 18:53:43,657 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1672231888656_0009 user: student48 leaf-queue of parent: root #applications: 0
2023-02-18 18:53:43,657 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1672231888656_0009_000001
2023-02-18 18:53:43,657 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0009,name=authors.jar,user=student48,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0009/,appMasterHost=adh-test-00-mnode2.ru-central1.internal,submitTime=1676746394492,startTime=1676746394493,finishTime=1676746417277,finalStatus=SUCCEEDED,memorySeconds=105366,vcoreSeconds=66,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=105366 MB-seconds\, 66 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2023-02-18 19:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 19:51:28,956 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-18 20:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 20:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-18 21:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 21:51:28,960 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-02-18 22:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 22:51:28,959 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-18 23:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-18 23:51:28,961 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-19 00:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 00:51:28,958 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-19 01:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 01:51:28,955 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-19 02:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 02:51:28,954 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-19 03:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 03:51:28,961 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-19 04:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 04:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-19 05:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 05:51:28,956 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-19 06:51:28,947 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 06:51:28,954 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-19 07:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 07:51:28,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-02-19 08:51:28,946 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 08:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-19 09:51:28,947 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 09:51:28,989 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 43 msec
2023-02-19 10:51:28,947 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 10:51:28,959 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-19 11:51:28,947 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 11:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-19 12:51:28,947 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-19 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-19 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-19 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892605 in 900000ms
2023-02-19 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-19 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220201 in 900000ms
2023-02-19 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-19 12:53:35,798 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-19 12:53:36,872 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 51
2023-02-19 12:53:36,872 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-19 12:53:36,873 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-19 12:53:36,873 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 59
2023-02-19 12:53:36,873 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-19 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220201
2023-02-19 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892605
2023-02-19 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626925
2023-02-19 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-19 13:51:28,947 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 13:51:28,957 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-19 14:51:28,947 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 14:51:28,961 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-19 15:51:28,947 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 15:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-19 16:51:28,947 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 16:51:28,961 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-02-19 17:51:28,947 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 17:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-19 18:51:28,947 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 18:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-19 19:51:28,947 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 19:51:28,954 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-02-19 20:51:28,948 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 20:51:28,959 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-19 21:51:28,948 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 21:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-19 22:51:28,949 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 22:51:28,955 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-19 23:51:28,949 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-19 23:51:28,956 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-02-20 00:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 00:51:28,955 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-20 01:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 01:51:28,959 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-20 02:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 02:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-20 03:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 03:51:28,954 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-20 04:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 04:51:28,961 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-20 05:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 05:51:28,960 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-20 06:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 06:51:28,961 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-20 07:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 07:51:28,959 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-20 08:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 08:51:28,967 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-20 09:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 09:51:28,957 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-20 10:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 10:51:28,965 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-20 11:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 11:51:28,969 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-02-20 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-20 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-20 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-20 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-20 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220200 in 900000ms
2023-02-20 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892606 in 900000ms
2023-02-20 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 12:51:28,963 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-20 12:53:38,234 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-20 12:53:39,308 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 52
2023-02-20 12:53:39,308 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-20 12:53:39,308 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-20 12:53:39,308 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 60
2023-02-20 12:53:39,308 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-20 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626926
2023-02-20 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-20 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220200
2023-02-20 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892606
2023-02-20 13:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 13:51:28,960 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-20 14:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 14:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-20 15:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 15:51:28,969 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-02-20 16:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 16:51:28,957 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-20 17:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 17:51:28,963 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-20 18:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 18:51:28,963 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-20 19:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 19:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-20 20:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 20:51:28,967 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-20 21:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 21:51:28,965 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-20 22:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 22:51:28,961 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-20 23:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-20 23:51:28,956 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-21 00:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 00:51:28,971 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-02-21 01:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 01:51:28,957 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-21 02:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 02:51:28,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-21 03:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 03:51:28,957 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-21 04:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 04:51:29,001 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 49 msec
2023-02-21 05:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 05:51:28,956 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-21 06:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 06:51:28,961 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-21 07:51:28,953 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 07:51:28,956 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-21 08:51:28,953 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 08:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-02-21 09:51:28,953 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 09:51:28,959 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-21 10:51:28,953 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 10:51:28,956 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-21 11:51:28,953 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 11:51:28,958 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-21 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-21 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-21 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-21 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220199 in 900000ms
2023-02-21 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892607 in 900000ms
2023-02-21 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-21 12:51:28,954 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 12:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-02-21 12:53:40,625 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-21 12:53:41,653 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 53
2023-02-21 12:53:41,653 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-21 12:53:41,653 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-21 12:53:41,653 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 61
2023-02-21 12:53:41,653 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-21 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892607
2023-02-21 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220199
2023-02-21 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626927
2023-02-21 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-21 13:51:28,954 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 13:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-21 14:51:28,954 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 14:51:28,967 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-21 15:51:28,954 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 15:51:28,963 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-21 16:51:28,954 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 16:51:28,957 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-21 17:51:28,955 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 17:51:28,958 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-21 18:51:28,955 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 18:51:28,972 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-21 19:51:28,955 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 19:51:28,969 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-21 20:51:28,955 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 20:51:28,960 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-21 21:51:28,956 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 21:51:28,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-21 22:51:28,956 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 22:51:28,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-21 23:51:28,957 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-21 23:51:28,961 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-22 00:51:28,956 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 00:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-22 01:51:28,956 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 01:51:28,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-22 02:51:28,957 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 02:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-22 03:51:28,957 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 03:51:28,961 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-22 04:51:28,957 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 04:51:28,985 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 27 msec
2023-02-22 05:51:28,957 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 05:51:29,001 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 43 msec
2023-02-22 06:51:28,957 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 06:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-02-22 07:51:28,957 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 07:51:28,961 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-22 08:51:28,957 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 08:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-22 09:51:28,957 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 09:51:28,960 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-22 10:51:28,958 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 10:51:28,961 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-22 11:51:28,958 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 11:51:28,963 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-22 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-22 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-22 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-22 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-22 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220198 in 900000ms
2023-02-22 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892608 in 900000ms
2023-02-22 12:51:28,958 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 12:51:28,980 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-02-22 12:53:43,043 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-22 12:53:44,075 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 54
2023-02-22 12:53:44,075 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-22 12:53:44,075 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-22 12:53:44,075 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 62
2023-02-22 12:53:44,075 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-22 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626928
2023-02-22 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-22 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220198
2023-02-22 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892608
2023-02-22 13:51:28,958 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 13:51:28,986 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 28 msec
2023-02-22 14:51:28,958 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 14:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-22 15:51:28,959 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 15:51:28,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-22 16:51:28,960 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 16:51:28,963 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-22 17:51:28,960 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 17:51:28,982 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-02-22 18:51:28,960 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 18:51:28,974 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-22 19:51:28,960 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 19:51:28,974 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-02-22 20:51:28,960 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 20:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-22 21:51:28,960 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 21:51:28,964 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-22 22:51:28,960 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 22:51:28,965 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-22 23:51:28,960 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-22 23:51:28,965 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-23 00:51:28,961 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 00:51:28,965 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-23 01:51:28,961 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 01:51:28,969 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-23 02:51:28,961 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 02:51:28,969 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-02-23 03:51:28,961 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 03:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-02-23 04:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 04:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-02-23 05:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 05:51:28,989 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 27 msec
2023-02-23 06:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 06:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-23 07:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 07:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-23 08:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 08:51:28,989 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 26 msec
2023-02-23 09:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 09:51:28,971 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-23 10:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 10:51:28,967 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-23 11:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 11:51:28,965 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-23 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-23 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-23 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-23 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892609 in 900000ms
2023-02-23 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-23 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220197 in 900000ms
2023-02-23 12:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 12:51:28,978 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-23 12:53:45,475 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-23 12:53:46,389 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 55
2023-02-23 12:53:46,389 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-23 12:53:46,389 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-23 12:53:46,389 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 63
2023-02-23 12:53:46,389 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-23 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626929
2023-02-23 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-23 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892609
2023-02-23 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220197
2023-02-23 13:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 13:51:28,974 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-23 14:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 14:51:28,974 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-23 15:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 15:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-23 16:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 16:51:28,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-23 17:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 17:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-23 18:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 18:51:28,975 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-23 19:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 19:51:28,967 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-23 20:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 20:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-23 21:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 21:51:28,967 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-23 22:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 22:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-23 23:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-23 23:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-02-24 00:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 00:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-02-24 01:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 01:51:28,971 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-24 02:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 02:51:28,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-02-24 03:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 03:51:28,969 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-24 04:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 04:51:28,967 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-24 05:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 05:51:28,972 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-24 06:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 06:51:28,980 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-02-24 07:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 07:51:28,965 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-24 08:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 08:51:28,974 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-24 09:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 09:51:28,967 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-24 10:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 10:51:28,967 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-24 11:51:28,963 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 11:51:28,971 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-24 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-24 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-24 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-24 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220196 in 900000ms
2023-02-24 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-24 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892610 in 900000ms
2023-02-24 12:51:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 12:51:28,977 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-24 12:53:47,895 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-24 12:53:48,817 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 56
2023-02-24 12:53:48,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-24 12:53:48,817 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-24 12:53:48,817 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 64
2023-02-24 12:53:48,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-24 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626930
2023-02-24 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-24 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220196
2023-02-24 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892610
2023-02-24 13:51:28,963 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 13:51:28,997 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 34 msec
2023-02-24 14:51:28,963 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 14:51:28,967 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-24 15:51:28,963 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 15:51:28,975 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-24 16:51:28,963 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 16:51:28,967 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-24 17:51:28,963 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 17:51:28,967 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-24 18:51:28,963 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 18:51:28,982 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-02-24 19:51:28,964 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 19:51:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-02-24 20:51:28,964 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 20:51:28,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-24 21:51:28,964 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 21:51:28,977 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-24 22:51:28,964 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 22:51:28,978 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-02-24 23:51:28,964 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-24 23:51:28,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-25 00:51:28,964 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 00:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-25 01:51:28,964 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 01:51:28,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-25 02:51:28,964 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 02:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-25 03:51:28,964 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 03:51:28,969 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-25 04:51:28,964 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 04:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-25 05:51:28,964 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 05:51:29,028 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 63 msec
2023-02-25 06:51:28,964 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 06:51:28,974 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-25 07:51:28,964 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 07:51:28,980 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-02-25 08:51:28,965 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 08:51:28,974 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-25 09:51:28,965 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 09:51:28,982 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-02-25 10:51:28,965 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 10:51:28,980 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-02-25 11:51:28,965 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 11:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-25 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-25 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-25 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-25 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220195 in 900000ms
2023-02-25 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-25 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892611 in 900000ms
2023-02-25 12:51:28,965 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 12:51:28,976 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-25 12:53:50,253 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-25 12:53:51,227 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 57
2023-02-25 12:53:51,227 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-25 12:53:51,227 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-25 12:53:51,227 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 65
2023-02-25 12:53:51,227 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-25 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626931
2023-02-25 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-25 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220195
2023-02-25 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892611
2023-02-25 13:51:28,965 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 13:51:28,991 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 26 msec
2023-02-25 14:51:28,965 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 14:51:28,969 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-25 15:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 15:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-25 16:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 16:51:28,972 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-25 17:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 17:51:28,971 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-25 18:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 18:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-25 19:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 19:51:28,986 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-02-25 20:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 20:51:28,972 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-25 21:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 21:51:28,972 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-25 22:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 22:51:28,972 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-25 23:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-25 23:51:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-02-26 00:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 00:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-26 01:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 01:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-02-26 02:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 02:51:28,982 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-26 03:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 03:51:28,980 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-02-26 04:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 04:51:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-02-26 05:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 05:51:28,971 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-26 06:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 06:51:28,969 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-26 07:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 07:51:28,980 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-26 08:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 08:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-26 09:51:28,966 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 09:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-26 10:51:28,967 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 10:51:28,989 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-02-26 11:51:28,967 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 11:51:28,980 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-02-26 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-26 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-26 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-26 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-26 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220194 in 900000ms
2023-02-26 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892612 in 900000ms
2023-02-26 12:51:28,967 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 12:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-26 12:53:52,669 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-26 12:53:53,657 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 58
2023-02-26 12:53:53,658 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-26 12:53:53,658 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-26 12:53:53,658 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 66
2023-02-26 12:53:53,658 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-26 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626932
2023-02-26 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-26 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892612
2023-02-26 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220194
2023-02-26 13:51:28,967 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 13:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-26 14:51:28,967 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 14:51:28,982 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-26 15:51:28,967 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 15:51:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-02-26 16:51:28,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 16:51:28,972 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-26 17:51:28,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 17:51:28,994 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-02-26 18:51:28,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 18:51:28,972 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-26 19:51:28,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 19:51:28,980 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-26 20:51:28,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 20:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-26 21:51:28,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 21:51:28,972 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-26 22:51:28,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 22:51:29,003 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 35 msec
2023-02-26 23:51:28,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-26 23:51:28,982 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-27 00:51:28,969 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 00:51:28,995 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 26 msec
2023-02-27 01:51:28,969 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 01:51:28,973 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-27 02:51:28,969 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 02:51:28,974 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-27 03:51:28,969 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 03:51:28,974 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-27 04:51:28,969 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 04:51:28,972 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-27 05:51:28,969 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 05:51:28,975 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-27 06:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 06:51:28,978 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-02-27 07:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 07:51:28,980 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-27 08:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 08:51:28,974 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-27 09:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 09:51:28,985 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-27 10:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 10:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-02-27 11:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 11:51:28,974 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-27 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-27 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-27 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-27 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220193 in 900000ms
2023-02-27 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-27 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892613 in 900000ms
2023-02-27 12:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 12:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-27 12:53:55,111 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-27 12:53:56,030 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 59
2023-02-27 12:53:56,030 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-27 12:53:56,030 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-27 12:53:56,030 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 67
2023-02-27 12:53:56,030 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-27 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220193
2023-02-27 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892613
2023-02-27 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626933
2023-02-27 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-27 13:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 13:51:28,999 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 29 msec
2023-02-27 14:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 14:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-27 15:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 15:51:28,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-02-27 16:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 16:51:28,975 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-27 17:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 17:51:28,982 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-27 18:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 18:51:28,975 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-27 19:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 19:51:28,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-02-27 20:51:28,970 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 20:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-27 21:51:28,971 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 21:51:28,988 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-02-27 22:51:28,971 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 22:51:28,976 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-27 23:51:28,971 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-27 23:51:28,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-02-28 00:51:28,972 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 00:51:28,988 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-02-28 01:51:28,973 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 01:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-02-28 02:51:28,974 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 02:51:28,978 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-28 03:51:28,974 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 03:51:28,990 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-28 04:51:28,974 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 04:51:28,977 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-28 05:51:28,974 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 05:51:28,991 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-02-28 06:51:28,975 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 06:51:28,978 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-02-28 07:51:28,975 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 07:51:28,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-02-28 08:51:28,975 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 08:51:28,980 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-28 09:51:28,975 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 09:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-28 10:51:28,975 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 10:51:28,994 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-02-28 11:51:28,975 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 11:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-02-28 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-02-28 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-28 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-02-28 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220192 in 900000ms
2023-02-28 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-02-28 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892614 in 900000ms
2023-02-28 12:51:28,975 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 12:51:28,987 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-28 12:53:57,527 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-28 12:53:58,448 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 60
2023-02-28 12:53:58,448 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-02-28 12:53:58,449 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-02-28 12:53:58,449 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 68
2023-02-28 12:53:58,449 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-02-28 13:06:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626934
2023-02-28 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-02-28 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220192
2023-02-28 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892614
2023-02-28 13:51:28,975 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 13:51:28,995 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-02-28 14:51:28,976 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 14:51:28,980 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-28 15:51:28,976 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 15:51:28,980 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-02-28 16:51:28,977 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 16:51:28,991 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-28 17:51:28,977 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 17:51:28,992 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-02-28 18:51:28,977 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 18:51:28,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-02-28 19:51:28,977 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 19:51:29,001 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-02-28 20:51:28,977 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 20:51:28,988 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-02-28 21:51:28,978 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 21:51:28,990 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-02-28 22:51:28,977 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 22:51:28,994 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-02-28 23:51:28,977 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-02-28 23:51:28,994 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-03-01 00:51:28,977 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 00:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-01 01:51:28,977 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 01:51:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-01 02:51:28,977 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 02:51:28,999 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-03-01 03:51:28,977 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 03:51:29,020 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 43 msec
2023-03-01 04:51:28,977 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 04:51:28,997 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-03-01 05:51:28,977 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 05:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-01 06:51:28,977 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 06:51:28,982 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-01 07:51:28,977 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 07:51:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-01 08:51:28,977 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 08:51:28,987 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-01 09:51:28,977 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 09:51:28,986 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-01 10:51:28,978 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 10:51:28,982 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-01 11:51:28,978 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 11:51:28,988 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-01 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-01 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-01 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-01 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220191 in 900000ms
2023-03-01 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-01 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892615 in 900000ms
2023-03-01 12:51:28,978 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 12:51:28,997 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-03-01 12:53:59,948 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-01 12:54:00,851 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 61
2023-03-01 12:54:00,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-01 12:54:00,851 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-01 12:54:00,851 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 69
2023-03-01 12:54:00,851 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-01 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626935
2023-03-01 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-01 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220191
2023-03-01 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892615
2023-03-01 13:51:28,978 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 13:51:28,998 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-03-01 14:51:28,978 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 14:51:28,982 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-01 15:51:28,978 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 15:51:28,982 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-01 16:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 16:51:28,992 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-01 17:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 17:51:28,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-01 18:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 18:51:28,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-01 19:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 19:51:28,996 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-03-01 20:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 20:51:28,992 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-01 21:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 21:51:28,988 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-01 22:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 22:51:28,987 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-03-01 23:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-01 23:51:28,994 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-02 00:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 00:51:29,011 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 31 msec
2023-03-02 01:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 01:51:28,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-02 02:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 02:51:28,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-02 03:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 03:51:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-02 04:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 04:51:28,988 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-02 05:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 05:51:28,988 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-02 06:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 06:51:28,992 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-02 07:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 07:51:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-02 08:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 08:51:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-02 09:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 09:51:28,988 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-02 10:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 10:51:28,994 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-02 11:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 11:51:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-02 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-02 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-02 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-02 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220190 in 900000ms
2023-03-02 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-02 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892616 in 900000ms
2023-03-02 12:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 12:51:29,037 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 58 msec
2023-03-02 12:54:02,281 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-02 12:54:03,272 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 62
2023-03-02 12:54:03,272 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-02 12:54:03,272 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-02 12:54:03,272 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 70
2023-03-02 12:54:03,272 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-02 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626936
2023-03-02 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220190
2023-03-02 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-02 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892616
2023-03-02 13:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 13:51:28,989 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-02 14:51:28,979 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 14:51:28,989 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-02 15:51:28,980 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 15:51:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-02 16:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 16:51:28,992 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-02 17:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 17:51:28,986 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-02 18:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 18:51:28,990 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-02 19:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 19:51:28,993 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-02 20:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 20:51:28,999 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-03-02 21:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 21:51:29,007 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-03-02 22:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 22:51:28,991 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-02 23:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-02 23:51:28,986 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-03 00:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 00:51:28,986 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-03 01:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 01:51:28,985 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-03 02:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 02:51:28,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-03 03:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 03:51:28,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-03 04:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 04:51:28,986 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-03 05:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 05:51:28,985 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-03 06:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 06:51:28,985 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-03 07:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 07:51:28,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-03 08:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 08:51:28,997 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-03-03 09:51:28,981 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 09:51:28,985 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-03 10:51:28,982 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 10:51:29,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 27 msec
2023-03-03 11:51:28,982 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 11:51:28,987 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-03 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-03 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-03 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-03 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892617 in 900000ms
2023-03-03 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-03 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220189 in 900000ms
2023-03-03 12:51:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 12:51:29,015 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 32 msec
2023-03-03 12:54:04,717 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-03 12:54:05,613 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 63
2023-03-03 12:54:05,613 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-03 12:54:05,613 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-03 12:54:05,613 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 71
2023-03-03 12:54:05,613 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-03 13:06:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626937
2023-03-03 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-03 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892617
2023-03-03 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220189
2023-03-03 13:51:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 13:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-03-03 14:51:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 14:51:28,991 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-03-03 15:51:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 15:51:28,988 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-03 16:51:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 16:51:29,011 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 28 msec
2023-03-03 17:51:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 17:51:28,989 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-03 18:51:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 18:51:28,995 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-03 19:51:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 19:51:29,002 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-03-03 20:51:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 20:51:28,995 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-03 21:51:28,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 21:51:28,987 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-03 22:51:28,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 22:51:28,998 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-03 23:51:28,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-03 23:51:28,988 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-04 00:51:28,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 00:51:28,988 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-04 01:51:28,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 01:51:29,000 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-03-04 02:51:28,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 02:51:28,999 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-04 03:51:28,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 03:51:28,988 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-04 04:51:28,985 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 04:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-03-04 05:51:28,986 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 05:51:28,993 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-03-04 06:51:28,987 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 06:51:29,004 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-03-04 07:51:28,987 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 07:51:28,991 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-04 08:51:28,987 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 08:51:28,991 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-04 09:51:28,987 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 09:51:28,990 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-04 10:51:28,988 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 10:51:28,992 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-04 11:51:28,989 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 11:51:28,997 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-03-04 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-04 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-04 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-04 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220188 in 900000ms
2023-03-04 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-04 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892618 in 900000ms
2023-03-04 12:51:28,989 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 12:51:29,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-03-04 12:54:07,112 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-04 12:54:08,028 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 64
2023-03-04 12:54:08,028 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-04 12:54:08,028 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-04 12:54:08,028 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 72
2023-03-04 12:54:08,028 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-04 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626938
2023-03-04 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-04 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220188
2023-03-04 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892618
2023-03-04 13:51:28,989 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 13:51:29,008 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-03-04 14:51:28,989 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 14:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-03-04 15:51:28,989 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 15:51:29,010 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-03-04 16:51:28,989 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 16:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-03-04 17:51:28,989 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 17:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-03-04 18:51:28,989 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 18:51:28,993 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-04 19:51:28,990 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 19:51:29,005 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-04 20:51:28,990 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 20:51:29,003 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-04 21:51:28,990 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 21:51:29,016 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-03-04 22:51:28,990 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 22:51:28,996 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-04 23:51:28,991 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-04 23:51:29,005 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-05 00:51:28,991 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 00:51:28,996 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-05 01:51:28,991 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 01:51:29,003 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-05 02:51:28,991 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 02:51:29,005 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-05 03:51:28,991 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 03:51:29,007 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-05 04:51:28,991 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 04:51:29,008 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-03-05 05:51:28,992 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 05:51:28,996 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-05 06:51:28,993 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 06:51:29,005 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-05 07:51:28,993 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 07:51:29,038 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 45 msec
2023-03-05 08:51:28,993 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 08:51:28,996 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-05 09:51:28,994 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 09:51:28,998 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-05 10:51:28,995 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 10:51:29,013 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-03-05 11:51:28,996 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 11:51:29,003 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-03-05 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-05 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-05 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-05 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220187 in 900000ms
2023-03-05 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-05 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892619 in 900000ms
2023-03-05 12:51:28,996 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 12:51:29,018 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-03-05 12:54:09,528 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-05 12:54:10,447 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 65
2023-03-05 12:54:10,447 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-05 12:54:10,447 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-05 12:54:10,447 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 73
2023-03-05 12:54:10,447 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-05 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626939
2023-03-05 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-05 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220187
2023-03-05 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892619
2023-03-05 13:51:28,996 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 13:51:29,017 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-03-05 14:51:28,996 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 14:51:29,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-05 15:51:28,996 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 15:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-05 16:51:28,996 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 16:51:29,011 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-05 17:51:28,996 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 17:51:29,000 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-05 18:51:28,996 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 18:51:29,026 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 29 msec
2023-03-05 19:51:28,996 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 19:51:29,010 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-05 20:51:28,996 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 20:51:29,000 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-05 21:51:28,996 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 21:51:29,020 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-03-05 22:51:28,996 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 22:51:29,000 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-05 23:51:28,996 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-05 23:51:29,007 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-06 00:51:28,996 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 00:51:29,008 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-06 01:51:28,996 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 01:51:29,016 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-03-06 02:51:28,996 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 02:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-06 03:51:28,996 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 03:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-06 04:51:28,997 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 04:51:29,005 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-03-06 05:51:28,997 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 05:51:29,024 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 27 msec
2023-03-06 06:51:28,997 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 06:51:29,002 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-06 07:51:28,997 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 07:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-06 08:51:28,997 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 08:51:29,003 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-06 09:51:28,997 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 09:51:29,004 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-06 10:51:28,997 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 10:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-06 11:51:28,997 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 11:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-06 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-06 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-06 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-06 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892620 in 900000ms
2023-03-06 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-06 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220186 in 900000ms
2023-03-06 12:51:28,997 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 12:51:29,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-06 12:54:11,879 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-06 12:54:12,859 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 66
2023-03-06 12:54:12,859 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-06 12:54:12,859 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-06 12:54:12,859 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 74
2023-03-06 12:54:12,859 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-06 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626940
2023-03-06 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892620
2023-03-06 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-06 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220186
2023-03-06 13:51:28,997 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 13:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-06 14:51:28,997 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 14:51:29,002 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-06 15:51:28,998 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 15:51:29,003 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-06 16:51:28,998 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 16:51:29,002 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-06 17:51:28,998 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 17:51:29,003 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-06 18:51:28,998 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 18:51:29,003 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-06 19:51:28,998 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 19:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-06 20:51:28,998 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 20:51:29,013 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-06 21:51:28,998 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 21:51:29,003 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-06 22:51:28,998 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 22:51:29,008 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-06 23:51:28,999 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-06 23:51:29,008 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-07 00:51:28,999 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 00:51:29,011 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-07 01:51:28,999 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 01:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-07 02:51:28,999 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 02:51:29,008 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-07 03:51:28,999 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 03:51:29,004 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-07 04:51:28,999 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 04:51:29,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-07 05:51:28,999 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 05:51:29,003 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-07 06:51:28,999 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 06:51:29,024 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-03-07 07:51:28,999 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 07:51:29,008 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-07 08:51:29,000 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 08:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-07 09:51:28,999 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 09:51:29,003 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-07 10:51:28,999 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 10:51:29,002 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-07 11:51:29,000 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 11:51:29,025 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 26 msec
2023-03-07 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-07 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-07 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-07 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220185 in 900000ms
2023-03-07 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-07 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892621 in 900000ms
2023-03-07 12:51:29,001 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 12:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-07 12:54:14,294 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-07 12:54:15,286 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 67
2023-03-07 12:54:15,286 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-07 12:54:15,286 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-07 12:54:15,286 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 75
2023-03-07 12:54:15,286 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-07 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220185
2023-03-07 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626941
2023-03-07 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892621
2023-03-07 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-07 13:51:29,001 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 13:51:29,011 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-07 14:51:29,001 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 14:51:29,007 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-07 15:51:29,001 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 15:51:29,007 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-03-07 16:51:29,002 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 16:51:29,007 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-07 17:51:29,002 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 17:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-07 18:51:29,003 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 18:51:29,007 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-07 19:51:29,003 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 19:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-07 20:51:29,003 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 20:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-07 21:51:29,003 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 21:51:29,013 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-07 22:51:29,003 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 22:51:29,019 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-03-07 23:51:29,003 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-07 23:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-08 00:51:29,003 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 00:51:29,007 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-08 01:51:29,003 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 01:51:29,007 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-08 02:51:29,003 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 02:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-08 03:51:29,004 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 03:51:29,008 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-08 04:51:29,004 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 04:51:29,013 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-08 05:51:29,004 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 05:51:29,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-08 06:51:29,004 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 06:51:29,010 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-03-08 07:51:29,004 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 07:51:29,010 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-03-08 08:51:29,005 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 08:51:29,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-08 09:51:29,005 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 09:51:29,019 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-08 10:51:29,005 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 10:51:29,015 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-08 11:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 11:51:29,010 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-08 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-08 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-08 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-08 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220184 in 900000ms
2023-03-08 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-08 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892622 in 900000ms
2023-03-08 12:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 12:51:29,015 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-08 12:54:16,659 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-08 12:54:17,620 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 68
2023-03-08 12:54:17,620 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-08 12:54:17,620 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-08 12:54:17,620 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 76
2023-03-08 12:54:17,620 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-08 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626942
2023-03-08 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-08 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892622
2023-03-08 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220184
2023-03-08 13:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 13:51:29,017 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-08 14:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 14:51:29,018 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-08 15:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 15:51:29,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-08 16:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 16:51:29,011 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-08 17:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 17:51:29,011 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-08 18:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 18:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-03-08 19:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 19:51:29,020 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-08 20:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 20:51:29,010 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-08 21:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 21:51:29,031 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-03-08 22:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 22:51:29,025 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-03-08 23:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-08 23:51:29,014 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-03-09 00:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 00:51:29,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-09 01:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 01:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-09 02:51:29,006 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 02:51:29,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-09 03:51:29,007 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 03:51:29,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-09 04:51:29,007 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 04:51:29,021 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-09 05:51:29,007 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 05:51:29,011 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-09 06:51:29,008 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 06:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-09 07:51:29,008 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 07:51:29,019 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-09 08:51:29,008 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 08:51:29,011 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-09 09:51:29,008 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 09:51:29,017 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-09 10:51:29,008 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 10:51:29,021 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-09 11:51:29,008 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 11:51:29,030 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-03-09 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-09 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-09 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-09 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892623 in 900000ms
2023-03-09 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-09 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220183 in 900000ms
2023-03-09 12:51:29,008 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 12:51:29,026 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-03-09 12:54:19,088 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-09 12:54:20,038 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 69
2023-03-09 12:54:20,038 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-09 12:54:20,038 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-09 12:54:20,038 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 77
2023-03-09 12:54:20,038 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-09 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626943
2023-03-09 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-09 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892623
2023-03-09 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220183
2023-03-09 13:51:29,008 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 13:51:29,037 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 29 msec
2023-03-09 14:51:29,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 14:51:29,014 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-09 15:51:29,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 15:51:29,020 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-09 16:51:29,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 16:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-09 17:51:29,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 17:51:29,034 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-03-09 18:51:29,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 18:51:29,013 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-09 19:51:29,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 19:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-09 20:51:29,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 20:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-09 21:51:29,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 21:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-09 22:51:29,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 22:51:29,018 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-09 23:51:29,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-09 23:51:29,015 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-10 00:51:29,009 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 00:51:29,013 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-10 01:51:29,010 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 01:51:29,014 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-10 02:51:29,010 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 02:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-10 03:51:29,010 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 03:51:29,015 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-10 04:51:29,011 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 04:51:29,014 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-10 05:51:29,011 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 05:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-10 06:51:29,011 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 06:51:29,033 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-03-10 07:51:29,011 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 07:51:29,017 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-03-10 08:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 08:51:29,015 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-10 09:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 09:51:29,019 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-03-10 10:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 10:51:29,018 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-10 11:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 11:51:29,017 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-10 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-10 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-10 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-10 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892624 in 900000ms
2023-03-10 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-10 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220182 in 900000ms
2023-03-10 12:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 12:51:29,025 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-10 12:54:21,384 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-10 12:54:22,409 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 70
2023-03-10 12:54:22,409 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-10 12:54:22,409 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-10 12:54:22,409 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 78
2023-03-10 12:54:22,409 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-10 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626944
2023-03-10 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-10 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892624
2023-03-10 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220182
2023-03-10 13:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 13:51:29,024 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-10 14:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 14:51:29,017 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-10 15:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 15:51:29,016 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-10 16:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 16:51:29,016 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-10 17:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 17:51:29,026 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-10 18:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 18:51:29,021 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-10 19:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 19:51:29,017 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-10 20:51:29,012 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 20:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-10 21:51:29,013 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 21:51:29,049 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 36 msec
2023-03-10 22:51:29,013 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 22:51:29,017 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-10 23:51:29,013 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-10 23:51:29,016 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-11 00:51:29,014 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 00:51:29,017 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-11 01:51:29,014 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 01:51:29,018 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-11 02:51:29,015 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 02:51:29,019 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-11 03:51:29,015 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 03:51:29,020 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-11 04:51:29,015 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 04:51:29,029 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-11 05:51:29,015 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 05:51:29,020 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-11 06:51:29,015 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 06:51:29,026 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-11 07:51:29,015 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 07:51:29,028 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-11 08:51:29,015 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 08:51:29,036 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-03-11 09:51:29,015 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 09:51:29,034 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-03-11 10:51:29,015 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 10:51:29,025 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-11 11:51:29,015 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 11:51:29,020 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-11 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-11 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-11 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-11 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892625 in 900000ms
2023-03-11 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-11 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220181 in 900000ms
2023-03-11 12:51:29,015 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 12:51:29,032 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-03-11 12:54:23,819 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-11 12:54:24,843 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 71
2023-03-11 12:54:24,843 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-11 12:54:24,844 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-11 12:54:24,844 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 79
2023-03-11 12:54:24,844 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-11 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626945
2023-03-11 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-11 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892625
2023-03-11 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220181
2023-03-11 13:51:29,015 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 13:51:29,024 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-11 14:51:29,015 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 14:51:29,019 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-11 15:51:29,015 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 15:51:29,019 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-11 16:51:29,015 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 16:51:29,019 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-11 17:51:29,016 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 17:51:29,029 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-11 18:51:29,016 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 18:51:29,031 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-11 19:51:29,016 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 19:51:29,040 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-03-11 20:51:29,016 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 20:51:29,028 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-11 21:51:29,016 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 21:51:29,030 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-11 22:51:29,016 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 22:51:29,031 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-11 23:51:29,016 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-11 23:51:29,021 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-12 00:51:29,017 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 00:51:29,020 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-12 01:51:29,017 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 01:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-12 02:51:29,017 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 02:51:29,032 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-12 03:51:29,017 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 03:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-12 04:51:29,017 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 04:51:29,039 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-03-12 05:51:29,017 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 05:51:29,021 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-12 06:51:29,017 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 06:51:29,020 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-12 07:51:29,017 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 07:51:29,021 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-12 08:51:29,018 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 08:51:29,030 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-12 09:51:29,018 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 09:51:29,029 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-12 10:51:29,018 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 10:51:29,027 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-12 11:51:29,018 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 11:51:29,032 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-12 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-12 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-12 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-12 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220180 in 900000ms
2023-03-12 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-12 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892626 in 900000ms
2023-03-12 12:51:29,018 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 12:51:29,027 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-12 12:54:26,187 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-12 12:54:27,178 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 72
2023-03-12 12:54:27,178 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-12 12:54:27,178 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-12 12:54:27,178 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 80
2023-03-12 12:54:27,178 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-12 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626946
2023-03-12 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220180
2023-03-12 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-12 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892626
2023-03-12 13:51:29,018 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 13:51:29,045 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 27 msec
2023-03-12 14:51:29,018 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 14:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-12 15:51:29,018 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 15:51:29,030 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-12 16:51:29,018 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 16:51:29,024 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-12 17:51:29,018 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 17:51:29,021 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-12 18:51:29,019 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 18:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-12 19:51:29,019 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 19:51:29,039 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-03-12 20:51:29,019 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 20:51:29,034 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-12 21:51:29,019 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 21:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-12 22:51:29,020 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 22:51:29,030 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-12 23:51:29,020 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-12 23:51:29,024 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-13 00:51:29,021 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 00:51:29,026 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-13 01:51:29,021 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 01:51:29,025 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-13 02:51:29,021 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 02:51:29,037 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-03-13 03:51:29,021 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 03:51:29,032 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-13 04:51:29,021 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 04:51:29,025 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-13 05:51:29,021 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 05:51:29,031 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-13 06:51:29,021 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 06:51:29,039 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-03-13 07:51:29,021 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 07:51:29,027 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-13 08:51:29,021 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 08:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-13 09:51:29,021 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 09:51:29,038 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-03-13 10:51:29,021 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 10:51:29,025 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-13 11:51:29,021 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 11:51:29,030 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-03-13 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-13 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-13 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892627 in 900000ms
2023-03-13 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-13 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-13 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220179 in 900000ms
2023-03-13 12:51:29,021 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 12:51:29,046 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 26 msec
2023-03-13 12:54:28,635 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-13 12:54:29,614 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 73
2023-03-13 12:54:29,614 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-13 12:54:29,614 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-13 12:54:29,614 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 81
2023-03-13 12:54:29,614 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-13 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892627
2023-03-13 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220179
2023-03-13 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626947
2023-03-13 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-13 13:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 13:51:29,032 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-13 14:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 14:51:29,027 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-13 15:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 15:51:29,031 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-03-13 16:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 16:51:29,026 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-13 17:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 17:51:29,036 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-13 18:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 18:51:29,028 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-13 19:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 19:51:29,026 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-13 20:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 20:51:29,027 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-13 21:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 21:51:29,033 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-13 22:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 22:51:29,033 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-13 23:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-13 23:51:29,036 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-14 00:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 00:51:29,025 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-14 01:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 01:51:29,027 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-14 02:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 02:51:29,027 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-14 03:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 03:51:29,025 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-14 04:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 04:51:29,026 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-14 05:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 05:51:29,025 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-14 06:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 06:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-14 07:51:29,022 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 07:51:29,027 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-14 08:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 08:51:29,028 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-14 09:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 09:51:29,034 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-14 10:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 10:51:29,041 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-03-14 11:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 11:51:29,029 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-14 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-14 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-14 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-14 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-14 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892628 in 900000ms
2023-03-14 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220178 in 900000ms
2023-03-14 12:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 12:51:29,054 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 31 msec
2023-03-14 12:54:31,013 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-14 12:54:31,960 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 74
2023-03-14 12:54:31,960 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-14 12:54:31,960 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-14 12:54:31,960 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 82
2023-03-14 12:54:31,960 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-14 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892628
2023-03-14 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220178
2023-03-14 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626948
2023-03-14 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-14 13:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 13:51:29,040 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-03-14 14:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 14:51:29,038 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-14 15:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 15:51:29,028 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-14 16:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 16:51:29,027 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-14 17:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 17:51:29,028 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-14 18:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 18:51:29,032 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-14 19:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 19:51:29,028 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-14 20:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 20:51:29,027 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-14 21:51:29,024 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 21:51:29,038 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-14 22:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 22:51:29,027 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-14 23:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-14 23:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-15 00:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 00:51:29,028 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-15 01:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 01:51:29,036 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-15 02:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 02:51:29,027 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-15 03:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 03:51:29,026 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-15 04:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 04:51:29,029 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-15 05:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 05:51:29,041 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-03-15 06:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 06:51:29,027 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-15 07:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 07:51:29,040 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-03-15 08:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 08:51:29,034 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-15 09:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 09:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-15 10:51:29,023 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 10:51:29,030 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-03-15 11:51:29,024 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 11:51:29,041 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-03-15 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-15 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-15 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-15 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-15 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220177 in 900000ms
2023-03-15 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892629 in 900000ms
2023-03-15 12:51:29,024 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 12:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 38 msec
2023-03-15 12:54:33,444 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-15 12:54:34,394 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 75
2023-03-15 12:54:34,394 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-15 12:54:34,394 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-15 12:54:34,394 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 83
2023-03-15 12:54:34,394 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-15 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892629
2023-03-15 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220177
2023-03-15 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626949
2023-03-15 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-15 13:51:29,024 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 13:51:29,033 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-15 14:51:29,024 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 14:51:29,028 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-15 15:51:29,024 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 15:51:29,028 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-15 16:51:29,024 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 16:51:29,034 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-15 17:51:29,024 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 17:51:29,029 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-15 18:51:29,024 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 18:51:29,027 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-15 19:51:29,025 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 19:51:29,033 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-03-15 20:51:29,025 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 20:51:29,030 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-15 21:51:29,025 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 21:51:29,029 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-15 22:51:29,025 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 22:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-15 23:51:29,025 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-15 23:51:29,039 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-16 00:51:29,025 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 00:51:29,031 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-03-16 01:51:29,026 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 01:51:29,028 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-16 02:51:29,025 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 02:51:29,040 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-16 03:51:29,025 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 03:51:29,029 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-16 04:51:29,026 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 04:51:29,029 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-16 05:51:29,026 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 05:51:29,037 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-16 06:51:29,026 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 06:51:29,030 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-16 07:51:29,026 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 07:51:29,030 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-16 08:51:29,026 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 08:51:29,032 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-16 09:51:29,026 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 09:51:29,031 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-16 10:51:29,026 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 10:51:29,030 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-16 11:51:29,026 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 11:51:29,039 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-16 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-16 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-16 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-16 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-16 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220176 in 900000ms
2023-03-16 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892630 in 900000ms
2023-03-16 12:51:29,026 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 12:51:29,048 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-03-16 12:54:35,803 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-16 12:54:36,753 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 76
2023-03-16 12:54:36,753 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-16 12:54:36,754 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-16 12:54:36,754 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 84
2023-03-16 12:54:36,754 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-16 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626950
2023-03-16 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-16 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220176
2023-03-16 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892630
2023-03-16 13:51:29,027 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 13:51:29,060 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 33 msec
2023-03-16 14:51:29,027 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 14:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-03-16 15:51:29,027 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 15:51:29,031 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-16 16:51:29,028 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 16:51:29,033 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-16 17:51:29,028 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 17:51:29,033 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-16 18:51:29,029 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 18:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-03-16 19:51:29,029 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 19:51:29,045 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-16 20:51:29,029 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 20:51:29,032 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-16 21:51:29,030 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 21:51:29,039 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-16 22:51:29,030 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 22:51:29,055 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-03-16 23:51:29,030 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-16 23:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-17 00:51:29,030 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 00:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-03-17 01:51:29,030 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 01:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-03-17 02:51:29,030 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 02:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-17 03:51:29,030 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 03:51:29,046 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-17 04:51:29,030 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 04:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-03-17 05:51:29,031 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 05:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-17 06:51:29,031 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 06:51:29,034 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-17 07:51:29,031 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 07:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-03-17 08:51:29,031 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 08:51:29,051 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-03-17 09:51:29,032 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 09:51:29,044 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-17 10:51:29,032 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 10:51:29,048 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-17 11:51:29,032 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 11:51:29,038 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-17 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-17 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-17 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-17 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-17 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220175 in 900000ms
2023-03-17 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892631 in 900000ms
2023-03-17 12:51:29,032 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 12:51:29,053 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-03-17 12:54:38,252 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-17 12:54:39,198 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 77
2023-03-17 12:54:39,198 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-17 12:54:39,199 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-17 12:54:39,199 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 85
2023-03-17 12:54:39,199 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-17 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626951
2023-03-17 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892631
2023-03-17 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220175
2023-03-17 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-17 13:51:29,033 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 13:51:29,044 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-17 14:51:29,033 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 14:51:29,036 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-17 15:51:29,033 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 15:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-17 16:51:29,034 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 16:51:29,046 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-17 17:51:29,034 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 17:51:29,045 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-17 18:51:29,034 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 18:51:29,046 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-17 19:51:29,034 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 19:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-17 20:51:29,034 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 20:51:29,043 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-03-17 21:51:29,034 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 21:51:29,049 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-17 22:51:29,034 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 22:51:29,051 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-03-17 23:51:29,034 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-17 23:51:29,046 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-18 00:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 00:51:29,039 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-18 01:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 01:51:29,041 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-18 02:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 02:51:29,041 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-18 03:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 03:51:29,039 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-18 04:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 04:51:29,040 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-18 05:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 05:51:29,038 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-18 06:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 06:51:29,043 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-03-18 07:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 07:51:29,056 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-03-18 08:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 08:51:29,039 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-18 09:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 09:51:29,039 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-18 10:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 10:51:29,048 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-18 11:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 11:51:29,049 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-18 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-18 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-18 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-18 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-18 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220174 in 900000ms
2023-03-18 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892632 in 900000ms
2023-03-18 12:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 12:51:29,045 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-18 12:54:40,618 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-18 12:54:41,541 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 78
2023-03-18 12:54:41,541 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-18 12:54:41,541 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-18 12:54:41,542 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 86
2023-03-18 12:54:41,542 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-18 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626952
2023-03-18 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-18 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220174
2023-03-18 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892632
2023-03-18 13:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 13:51:29,045 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-18 14:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 14:51:29,045 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-18 15:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 15:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-18 16:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 16:51:29,042 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-03-18 17:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 17:51:29,039 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-18 18:51:29,035 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 18:51:29,039 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-18 19:51:29,036 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 19:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-18 20:51:29,036 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 20:51:29,041 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-18 21:51:29,036 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 21:51:29,046 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-18 22:51:29,036 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 22:51:29,046 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-18 23:51:29,037 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-18 23:51:29,048 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-19 00:51:29,037 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 00:51:29,042 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-19 01:51:29,037 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 01:51:29,040 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-19 02:51:29,038 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 02:51:29,042 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-19 03:51:29,039 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 03:51:29,044 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-19 04:51:29,038 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 04:51:29,041 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-19 05:51:29,039 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 05:51:29,049 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-19 06:51:29,039 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 06:51:29,048 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-19 07:51:29,039 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 07:51:29,055 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-19 08:51:29,039 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 08:51:29,052 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-19 09:51:29,039 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 09:51:29,044 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-19 10:51:29,039 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 10:51:29,044 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-19 11:51:29,039 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 11:51:29,055 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-19 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-19 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-19 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-19 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-19 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892633 in 900000ms
2023-03-19 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220173 in 900000ms
2023-03-19 12:51:29,039 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 12:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-19 12:54:43,055 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-19 12:54:43,959 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 79
2023-03-19 12:54:43,960 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-19 12:54:43,960 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-19 12:54:43,960 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 87
2023-03-19 12:54:43,960 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-19 13:06:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626953
2023-03-19 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-19 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892633
2023-03-19 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220173
2023-03-19 13:51:29,039 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 13:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-19 14:51:29,040 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 14:51:29,053 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-19 15:51:29,040 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 15:51:29,057 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-03-19 16:51:29,041 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 16:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 69 msec
2023-03-19 17:51:29,041 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 17:51:29,054 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-19 18:51:29,041 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 18:51:29,052 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-19 19:51:29,041 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 19:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-19 20:51:29,042 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 20:51:29,046 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-19 21:51:29,042 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 21:51:29,058 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-03-19 22:51:29,042 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 22:51:29,067 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-03-19 23:51:29,043 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-19 23:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-20 00:51:29,043 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 00:51:29,055 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-20 01:51:29,043 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 01:51:29,058 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-20 02:51:29,043 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 02:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-20 03:51:29,043 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 03:51:29,046 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-20 04:51:29,043 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 04:51:29,046 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 2 msec
2023-03-20 05:51:29,043 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 05:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-20 06:51:29,043 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 06:51:29,046 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-20 07:51:29,043 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 07:51:29,058 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-20 08:51:29,043 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 08:51:29,056 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-20 09:51:29,043 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 09:51:29,058 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-20 10:51:29,043 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 10:51:29,049 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-03-20 11:51:29,044 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 11:51:29,053 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-20 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-20 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-20 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-20 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-20 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892634 in 900000ms
2023-03-20 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220172 in 900000ms
2023-03-20 12:51:29,044 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 12:51:29,060 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-03-20 12:54:45,425 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-20 12:54:46,305 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 80
2023-03-20 12:54:46,305 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-20 12:54:46,305 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-20 12:54:46,305 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 88
2023-03-20 12:54:46,305 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-20 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626954
2023-03-20 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-20 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220172
2023-03-20 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892634
2023-03-20 13:51:29,044 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 13:51:29,057 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-20 14:51:29,044 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 14:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-20 15:51:29,044 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 15:51:29,078 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 33 msec
2023-03-20 16:51:29,044 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 16:51:29,059 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-20 17:51:29,044 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 17:51:29,088 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 43 msec
2023-03-20 18:51:29,044 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 18:51:29,049 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-20 19:51:29,044 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 19:51:29,053 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-20 20:51:29,044 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 20:51:29,064 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-03-20 21:51:29,045 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 21:51:29,048 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-20 22:51:29,045 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 22:51:29,049 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-20 23:51:29,045 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-20 23:51:29,053 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-03-21 00:51:29,045 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 00:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-03-21 01:51:29,046 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 01:51:29,061 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-21 02:51:29,046 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 02:51:29,051 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-21 03:51:29,046 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 03:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-21 04:51:29,046 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 04:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-21 05:51:29,046 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 05:51:29,049 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-21 06:51:29,046 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 06:51:29,051 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-21 07:51:29,046 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 07:51:29,058 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-21 08:51:29,046 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 08:51:29,058 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-21 09:51:29,046 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 09:51:29,049 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 2 msec
2023-03-21 10:51:29,046 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 10:51:29,049 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-21 11:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 11:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-21 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-21 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-21 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-21 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-21 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220171 in 900000ms
2023-03-21 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892635 in 900000ms
2023-03-21 12:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 12:51:29,066 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-03-21 12:54:47,858 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-21 12:54:48,720 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 81
2023-03-21 12:54:48,720 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-21 12:54:48,720 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-21 12:54:48,720 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 89
2023-03-21 12:54:48,720 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-21 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626955
2023-03-21 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-21 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220171
2023-03-21 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892635
2023-03-21 13:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 13:51:29,081 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 33 msec
2023-03-21 14:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 14:51:29,068 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-03-21 15:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 15:51:29,060 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-21 16:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 16:51:29,061 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-21 17:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 17:51:29,064 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-03-21 18:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 18:51:29,052 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-21 19:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 19:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-21 20:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 20:51:29,060 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-21 21:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 21:51:29,052 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-21 22:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 22:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-21 23:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-21 23:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-22 00:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 00:51:29,052 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-22 01:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 01:51:29,068 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-03-22 02:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 02:51:29,051 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-22 03:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 03:51:29,069 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-03-22 04:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 04:51:29,051 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-22 05:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 05:51:29,051 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-22 06:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 06:51:29,054 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-03-22 07:51:29,047 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 07:51:29,058 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-22 08:51:29,048 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 08:51:29,067 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-03-22 09:51:29,048 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 09:51:29,061 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-22 10:51:29,048 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 10:51:29,052 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-22 11:51:29,049 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 11:51:29,064 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-22 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-22 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-22 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-22 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-22 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220170 in 900000ms
2023-03-22 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892636 in 900000ms
2023-03-22 12:51:29,049 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 12:51:29,078 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 29 msec
2023-03-22 12:54:50,222 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-22 12:54:51,051 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 82
2023-03-22 12:54:51,051 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-22 12:54:51,052 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-22 12:54:51,052 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 90
2023-03-22 12:54:51,052 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-22 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626956
2023-03-22 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-22 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892636
2023-03-22 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220170
2023-03-22 13:51:29,049 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 13:51:29,084 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 35 msec
2023-03-22 14:51:29,049 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 14:51:29,063 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-22 15:51:29,049 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 15:51:29,057 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-03-22 16:51:29,049 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 16:51:29,063 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-22 17:51:29,049 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 17:51:29,070 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-03-22 18:51:29,049 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 18:51:29,069 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-03-22 19:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 19:51:29,055 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-22 20:51:29,049 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 20:51:29,052 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-22 21:51:29,049 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 21:51:29,058 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-22 22:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 22:51:29,068 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-03-22 23:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-22 23:51:29,054 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-23 00:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 00:51:29,082 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 32 msec
2023-03-23 01:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 01:51:29,065 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-03-23 02:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 02:51:29,055 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-23 03:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 03:51:29,066 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-03-23 04:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 04:51:29,055 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-23 05:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 05:51:29,058 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-03-23 06:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 06:51:29,112 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 62 msec
2023-03-23 07:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 07:51:29,073 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-03-23 08:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 08:51:29,056 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-23 09:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 09:51:29,070 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-03-23 10:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 10:51:29,066 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-03-23 11:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 11:51:29,054 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-23 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-23 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-23 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-23 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-23 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892637 in 900000ms
2023-03-23 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220169 in 900000ms
2023-03-23 12:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 12:51:29,072 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-03-23 12:54:52,572 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-23 12:54:53,472 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 83
2023-03-23 12:54:53,472 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-23 12:54:53,472 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-23 12:54:53,472 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 91
2023-03-23 12:54:53,472 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-23 13:06:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626957
2023-03-23 13:06:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-23 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892637
2023-03-23 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220169
2023-03-23 13:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 13:51:29,059 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-23 14:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 14:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-23 15:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 15:51:29,056 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-23 16:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 16:51:29,075 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-03-23 17:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 17:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-23 18:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 18:51:29,072 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-03-23 19:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 19:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-23 20:51:29,050 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 20:51:29,068 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-03-23 21:51:29,051 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 21:51:29,073 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-03-23 22:51:29,051 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 22:51:29,066 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-23 23:51:29,051 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-23 23:51:29,061 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-24 00:51:29,051 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 00:51:29,056 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-24 01:51:29,052 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 01:51:29,061 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-24 02:51:29,052 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 02:51:29,056 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-24 03:51:29,052 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 03:51:29,056 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-24 04:51:29,052 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 04:51:29,058 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-24 05:51:29,052 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 05:51:29,100 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 48 msec
2023-03-24 06:51:29,052 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 06:51:29,057 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-24 07:51:29,053 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 07:51:29,069 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-03-24 08:51:29,054 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 08:51:29,058 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-24 09:51:29,054 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 09:51:29,068 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-24 10:51:29,054 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 10:51:29,072 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-03-24 11:51:29,054 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 11:51:29,058 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-24 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-24 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-24 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-24 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-24 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892638 in 900000ms
2023-03-24 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220168 in 900000ms
2023-03-24 12:51:29,054 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 12:51:29,076 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-03-24 12:54:54,995 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-24 12:54:55,821 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 84
2023-03-24 12:54:55,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-24 12:54:55,821 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-24 12:54:55,821 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 92
2023-03-24 12:54:55,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-24 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626958
2023-03-24 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892638
2023-03-24 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-24 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220168
2023-03-24 13:51:29,054 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 13:51:29,063 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-24 14:51:29,054 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 14:51:29,057 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-24 15:51:29,054 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 15:51:29,058 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-24 16:51:29,054 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 16:51:29,061 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-03-24 17:51:29,054 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 17:51:29,067 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-24 18:51:29,054 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 18:51:29,057 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-24 19:51:29,054 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 19:51:29,059 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-24 20:51:29,054 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 20:51:29,069 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-24 21:51:29,054 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 21:51:29,058 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-24 22:51:29,054 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 22:51:29,069 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-03-24 23:51:29,055 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-24 23:51:29,072 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-03-25 00:51:29,055 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 00:51:29,060 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-25 01:51:29,055 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 01:51:29,059 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-25 02:51:29,056 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 02:51:29,059 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-25 03:51:29,056 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 03:51:29,072 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-03-25 04:51:29,056 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 04:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-03-25 05:51:29,056 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 05:51:29,070 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-25 06:51:29,056 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 06:51:29,071 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-25 07:51:29,056 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 07:51:29,077 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-03-25 08:51:29,056 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 08:51:29,059 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-25 09:51:29,057 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 09:51:29,070 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-25 10:51:29,057 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 10:51:29,071 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-25 11:51:29,057 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 11:51:29,072 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-03-25 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-25 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-25 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-25 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-25 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892639 in 900000ms
2023-03-25 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220167 in 900000ms
2023-03-25 12:51:29,058 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 12:51:29,111 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 54 msec
2023-03-25 12:54:57,334 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-25 12:54:58,227 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 85
2023-03-25 12:54:58,228 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-25 12:54:58,228 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-25 12:54:58,228 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 93
2023-03-25 12:54:58,228 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-25 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626959
2023-03-25 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-25 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892639
2023-03-25 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220167
2023-03-25 13:51:29,058 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 13:51:29,072 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-25 14:51:29,058 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 14:51:29,078 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-03-25 15:51:29,058 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 15:51:29,061 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-25 16:51:29,059 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 16:51:29,063 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-25 17:51:29,059 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 17:51:29,063 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-25 18:51:29,059 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 18:51:29,072 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-25 19:51:29,060 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 19:51:29,065 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-25 20:51:29,060 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 20:51:29,068 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-03-25 21:51:29,059 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 21:51:29,072 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-25 22:51:29,059 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 22:51:29,074 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-25 23:51:29,059 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-25 23:51:29,068 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-26 00:51:29,059 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 00:51:29,073 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-26 01:51:29,059 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 01:51:29,071 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-26 02:51:29,059 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 02:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-26 03:51:29,059 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 03:51:29,072 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-26 04:51:29,059 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 04:51:29,063 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-26 05:51:29,060 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 05:51:29,064 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-26 06:51:29,060 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 06:51:29,065 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-26 07:51:29,060 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 07:51:29,076 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-26 08:51:29,060 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 08:51:29,065 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-26 09:51:29,061 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 09:51:29,075 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-26 10:51:29,061 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 10:51:29,072 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-26 11:51:29,061 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 11:51:29,067 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-26 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-26 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-26 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-26 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220166 in 900000ms
2023-03-26 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-26 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892640 in 900000ms
2023-03-26 12:51:29,061 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 12:51:29,096 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 35 msec
2023-03-26 12:54:59,755 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-26 12:55:00,558 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 86
2023-03-26 12:55:00,558 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-26 12:55:00,558 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-26 12:55:00,558 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 94
2023-03-26 12:55:00,558 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-26 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626960
2023-03-26 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220166
2023-03-26 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-26 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892640
2023-03-26 13:51:29,061 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 13:51:29,077 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-03-26 14:51:29,061 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 14:51:29,073 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-26 15:51:29,061 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 15:51:29,066 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-26 16:51:29,061 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 16:51:29,079 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-03-26 17:51:29,061 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 17:51:29,065 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-26 18:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 18:51:29,065 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-26 19:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 19:51:29,074 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-26 20:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 20:51:29,066 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-26 21:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 21:51:29,066 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-26 22:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 22:51:29,066 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-26 23:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-26 23:51:29,076 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-27 00:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 00:51:29,067 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-27 01:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 01:51:29,075 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-27 02:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 02:51:29,066 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-27 03:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 03:51:29,078 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-03-27 04:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 04:51:29,067 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-27 05:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 05:51:29,067 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-27 06:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 06:51:29,066 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-27 07:51:29,062 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 07:51:29,075 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-27 08:51:29,063 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 08:51:29,066 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-27 09:51:29,064 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 09:51:29,081 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-03-27 10:51:29,064 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 10:51:29,084 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-03-27 11:51:29,064 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 11:51:29,087 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-03-27 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-27 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-27 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-27 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-27 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220165 in 900000ms
2023-03-27 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892641 in 900000ms
2023-03-27 12:51:29,064 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 12:51:29,088 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-03-27 12:55:02,068 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-27 12:55:02,974 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 87
2023-03-27 12:55:02,974 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-27 12:55:02,974 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-27 12:55:02,974 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 95
2023-03-27 12:55:02,974 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-27 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626961
2023-03-27 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-27 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892641
2023-03-27 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220165
2023-03-27 13:51:29,064 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 13:51:29,088 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-03-27 14:51:29,064 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 14:51:29,092 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 28 msec
2023-03-27 15:51:29,064 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 15:51:29,069 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-27 16:51:29,064 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 16:51:29,077 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-27 17:51:29,064 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 17:51:29,067 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-27 18:51:29,065 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 18:51:29,074 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-27 19:51:29,065 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 19:51:29,071 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-27 20:51:29,065 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 20:51:29,082 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-03-27 21:51:29,065 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 21:51:29,087 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-03-27 22:51:29,065 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 22:51:29,078 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-27 23:51:29,065 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-27 23:51:29,119 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 54 msec
2023-03-28 00:51:29,065 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 00:51:29,073 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-03-28 01:51:29,065 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 01:51:29,070 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-28 02:51:29,065 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 02:51:29,069 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-28 03:51:29,065 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 03:51:29,070 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-28 04:51:29,065 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 04:51:29,073 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-03-28 05:51:29,066 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 05:51:29,076 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-28 06:51:29,066 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 06:51:29,071 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-28 07:51:29,066 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 07:51:29,070 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-28 08:51:29,066 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 08:51:29,077 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-28 09:51:29,066 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 09:51:29,071 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-28 10:51:29,066 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 10:51:29,080 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-28 11:51:29,066 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 11:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-03-28 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-28 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-28 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-28 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892642 in 900000ms
2023-03-28 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-28 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220164 in 900000ms
2023-03-28 12:51:29,066 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 12:51:29,085 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-03-28 12:55:04,500 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-28 12:55:05,389 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 88
2023-03-28 12:55:05,389 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-28 12:55:05,390 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-28 12:55:05,390 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 96
2023-03-28 12:55:05,390 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-28 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220164
2023-03-28 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892642
2023-03-28 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626962
2023-03-28 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-28 13:51:29,066 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 13:51:29,086 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-03-28 14:51:29,066 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 14:51:29,087 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-03-28 15:51:29,067 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 15:51:29,088 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-03-28 16:51:29,067 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 16:51:29,079 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-28 17:51:29,067 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 17:51:29,072 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-28 18:51:29,067 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 18:51:29,076 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-28 19:51:29,067 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 19:51:29,082 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-28 20:51:29,067 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 20:51:29,071 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-28 21:51:29,067 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 21:51:29,082 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-28 22:51:29,067 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 22:51:29,095 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 28 msec
2023-03-28 23:51:29,067 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-28 23:51:29,085 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-03-29 00:51:29,067 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 00:51:29,070 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-29 01:51:29,067 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 01:51:29,072 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-29 02:51:29,068 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 02:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-29 03:51:29,068 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 03:51:29,081 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-29 04:51:29,069 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 04:51:29,072 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-29 05:51:29,069 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 05:51:29,074 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-29 06:51:29,069 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 06:51:29,076 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-03-29 07:51:29,069 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 07:51:29,080 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-29 08:51:29,069 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 08:51:29,081 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-29 09:51:29,069 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 09:51:29,078 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-29 10:51:29,070 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 10:51:29,078 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-03-29 11:51:29,070 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 11:51:29,074 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-29 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-29 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-29 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-29 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-29 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220163 in 900000ms
2023-03-29 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892643 in 900000ms
2023-03-29 12:51:29,070 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 12:51:29,079 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-29 12:55:06,849 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-29 12:55:07,742 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 89
2023-03-29 12:55:07,742 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-29 12:55:07,742 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-29 12:55:07,742 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 97
2023-03-29 12:55:07,742 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-29 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626963
2023-03-29 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-29 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220163
2023-03-29 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892643
2023-03-29 13:51:29,070 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 13:51:29,105 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 35 msec
2023-03-29 14:51:29,071 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 14:51:29,076 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-29 15:51:29,071 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 15:51:29,088 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-03-29 16:51:29,071 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 16:51:29,076 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-29 17:51:29,071 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 17:51:29,093 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-03-29 18:51:29,071 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 18:51:29,074 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-29 19:51:29,072 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 19:51:29,086 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-03-29 20:51:29,072 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 20:51:29,080 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-03-29 21:51:29,072 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 21:51:29,076 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-29 22:51:29,072 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 22:51:29,076 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-29 23:51:29,072 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-29 23:51:29,085 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-30 00:51:29,072 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 00:51:29,076 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-30 01:51:29,072 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 01:51:29,079 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-03-30 02:51:29,072 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 02:51:29,078 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-03-30 03:51:29,073 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 03:51:29,076 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-30 04:51:29,073 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 04:51:29,093 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-03-30 05:51:29,073 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 05:51:29,078 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-30 06:51:29,073 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 06:51:29,094 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-03-30 07:51:29,074 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 07:51:29,077 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-30 08:51:29,074 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 08:51:29,078 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-30 09:51:29,074 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 09:51:29,078 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-30 10:51:29,075 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 10:51:29,079 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-30 11:51:29,075 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 11:51:29,078 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-30 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-30 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-30 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-30 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-30 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220162 in 900000ms
2023-03-30 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892644 in 900000ms
2023-03-30 12:51:29,076 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 12:51:29,086 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-30 12:55:09,281 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-30 12:55:10,153 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 90
2023-03-30 12:55:10,153 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-30 12:55:10,153 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-30 12:55:10,153 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 98
2023-03-30 12:55:10,153 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-30 13:06:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626964
2023-03-30 13:06:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-30 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220162
2023-03-30 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892644
2023-03-30 13:51:29,076 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 13:51:29,156 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 80 msec
2023-03-30 14:51:29,076 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 14:51:29,087 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-03-30 15:51:29,076 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 15:51:29,080 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-30 16:51:29,077 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 16:51:29,081 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-30 17:51:29,078 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 17:51:29,087 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-30 18:51:29,078 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 18:51:29,096 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-03-30 19:51:29,078 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 19:51:29,082 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-30 20:51:29,078 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 20:51:29,087 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-30 21:51:29,079 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 21:51:29,085 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-30 22:51:29,079 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 22:51:29,084 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-30 23:51:29,079 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-30 23:51:29,109 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 30 msec
2023-03-31 00:51:29,079 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 00:51:29,085 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-31 01:51:29,079 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 01:51:29,084 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-31 02:51:29,079 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 02:51:29,094 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-03-31 03:51:29,079 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 03:51:29,086 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-03-31 04:51:29,079 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 04:51:29,097 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-03-31 05:51:29,080 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 05:51:29,092 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-03-31 06:51:29,080 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 06:51:29,096 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-03-31 07:51:29,080 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 07:51:29,085 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-31 08:51:29,080 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 08:51:29,085 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-03-31 09:51:29,081 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 09:51:29,088 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-03-31 10:51:29,081 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 10:51:29,086 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-31 11:51:29,081 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 11:51:29,090 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-31 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-03-31 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-31 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-03-31 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-03-31 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892645 in 900000ms
2023-03-31 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220161 in 900000ms
2023-03-31 12:51:29,081 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 12:51:29,090 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-03-31 12:55:11,621 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-31 12:55:12,477 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 91
2023-03-31 12:55:12,477 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-03-31 12:55:12,477 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-03-31 12:55:12,477 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 99
2023-03-31 12:55:12,477 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-03-31 13:06:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626965
2023-03-31 13:06:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-03-31 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220161
2023-03-31 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892645
2023-03-31 13:51:29,081 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 13:51:29,106 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-03-31 14:51:29,081 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 14:51:29,086 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-31 15:51:29,081 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 15:51:29,092 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-03-31 16:51:29,081 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 16:51:29,086 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-31 17:51:29,081 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 17:51:29,085 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-03-31 18:51:29,082 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 18:51:29,085 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-31 19:51:29,082 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 19:51:29,095 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-03-31 20:51:29,082 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 20:51:29,100 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-03-31 21:51:29,082 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 21:51:29,086 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-03-31 22:51:29,082 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 22:51:29,086 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-03-31 23:51:29,082 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-03-31 23:51:29,086 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-01 00:51:29,082 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 00:51:29,087 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-01 01:51:29,082 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 01:51:29,087 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-01 02:51:29,082 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 02:51:29,087 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-01 03:51:29,082 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 03:51:29,095 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-01 04:51:29,082 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 04:51:29,089 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-01 05:51:29,082 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 05:51:29,101 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-04-01 06:51:29,082 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 06:51:29,098 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-01 07:51:29,082 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 07:51:29,103 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-04-01 08:51:29,082 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 08:51:29,088 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-01 09:51:29,082 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 09:51:29,095 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-04-01 10:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 10:51:29,087 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-01 11:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 11:51:29,106 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-04-01 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-04-01 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-01 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-01 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-01 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220160 in 900000ms
2023-04-01 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892646 in 900000ms
2023-04-01 12:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 12:51:29,093 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-01 12:55:14,052 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-01 12:55:14,883 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 92
2023-04-01 12:55:14,883 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-01 12:55:14,883 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-01 12:55:14,883 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 100
2023-04-01 12:55:14,883 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-01 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626966
2023-04-01 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-01 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220160
2023-04-01 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892646
2023-04-01 13:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 13:51:29,107 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-04-01 14:51:29,084 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 14:51:29,100 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-01 15:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 15:51:29,116 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 33 msec
2023-04-01 16:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 16:51:29,088 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-01 17:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 17:51:29,096 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-01 18:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 18:51:29,089 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-01 19:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 19:51:29,099 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-01 20:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 20:51:29,088 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-01 21:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 21:51:29,094 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-01 22:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 22:51:29,098 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-01 23:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-01 23:51:29,092 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-02 00:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 00:51:29,088 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-02 01:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 01:51:29,089 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-02 02:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 02:51:29,087 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-02 03:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 03:51:29,088 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-02 04:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 04:51:29,093 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-02 05:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 05:51:29,093 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-02 06:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 06:51:29,088 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-02 07:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 07:51:29,093 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-02 08:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 08:51:29,103 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-04-02 09:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 09:51:29,096 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-02 10:51:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 10:51:29,093 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-02 11:51:29,084 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 11:51:29,097 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-04-02 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-04-02 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-02 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-02 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-02 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220159 in 900000ms
2023-04-02 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892647 in 900000ms
2023-04-02 12:51:29,085 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 12:51:29,099 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-02 12:55:16,389 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-02 12:55:17,270 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 93
2023-04-02 12:55:17,270 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-02 12:55:17,271 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-02 12:55:17,271 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 101
2023-04-02 12:55:17,271 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-02 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626967
2023-04-02 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-02 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892647
2023-04-02 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220159
2023-04-02 13:51:29,086 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 13:51:29,105 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-04-02 14:51:29,086 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 14:51:29,089 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-04-02 15:51:29,086 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 15:51:29,117 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 32 msec
2023-04-02 16:51:29,087 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 16:51:29,091 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-02 17:51:29,087 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 17:51:29,095 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-02 18:51:29,087 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 18:51:29,092 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-02 19:51:29,087 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 19:51:29,121 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 34 msec
2023-04-02 20:51:29,087 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 20:51:29,102 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-02 21:51:29,087 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 21:51:29,091 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-04-02 22:51:29,087 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 22:51:29,092 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-02 23:51:29,087 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-02 23:51:29,093 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-03 00:51:29,087 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 00:51:29,106 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-04-03 01:51:29,088 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 01:51:29,092 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-03 02:51:29,088 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 02:51:29,104 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-03 03:51:29,088 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 03:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-04-03 04:51:29,088 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 04:51:29,093 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-03 05:51:29,089 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 05:51:29,096 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-03 06:51:29,088 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 06:51:29,103 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-03 07:51:29,089 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 07:51:29,097 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-03 08:51:29,089 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 08:51:29,095 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-03 09:51:29,089 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 09:51:29,098 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-03 10:51:29,089 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 10:51:29,101 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-03 11:51:29,089 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 11:51:29,100 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-03 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-04-03 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-03 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-03 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-03 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220158 in 900000ms
2023-04-03 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892648 in 900000ms
2023-04-03 12:51:29,089 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 12:51:29,099 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-03 12:55:18,799 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-03 12:55:19,680 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 94
2023-04-03 12:55:19,680 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-03 12:55:19,680 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-03 12:55:19,680 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 102
2023-04-03 12:55:19,680 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-03 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626968
2023-04-03 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-03 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220158
2023-04-03 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892648
2023-04-03 13:51:29,089 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 13:51:29,112 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-04-03 14:51:29,089 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 14:51:29,093 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-03 15:51:29,089 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 15:51:29,106 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-04-03 16:51:29,089 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 16:51:29,106 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-04-03 17:51:29,089 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 17:51:29,102 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-03 18:51:29,089 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 18:51:29,102 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-04-03 19:51:29,090 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 19:51:29,098 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-03 20:51:29,091 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 20:51:29,103 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-03 21:51:29,091 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 21:51:29,097 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-03 22:51:29,091 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 22:51:29,101 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-03 23:51:29,092 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-03 23:51:29,097 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-04 00:51:29,092 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 00:51:29,097 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-04 01:51:29,093 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 01:51:29,100 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-04 02:51:29,093 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 02:51:29,098 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-04 03:51:29,093 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 03:51:29,101 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-04 04:51:29,093 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 04:51:29,101 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-04 05:51:29,094 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 05:51:29,099 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-04 06:51:29,094 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 06:51:29,107 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-04 07:51:29,095 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 07:51:29,111 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-04 08:51:29,095 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 08:51:29,099 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-04 09:51:29,096 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 09:51:29,100 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-04 10:51:29,096 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 10:51:29,101 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-04 11:51:29,097 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 11:51:29,117 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-04-04 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-04-04 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-04 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-04 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892649 in 900000ms
2023-04-04 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-04 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220157 in 900000ms
2023-04-04 12:51:29,097 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 12:51:29,134 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 37 msec
2023-04-04 12:55:21,099 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-04 12:55:22,017 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 95
2023-04-04 12:55:22,017 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-04 12:55:22,017 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-04 12:55:22,017 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 103
2023-04-04 12:55:22,018 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-04 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626969
2023-04-04 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-04 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892649
2023-04-04 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220157
2023-04-04 13:51:29,097 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 13:51:29,141 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 45 msec
2023-04-04 14:51:29,098 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 14:51:29,117 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-04-04 15:51:29,098 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 15:51:29,103 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-04 16:51:29,098 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 16:51:29,114 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-04-04 17:51:29,099 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 17:51:29,109 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-04 18:51:29,099 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 18:51:29,105 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-04 19:51:29,099 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 19:51:29,104 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-04 20:51:29,099 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 20:51:29,122 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-04-04 21:51:29,099 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 21:51:29,109 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-04 22:51:29,100 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 22:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-04 23:51:29,101 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-04 23:51:29,106 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-05 00:51:29,102 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 00:51:29,106 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-05 01:51:29,102 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 01:51:29,117 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-04-05 02:51:29,102 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 02:51:29,118 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-04-05 03:51:29,103 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 03:51:29,106 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-05 04:51:29,104 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 04:51:29,115 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-05 05:51:29,104 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 05:51:29,107 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-05 06:51:29,104 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 06:51:29,118 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-05 07:51:29,104 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 07:51:29,116 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-05 08:51:29,104 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 08:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-05 09:51:29,104 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 09:51:29,109 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-05 10:51:29,105 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 10:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-05 11:51:29,105 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 11:51:29,109 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-05 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-04-05 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-05 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-05 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892650 in 900000ms
2023-04-05 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-05 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220156 in 900000ms
2023-04-05 12:51:29,105 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 12:51:29,141 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 35 msec
2023-04-05 12:55:23,532 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-05 12:55:24,428 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 96
2023-04-05 12:55:24,428 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-05 12:55:24,428 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-05 12:55:24,428 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 104
2023-04-05 12:55:24,428 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-05 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626970
2023-04-05 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-05 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892650
2023-04-05 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220156
2023-04-05 13:51:29,105 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 13:51:29,119 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-04-05 14:51:29,105 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 14:51:29,119 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-05 15:51:29,105 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 15:51:29,108 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-04-05 16:51:29,105 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 16:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-05 17:51:29,105 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 17:51:29,118 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-05 18:51:29,105 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 18:51:29,126 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-04-05 19:51:29,105 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 19:51:29,119 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-05 20:51:29,105 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 20:51:29,115 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-05 21:51:29,105 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 21:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-05 22:51:29,105 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 22:51:29,109 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-05 23:51:29,106 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-05 23:51:29,111 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-06 00:51:29,106 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 00:51:29,120 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-04-06 01:51:29,106 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 01:51:29,122 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-04-06 02:51:29,106 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 02:51:29,117 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-06 03:51:29,106 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 03:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-06 04:51:29,107 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 04:51:29,111 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-06 05:51:29,107 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 05:51:29,115 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-06 06:51:29,107 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 06:51:29,120 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-06 07:51:29,107 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 07:51:29,113 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-06 08:51:29,108 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 08:51:29,112 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-06 09:51:29,108 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 09:51:29,111 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-06 10:51:29,108 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 10:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-06 11:51:29,108 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 11:51:29,127 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-04-06 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-04-06 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-06 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-06 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892651 in 900000ms
2023-04-06 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-06 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220155 in 900000ms
2023-04-06 12:51:29,108 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 12:51:29,123 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-06 12:55:25,872 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-06 12:55:26,767 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 97
2023-04-06 12:55:26,767 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-06 12:55:26,767 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-06 12:55:26,767 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 105
2023-04-06 12:55:26,767 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-06 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892651
2023-04-06 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626971
2023-04-06 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220155
2023-04-06 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-06 13:51:29,108 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 13:51:29,131 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-04-06 14:51:29,108 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 14:51:29,133 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-04-06 15:51:29,108 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 15:51:29,124 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-04-06 16:51:29,109 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 16:51:29,113 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-06 17:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 17:51:29,114 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-06 18:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 18:51:29,123 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-06 19:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 19:51:29,118 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-06 20:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 20:51:29,116 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-06 21:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 21:51:29,115 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-06 22:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 22:51:29,114 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-06 23:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-06 23:51:29,116 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-07 00:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 00:51:29,114 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-04-07 01:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 01:51:29,133 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-04-07 02:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 02:51:29,114 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-04-07 03:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 03:51:29,124 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-07 04:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 04:51:29,128 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-04-07 05:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 05:51:29,117 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-07 06:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 06:51:29,121 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-07 07:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 07:51:29,158 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 48 msec
2023-04-07 08:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 08:51:29,115 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-07 09:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 09:51:29,135 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-04-07 10:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 10:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-07 11:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 11:51:29,115 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-07 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-04-07 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-07 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-07 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892652 in 900000ms
2023-04-07 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-07 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220154 in 900000ms
2023-04-07 12:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 12:51:29,120 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-07 12:55:28,273 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-07 12:55:29,192 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 98
2023-04-07 12:55:29,192 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-07 12:55:29,193 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-07 12:55:29,193 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 106
2023-04-07 12:55:29,193 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-07 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626972
2023-04-07 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-07 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220154
2023-04-07 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892652
2023-04-07 13:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 13:51:29,147 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 37 msec
2023-04-07 14:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 14:51:29,137 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 26 msec
2023-04-07 15:51:29,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 15:51:29,113 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-07 16:51:29,111 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 16:51:29,133 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-04-07 17:51:29,111 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 17:51:29,140 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 29 msec
2023-04-07 18:51:29,111 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 18:51:29,119 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-07 19:51:29,111 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 19:51:29,140 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 28 msec
2023-04-07 20:51:29,111 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 20:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-04-07 21:51:29,111 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 21:51:29,133 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-04-07 22:51:29,111 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 22:51:29,115 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-07 23:51:29,111 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-07 23:51:29,115 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-04-08 00:51:29,111 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 00:51:29,115 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-08 01:51:29,111 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 01:51:29,129 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-04-08 02:51:29,111 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 02:51:29,117 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-08 03:51:29,111 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 03:51:29,122 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-08 04:51:29,112 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 04:51:29,122 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-08 05:51:29,112 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 05:51:29,129 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-04-08 06:51:29,112 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 06:51:29,136 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-04-08 07:51:29,112 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 07:51:29,120 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-08 08:51:29,112 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 08:51:29,118 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-08 09:51:29,113 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 09:51:29,135 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-04-08 10:51:29,113 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 10:51:29,122 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-08 11:51:29,113 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 11:51:29,133 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-04-08 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-04-08 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-08 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-08 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220153 in 900000ms
2023-04-08 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-08 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892653 in 900000ms
2023-04-08 12:51:29,114 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 12:51:29,138 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-04-08 12:55:30,611 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-08 12:55:31,487 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 99
2023-04-08 12:55:31,487 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-08 12:55:31,487 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-08 12:55:31,487 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 107
2023-04-08 12:55:31,487 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-08 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626973
2023-04-08 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-08 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220153
2023-04-08 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892653
2023-04-08 13:51:29,115 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 13:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-08 14:51:29,115 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 14:51:29,132 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-04-08 15:51:29,115 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 15:51:29,130 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-08 16:51:29,116 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 16:51:29,120 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-08 17:51:29,116 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 17:51:29,121 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-08 18:51:29,116 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 18:51:29,135 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-04-08 19:51:29,116 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 19:51:29,121 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-08 20:51:29,117 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 20:51:29,121 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-08 21:51:29,117 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 21:51:29,132 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-08 22:51:29,117 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 22:51:29,122 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-08 23:51:29,117 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-08 23:51:29,121 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-09 00:51:29,118 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 00:51:29,145 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 27 msec
2023-04-09 01:51:29,118 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 01:51:29,122 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-09 02:51:29,118 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 02:51:29,133 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-09 03:51:29,118 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 03:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-09 04:51:29,118 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 04:51:29,133 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-09 05:51:29,119 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 05:51:29,126 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-09 06:51:29,120 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 06:51:29,124 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-09 07:51:29,120 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 07:51:29,124 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-09 08:51:29,120 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 08:51:29,129 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-09 09:51:29,120 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 09:51:29,132 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-09 10:51:29,120 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 10:51:29,131 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-09 11:51:29,121 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 11:51:29,126 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-09 12:51:28,950 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-04-09 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-09 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-09 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220152 in 900000ms
2023-04-09 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-09 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892654 in 900000ms
2023-04-09 12:51:29,121 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 12:51:29,134 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-09 12:55:33,037 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-09 12:55:33,896 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 100
2023-04-09 12:55:33,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-09 12:55:33,896 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-09 12:55:33,896 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 108
2023-04-09 12:55:33,896 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-09 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220152
2023-04-09 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626974
2023-04-09 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-09 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892654
2023-04-09 13:51:29,121 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 13:51:29,132 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-09 14:51:29,121 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 14:51:29,124 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-04-09 15:51:29,121 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 15:51:29,145 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-04-09 16:51:29,122 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 16:51:29,136 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-04-09 17:51:29,122 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 17:51:29,126 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-09 18:51:29,122 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 18:51:29,133 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-09 19:51:29,122 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 19:51:29,131 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-09 20:51:29,122 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 20:51:29,126 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-09 21:51:29,122 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 21:51:29,126 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-04-09 22:51:29,122 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 22:51:29,202 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 80 msec
2023-04-09 23:51:29,123 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-09 23:51:29,127 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-10 00:51:29,123 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 00:51:29,136 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-10 01:51:29,123 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 01:51:29,135 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-10 02:51:29,124 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 02:51:29,128 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-10 03:51:29,124 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 03:51:29,133 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-10 04:51:29,124 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 04:51:29,146 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-04-10 05:51:29,124 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 05:51:29,129 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-10 06:51:29,124 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 06:51:29,133 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-10 07:51:29,124 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 07:51:29,138 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-04-10 08:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 08:51:29,136 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-10 09:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 09:51:29,149 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-04-10 10:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 10:51:29,135 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-10 11:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 11:51:29,137 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-10 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-04-10 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-10 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-10 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220151 in 900000ms
2023-04-10 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-10 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892655 in 900000ms
2023-04-10 12:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 12:51:29,136 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-10 12:55:35,411 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-10 12:55:36,291 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 101
2023-04-10 12:55:36,291 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-10 12:55:36,291 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-10 12:55:36,292 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 109
2023-04-10 12:55:36,292 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-10 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626975
2023-04-10 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-10 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220151
2023-04-10 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892655
2023-04-10 13:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 13:51:29,142 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-04-10 14:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 14:51:29,143 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-04-10 15:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 15:51:29,138 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-10 16:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 16:51:29,134 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-10 17:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 17:51:29,136 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-10 18:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 18:51:29,128 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-04-10 19:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 19:51:29,135 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-10 20:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 20:51:29,138 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-10 21:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 21:51:29,138 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-10 22:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 22:51:29,133 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-10 23:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-10 23:51:29,189 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 64 msec
2023-04-11 00:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 00:51:29,130 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-11 01:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 01:51:29,138 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-11 02:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 02:51:29,140 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-04-11 03:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 03:51:29,130 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-11 04:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 04:51:29,129 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-11 05:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 05:51:29,130 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-11 06:51:29,125 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 06:51:29,133 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-11 07:51:29,126 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 07:51:29,141 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-11 08:51:29,126 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 08:51:29,130 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-11 09:51:29,127 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 09:51:29,140 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-11 10:51:29,127 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 10:51:29,132 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-11 11:32:35,108 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 10
2023-04-11 11:51:29,127 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 11:51:29,137 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-11 12:16:55,461 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 11
2023-04-11 12:16:56,142 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1672231888656_0011' is submitted without priority hence considering default queue/cluster priority: 0
2023-04-11 12:16:56,142 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1672231888656_0011
2023-04-11 12:16:56,142 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 11 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 12:16:56,143 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 11 submitted by user spark
2023-04-11 12:16:56,143 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=spark	IP=10.130.0.4	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1672231888656_0011	QUEUENAME=default
2023-04-11 12:16:56,144 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1672231888656_0011
2023-04-11 12:16:56,144 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0011 State change from NEW to NEW_SAVING on event = START
2023-04-11 12:16:56,144 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1672231888656_0011
2023-04-11 12:16:56,146 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0011 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2023-04-11 12:16:56,146 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1672231888656_0011 user: spark leaf-queue of parent: root #applications: 1
2023-04-11 12:16:56,146 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1672231888656_0011 from user: spark, in queue: default
2023-04-11 12:16:56,147 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0011 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2023-04-11 12:16:56,147 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1672231888656_0011_000001
2023-04-11 12:16:56,147 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0011_000001 State change from NEW to SUBMITTED on event = START
2023-04-11 12:16:56,148 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1672231888656_0011 from user: spark activated in queue: default
2023-04-11 12:16:56,148 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1672231888656_0011 user: spark, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-11 12:16:56,148 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1672231888656_0011_000001 to scheduler from user spark in queue default
2023-04-11 12:16:56,149 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0011_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-11 12:16:56,429 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0011_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-11 12:16:56,430 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0011_01_000001 Container Transitioned from NEW to ALLOCATED
2023-04-11 12:16:56,430 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0011_01_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-04-11 12:16:56,430 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=spark	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0011	CONTAINERID=container_e02_1672231888656_0011_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-11 12:16:56,432 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e02_1672231888656_0011_01_000001
2023-04-11 12:16:56,433 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0011_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-11 12:16:56,433 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1672231888656_0011_000001
2023-04-11 12:16:56,433 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1672231888656_0011 AttemptId: appattempt_1672231888656_0011_000001 MasterContainer: Container: [ContainerId: container_e02_1672231888656_0011_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ]
2023-04-11 12:16:56,433 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.04836805 absoluteUsedCapacity=0.04836805 used=<memory:1024, vCores:1> cluster=<memory:21171, vCores:6>
2023-04-11 12:16:56,433 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-11 12:16:56,434 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0011_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-11 12:16:56,434 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0011_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-11 12:16:56,435 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1672231888656_0011_000001
2023-04-11 12:16:56,436 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e02_1672231888656_0011_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1672231888656_0011_000001
2023-04-11 12:16:56,436 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1672231888656_0011_000001
2023-04-11 12:16:56,437 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1672231888656_0011_000001
2023-04-11 12:16:56,459 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e02_1672231888656_0011_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1672231888656_0011_000001
2023-04-11 12:16:56,459 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0011_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-11 12:16:57,431 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0011_01_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-11 12:17:03,596 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1672231888656_0011_000001 (auth:SIMPLE)
2023-04-11 12:17:03,599 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1672231888656_0011_000001
2023-04-11 12:17:03,600 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=spark	IP=10.130.0.5	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1672231888656_0011	APPATTEMPTID=appattempt_1672231888656_0011_000001
2023-04-11 12:17:03,600 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0011_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-11 12:17:03,600 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0011 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-11 12:17:04,074 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0011_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-11 12:17:04,074 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0011_01_000002 Container Transitioned from NEW to ALLOCATED
2023-04-11 12:17:04,074 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0011_01_000002 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2023-04-11 12:17:04,074 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=spark	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0011	CONTAINERID=container_e02_1672231888656_0011_01_000002	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-11 12:17:04,075 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.14510415 absoluteUsedCapacity=0.14510415 used=<memory:3072, vCores:2> cluster=<memory:21171, vCores:6>
2023-04-11 12:17:04,075 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-11 12:17:04,436 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_e02_1672231888656_0011_01_000002
2023-04-11 12:17:04,437 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0011_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-11 12:17:04,500 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0011_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-11 12:17:04,501 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0011_01_000003 Container Transitioned from NEW to ALLOCATED
2023-04-11 12:17:04,501 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0011_01_000003 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2023-04-11 12:17:04,501 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=spark	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0011	CONTAINERID=container_e02_1672231888656_0011_01_000003	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-11 12:17:04,502 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.24184026 absoluteUsedCapacity=0.24184026 used=<memory:5120, vCores:3> cluster=<memory:21171, vCores:6>
2023-04-11 12:17:04,502 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-11 12:17:05,076 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0011_01_000002 Container Transitioned from ACQUIRED to RUNNING
2023-04-11 12:17:05,271 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode1.ru-central1.internal:8041 for container : container_e02_1672231888656_0011_01_000003
2023-04-11 12:17:05,272 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0011_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-11 12:17:05,503 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0011_01_000003 Container Transitioned from ACQUIRED to RUNNING
2023-04-11 12:17:05,504 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0011_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-11 12:17:05,505 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0011_01_000004 Container Transitioned from NEW to ALLOCATED
2023-04-11 12:17:05,505 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0011_01_000004 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 2 containers, <memory:4096, vCores:2> used and <memory:2961, vCores:0> available after allocation
2023-04-11 12:17:05,505 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=spark	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0011	CONTAINERID=container_e02_1672231888656_0011_01_000004	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-11 12:17:05,505 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.33857635 absoluteUsedCapacity=0.33857635 used=<memory:7168, vCores:4> cluster=<memory:21171, vCores:6>
2023-04-11 12:17:05,505 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-11 12:17:08,280 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1672231888656_0011
2023-04-11 12:17:08,281 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0011_01_000004 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-11 12:17:11,288 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0011_01_000004 Container Transitioned from ACQUIRED to RELEASED
2023-04-11 12:17:11,288 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=spark	IP=10.130.0.5	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0011	CONTAINERID=container_e02_1672231888656_0011_01_000004	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-11 12:17:17,444 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0011_01_000002 Container Transitioned from RUNNING to COMPLETED
2023-04-11 12:17:17,444 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=spark	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0011	CONTAINERID=container_e02_1672231888656_0011_01_000002	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-11 12:17:17,446 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0011_01_000003 Container Transitioned from RUNNING to COMPLETED
2023-04-11 12:17:17,446 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=spark	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0011	CONTAINERID=container_e02_1672231888656_0011_01_000003	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-11 12:17:17,454 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1672231888656_0011_000001 with final state: FINISHING, and exit status: -1000
2023-04-11 12:17:17,454 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0011_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2023-04-11 12:17:17,454 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1672231888656_0011 with final state: FINISHING
2023-04-11 12:17:17,455 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0011 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2023-04-11 12:17:17,455 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1672231888656_0011
2023-04-11 12:17:17,455 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0011_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2023-04-11 12:17:17,455 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0011 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2023-04-11 12:17:17,557 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1672231888656_0011 unregistered successfully. 
2023-04-11 12:17:17,930 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1672231888656_0011_000001
2023-04-11 12:17:17,930 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0011_01_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-11 12:17:17,930 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1672231888656_0011_000001
2023-04-11 12:17:17,930 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=spark	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0011	CONTAINERID=container_e02_1672231888656_0011_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-11 12:17:17,930 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0011_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2023-04-11 12:17:17,931 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0011 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2023-04-11 12:17:17,931 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=spark	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0011
2023-04-11 12:17:17,931 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1672231888656_0011_000001
2023-04-11 12:17:17,932 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1672231888656_0011_000001 is done. finalState=FINISHED
2023-04-11 12:17:17,932 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0011,name=Spark shell,user=spark,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0011/,appMasterHost=10.130.0.5,submitTime=1681215416141,startTime=1681215416142,finishTime=1681215437454,finalStatus=SUCCEEDED,memorySeconds=87754,vcoreSeconds=51,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=SPARK,resourceSeconds=87754 MB-seconds\, 51 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2023-04-11 12:17:17,932 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1672231888656_0011 requests cleared
2023-04-11 12:17:17,932 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1672231888656_0011 user: spark queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-04-11 12:17:17,932 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1672231888656_0011 user: spark leaf-queue of parent: root #applications: 0
2023-04-11 12:35:04,913 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 12
2023-04-11 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-04-11 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-11 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-11 12:51:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id 1855892656 in 900000ms
2023-04-11 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-11 12:51:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id -769220150 in 900000ms
2023-04-11 12:51:29,127 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 12:51:29,161 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 34 msec
2023-04-11 12:55:37,897 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-11 12:55:38,719 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 102
2023-04-11 12:55:38,720 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-11 12:55:38,720 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-11 12:55:38,720 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 110
2023-04-11 12:55:38,720 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-11 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: 1926626976
2023-04-11 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: 1855892656
2023-04-11 13:06:28,951 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-11 13:06:28,952 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: -769220150
2023-04-11 13:51:29,127 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 13:51:29,136 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-11 14:51:29,128 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 14:51:29,144 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-04-11 15:02:15,239 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 13
2023-04-11 15:02:15,893 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1672231888656_0013' is submitted without priority hence considering default queue/cluster priority: 0
2023-04-11 15:02:15,893 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1672231888656_0013
2023-04-11 15:02:15,893 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 13 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:02:15,893 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 13 submitted by user root
2023-04-11 15:02:15,894 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=root	IP=10.130.0.4	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1672231888656_0013	QUEUENAME=default
2023-04-11 15:02:15,894 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1672231888656_0013
2023-04-11 15:02:15,895 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1672231888656_0013
2023-04-11 15:02:15,895 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0013 State change from NEW to NEW_SAVING on event = START
2023-04-11 15:02:15,895 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0013 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2023-04-11 15:02:15,896 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1672231888656_0013 user: root leaf-queue of parent: root #applications: 1
2023-04-11 15:02:15,896 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1672231888656_0013 from user: root, in queue: default
2023-04-11 15:02:15,896 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0013 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2023-04-11 15:02:15,896 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1672231888656_0013_000001
2023-04-11 15:02:15,896 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0013_000001 State change from NEW to SUBMITTED on event = START
2023-04-11 15:02:15,896 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1672231888656_0013 from user: root activated in queue: default
2023-04-11 15:02:15,896 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1672231888656_0013 user: root, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-11 15:02:15,896 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1672231888656_0013_000001 to scheduler from user root in queue default
2023-04-11 15:02:15,898 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0013_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-11 15:02:16,057 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0013_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-11 15:02:16,057 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0013_01_000001 Container Transitioned from NEW to ALLOCATED
2023-04-11 15:02:16,058 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0013_01_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-04-11 15:02:16,058 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0013	CONTAINERID=container_e02_1672231888656_0013_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-11 15:02:16,059 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_e02_1672231888656_0013_01_000001
2023-04-11 15:02:16,060 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0013_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-11 15:02:16,060 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1672231888656_0013_000001
2023-04-11 15:02:16,060 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1672231888656_0013 AttemptId: appattempt_1672231888656_0013_000001 MasterContainer: Container: [ContainerId: container_e02_1672231888656_0013_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ]
2023-04-11 15:02:16,060 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.04836805 absoluteUsedCapacity=0.04836805 used=<memory:1024, vCores:1> cluster=<memory:21171, vCores:6>
2023-04-11 15:02:16,060 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-11 15:02:16,060 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0013_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-11 15:02:16,063 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0013_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-11 15:02:16,063 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1672231888656_0013_000001
2023-04-11 15:02:16,065 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e02_1672231888656_0013_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1672231888656_0013_000001
2023-04-11 15:02:16,065 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1672231888656_0013_000001
2023-04-11 15:02:16,066 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1672231888656_0013_000001
2023-04-11 15:02:16,084 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e02_1672231888656_0013_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1672231888656_0013_000001
2023-04-11 15:02:16,084 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0013_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-11 15:02:17,058 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0013_01_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-11 15:02:19,230 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1672231888656_0013_000001 (auth:SIMPLE)
2023-04-11 15:02:19,234 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1672231888656_0013_000001
2023-04-11 15:02:19,234 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=root	IP=10.130.0.6	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1672231888656_0013	APPATTEMPTID=appattempt_1672231888656_0013_000001
2023-04-11 15:02:19,234 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0013_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-11 15:02:19,234 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0013 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-11 15:02:19,676 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0013_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-11 15:02:19,677 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0013_01_000002 Container Transitioned from NEW to ALLOCATED
2023-04-11 15:02:19,677 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0013_01_000002 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2023-04-11 15:02:19,677 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0013	CONTAINERID=container_e02_1672231888656_0013_01_000002	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-11 15:02:19,677 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.14510415 absoluteUsedCapacity=0.14510415 used=<memory:3072, vCores:2> cluster=<memory:21171, vCores:6>
2023-04-11 15:02:19,677 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-11 15:02:20,047 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e02_1672231888656_0013_01_000002
2023-04-11 15:02:20,048 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0013_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-11 15:02:20,108 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1672231888656_0013_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-11 15:02:20,108 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0013_01_000003 Container Transitioned from NEW to ALLOCATED
2023-04-11 15:02:20,109 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e02_1672231888656_0013_01_000003 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 2 containers, <memory:3072, vCores:2> used and <memory:3985, vCores:0> available after allocation
2023-04-11 15:02:20,109 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0013	CONTAINERID=container_e02_1672231888656_0013_01_000003	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-11 15:02:20,109 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.24184026 absoluteUsedCapacity=0.24184026 used=<memory:5120, vCores:3> cluster=<memory:21171, vCores:6>
2023-04-11 15:02:20,109 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-11 15:02:20,276 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1672231888656_0013
2023-04-11 15:02:20,277 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_e02_1672231888656_0013_01_000003
2023-04-11 15:02:20,278 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0013_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-11 15:02:20,303 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1672231888656_0013_000001 with final state: FINISHING, and exit status: -1000
2023-04-11 15:02:20,303 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0013_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2023-04-11 15:02:20,303 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1672231888656_0013 with final state: FINISHING
2023-04-11 15:02:20,303 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0013 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2023-04-11 15:02:20,304 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1672231888656_0013
2023-04-11 15:02:20,304 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0013_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2023-04-11 15:02:20,304 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0013 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2023-04-11 15:02:20,420 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1672231888656_0013 unregistered successfully. 
2023-04-11 15:02:20,694 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0013_01_000002 Container Transitioned from ACQUIRED to RUNNING
2023-04-11 15:02:20,924 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0013_01_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-11 15:02:20,924 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0013	CONTAINERID=container_e02_1672231888656_0013_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-11 15:02:20,924 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1672231888656_0013_000001
2023-04-11 15:02:20,924 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1672231888656_0013_000001
2023-04-11 15:02:20,924 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1672231888656_0013_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2023-04-11 15:02:20,925 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1672231888656_0013 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2023-04-11 15:02:20,927 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=root	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0013
2023-04-11 15:02:20,928 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0013,name=Spark shell,user=root,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0013/,appMasterHost=10.130.0.6,submitTime=1681225335892,startTime=1681225335893,finishTime=1681225340303,finalStatus=SUCCEEDED,memorySeconds=9208,vcoreSeconds=5,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=SPARK,resourceSeconds=9208 MB-seconds\, 5 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2023-04-11 15:02:20,928 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1672231888656_0013_000001 is done. finalState=FINISHED
2023-04-11 15:02:20,928 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0013_01_000002 Container Transitioned from RUNNING to KILLED
2023-04-11 15:02:20,928 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0013	CONTAINERID=container_e02_1672231888656_0013_01_000002	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-11 15:02:20,928 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e02_1672231888656_0013_01_000003 Container Transitioned from ACQUIRED to KILLED
2023-04-11 15:02:20,928 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=root	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1672231888656_0013	CONTAINERID=container_e02_1672231888656_0013_01_000003	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-11 15:02:20,929 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1672231888656_0013 requests cleared
2023-04-11 15:02:20,929 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1672231888656_0013 user: root queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-04-11 15:02:20,929 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1672231888656_0013 user: root leaf-queue of parent: root #applications: 0
2023-04-11 15:02:20,929 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1672231888656_0013_000001
2023-04-11 15:02:21,739 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Container container_e02_1672231888656_0013_01_000002 completed with event FINISHED, but corresponding RMContainer doesn't exist.
2023-04-11 15:10:46,731 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: RECEIVED SIGNAL 15: SIGTERM
2023-04-11 15:10:46,743 ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted
2023-04-11 15:10:46,752 INFO org.eclipse.jetty.server.handler.ContextHandler: Stopped o.e.j.w.WebAppContext@16c587de{/,null,UNAVAILABLE}{/cluster}
2023-04-11 15:10:46,772 INFO org.eclipse.jetty.server.AbstractConnector: Stopped ServerConnector@13741d5a{HTTP/1.1,[http/1.1]}{0.0.0.0:8088}
2023-04-11 15:10:46,798 INFO org.eclipse.jetty.server.handler.ContextHandler: Stopped o.e.j.w.WebAppContext@3b1ed14b{/ui2,file:///usr/lib/hadoop-yarn/webapps/ui2/,UNAVAILABLE}
2023-04-11 15:10:46,799 INFO org.eclipse.jetty.server.handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@7c351808{/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.1.2.jar!/webapps/static,UNAVAILABLE}
2023-04-11 15:10:46,799 INFO org.eclipse.jetty.server.handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@61526469{/logs,file:///var/log/hadoop-yarn/,UNAVAILABLE}
2023-04-11 15:10:46,821 INFO org.apache.hadoop.ipc.Server: Stopping server on 8032
2023-04-11 15:10:46,860 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: Node with node id : adh-test-00-mnode1.ru-central1.internal:8041 has shutdown, hence unregistering the node.
2023-04-11 15:10:46,865 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: Deactivating Node adh-test-00-mnode1.ru-central1.internal:8041 as it is now SHUTDOWN
2023-04-11 15:10:46,865 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: adh-test-00-mnode1.ru-central1.internal:8041 Node Transitioned from RUNNING to SHUTDOWN
2023-04-11 15:10:46,869 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Removed node adh-test-00-mnode1.ru-central1.internal:8041 clusterResource: <memory:14114, vCores:4>
2023-04-11 15:10:46,891 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:10:46,892 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8032
2023-04-11 15:10:46,893 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2023-04-11 15:10:46,909 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:10:46,911 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:10:46,911 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:10:46,915 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:10:46,916 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:10:46,930 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:10:46,931 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:10:46,935 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:10:46,936 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:10:46,939 INFO org.apache.hadoop.ipc.Server: Stopping server on 8033
2023-04-11 15:10:46,941 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2023-04-11 15:10:46,943 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8033
2023-04-11 15:10:46,947 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning to standby state
2023-04-11 15:10:46,947 WARN org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher: org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher$LauncherThread interrupted. Returning.
2023-04-11 15:10:46,948 INFO org.apache.hadoop.ipc.Server: Stopping server on 8030
2023-04-11 15:10:46,951 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2023-04-11 15:10:46,951 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8030
2023-04-11 15:10:46,951 INFO org.apache.hadoop.ipc.Server: Stopping server on 8031
2023-04-11 15:10:46,978 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8031
2023-04-11 15:10:46,978 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2023-04-11 15:10:46,980 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: NMLivelinessMonitor thread interrupted
2023-04-11 15:10:46,981 ERROR org.apache.hadoop.yarn.event.EventDispatcher: Returning, interrupted : java.lang.InterruptedException
2023-04-11 15:10:46,981 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager: org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager thread interrupted
2023-04-11 15:10:47,015 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:10:47,016 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: AMLivelinessMonitor thread interrupted
2023-04-11 15:10:47,016 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: AMLivelinessMonitor thread interrupted
2023-04-11 15:10:47,016 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.ContainerAllocationExpirer thread interrupted
2023-04-11 15:10:47,017 ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted
2023-04-11 15:10:47,017 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppLifetimeMonitor thread interrupted
2023-04-11 15:10:47,017 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping ResourceManager metrics system...
2023-04-11 15:10:47,044 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system stopped.
2023-04-11 15:10:47,045 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system shutdown complete.
2023-04-11 15:10:47,045 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:10:47,048 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioned to standby state
2023-04-11 15:10:47,053 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down ResourceManager at adh-test-00-mnode1.ru-central1.internal/10.130.0.4
************************************************************/
2023-04-11 15:12:17,864 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting ResourceManager
STARTUP_MSG:   host = adh-test-00-mnode1.ru-central1.internal/10.130.0.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.2
STARTUP_MSG:   classpath = /etc/hadoop/conf:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/stax2-api.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jetty-io-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-security-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/jetty-server-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/woodstox-core.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/zookeeper-3.4.13.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jetty-util-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/httpclient-4.5.2.jar:/usr/lib/hadoop/lib/httpcore-4.4.4.jar:/usr/lib/hadoop/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/jackson-annotations-2.7.8.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/jackson-core-2.7.8.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jackson-databind-2.7.8.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-xml-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/jetty-http-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.1.2.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-auth-3.1.2.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-common-3.1.2-tests.jar:/usr/lib/hadoop/.//hadoop-common-3.1.2.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms-3.1.2.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-nfs-3.1.2.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.2.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.1.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.1.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.1.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.1.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-2.8.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.271.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-0.8.2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//lz4-1.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//netty-buffer-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//netty-codec-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//netty-codec-http-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//netty-common-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//netty-handler-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//netty-resolver-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//netty-transport-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.1.2-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-yarn/./:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar
STARTUP_MSG:   build = git@ssh.gitlab.adsw.io:arenadata/infrastructure/code/ci/prj_adh.git -r 15525ac41ebda8bc83c285e9b6ea7fb9921fd7fd; compiled by 'jenkins' on 2022-09-08T14:33Z
STARTUP_MSG:   java = 1.8.0_362
************************************************************/
2023-04-11 15:12:17,932 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]
2023-04-11 15:12:18,483 INFO org.apache.hadoop.conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf/core-site.xml
2023-04-11 15:12:18,849 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found
2023-04-11 15:12:18,850 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.
2023-04-11 15:12:19,014 INFO org.apache.hadoop.conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf/yarn-site.xml
2023-04-11 15:12:19,101 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
2023-04-11 15:12:19,354 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
2023-04-11 15:12:19,365 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
2023-04-11 15:12:19,384 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
2023-04-11 15:12:19,615 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreFactory: Using RMStateStore implementation - class org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore
2023-04-11 15:12:19,630 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
2023-04-11 15:12:19,637 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
2023-04-11 15:12:19,637 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
2023-04-11 15:12:19,716 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher
2023-04-11 15:12:19,718 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher
2023-04-11 15:12:19,719 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher
2023-04-11 15:12:19,720 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher
2023-04-11 15:12:20,039 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-resourcemanager.properties,hadoop-metrics2.properties
2023-04-11 15:12:20,087 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2023-04-11 15:12:20,087 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system started
2023-04-11 15:12:20,110 INFO org.apache.hadoop.yarn.security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.
2023-04-11 15:12:20,114 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
2023-04-11 15:12:20,129 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
2023-04-11 15:12:20,132 INFO org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo: Registered RMNMInfo MBean
2023-04-11 15:12:20,132 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.
2023-04-11 15:12:20,285 INFO org.apache.hadoop.util.HostsFileReader: Refreshing hosts (include/exclude) list
2023-04-11 15:12:20,366 INFO org.apache.hadoop.util.HostsFileReader: Adding a node "adh-test-00-mnode1.ru-central1.internal" to the list of included hosts from /etc/hadoop/conf/include-path
2023-04-11 15:12:20,366 INFO org.apache.hadoop.util.HostsFileReader: Adding a node "adh-test-00-mnode2.ru-central1.internal" to the list of included hosts from /etc/hadoop/conf/include-path
2023-04-11 15:12:20,366 INFO org.apache.hadoop.util.HostsFileReader: Adding a node "adh-test-00-mnode3.ru-central1.internal" to the list of included hosts from /etc/hadoop/conf/include-path
2023-04-11 15:12:20,749 INFO org.apache.hadoop.conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf/capacity-scheduler.xml
2023-04-11 15:12:20,798 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1024, vCores:1>
2023-04-11 15:12:20,798 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Maximum allocation = <memory:4096, vCores:2>
2023-04-11 15:12:20,928 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined
2023-04-11 15:12:20,928 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined
2023-04-11 15:12:20,945 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,
, reservationsContinueLooking=true, orderingPolicy=utilization, priority=0
2023-04-11 15:12:20,945 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root
2023-04-11 15:12:20,963 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined
2023-04-11 15:12:20,963 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined
2023-04-11 15:12:20,968 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Initializing default
capacity = 1.0 [= (float) configuredCapacity / 100 ]
absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]
maxCapacity = 1.0 [= configuredMaxCapacity ]
absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
effectiveMinResource=<memory:0, vCores:0>
 , effectiveMaxResource=<memory:0, vCores:0>
userLimit = 100 [= configuredUserLimit ]
userLimitFactor = 1.0 [= configuredUserLimitFactor ]
maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
maxAMResourcePerQueuePercent = 0.2 [= configuredMaximumAMResourcePercent ]
minimumAllocationFactor = 0.75 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
maximumAllocation = <memory:4096, vCores:2> [= configuredMaxAllocation ]
numContainers = 0 [= currentNumContainers ]
state = RUNNING [= configuredState ]
acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]
nodeLocalityDelay = 40
rackLocalityAdditionalDelay = -1
labels=*,
reservationsContinueLooking = true
preemptionDisabled = true
defaultAppPriorityPerQueue = 0
priority = 0
maxLifetime = -1 seconds
defaultLifetime = -1 seconds
2023-04-11 15:12:20,969 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>
2023-04-11 15:12:20,969 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2023-04-11 15:12:20,971 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2023-04-11 15:12:20,971 INFO org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false
2023-04-11 15:12:20,972 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAllocation=<<memory:4096, vCores:2>>, asynchronousScheduling=false, asyncScheduleInterval=5ms
2023-04-11 15:12:20,976 INFO org.apache.hadoop.conf.Configuration: dynamic-resources.xml not found
2023-04-11 15:12:20,978 INFO org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].
2023-04-11 15:12:20,978 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.
2023-04-11 15:12:20,981 INFO org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. 
2023-04-11 15:12:21,315 INFO org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl: Timeline service address: adh-test-00-mnode2.ru-central1.internal:8188
2023-04-11 15:12:21,859 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2023-04-11 15:12:21,859 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2023-04-11 15:12:21,859 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2023-04-11 15:12:21,859 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2023-04-11 15:12:21,859 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2023-04-11 15:12:21,859 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2023-04-11 15:12:21,860 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2023-04-11 15:12:21,860 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2023-04-11 15:12:21,860 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2023-04-11 15:12:21,860 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2023-04-11 15:12:21,912 INFO org.eclipse.jetty.util.log: Logging initialized @19139ms
2023-04-11 15:12:21,970 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Using webapps at: file:/usr/lib/hadoop-yarn/webapps/ui2
2023-04-11 15:12:22,076 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2023-04-11 15:12:22,095 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined
2023-04-11 15:12:22,099 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2023-04-11 15:12:22,104 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster
2023-04-11 15:12:22,104 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs
2023-04-11 15:12:22,104 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static
2023-04-11 15:12:22,112 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster
2023-04-11 15:12:22,112 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-04-11 15:12:22,112 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-04-11 15:12:22,115 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /cluster/*
2023-04-11 15:12:22,115 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /ws/*
2023-04-11 15:12:22,115 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /app/*
2023-04-11 15:12:22,968 INFO org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules
2023-04-11 15:12:22,978 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 8088
2023-04-11 15:12:22,979 INFO org.eclipse.jetty.server.Server: jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2023-04-11 15:12:23,037 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2023-04-11 15:12:23,164 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-11 15:12:23,183 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@569bf9eb{/logs,file:///var/log/hadoop-yarn/,AVAILABLE}
2023-04-11 15:12:23,184 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2023-04-11 15:12:23,187 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@eb6449b{/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.1.2.jar!/webapps/static,AVAILABLE}
2023-04-11 15:12:23,198 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-11 15:12:25,950 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@ebd06a9{/,file:///tmp/jetty-0.0.0.0-8088-cluster-_-any-4413959544758313553.dir/webapp/,AVAILABLE}{/cluster}
2023-04-11 15:12:26,034 INFO org.eclipse.jetty.webapp.StandardDescriptorProcessor: NO JSP Support for /ui2, did not find org.eclipse.jetty.jsp.JettyJspServlet
2023-04-11 15:12:26,042 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@a7ad6e5{/ui2,file:///usr/lib/hadoop-yarn/webapps/ui2/,AVAILABLE}
2023-04-11 15:12:26,055 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@43841613{HTTP/1.1,[http/1.1]}{0.0.0.0:8088}
2023-04-11 15:12:26,055 INFO org.eclipse.jetty.server.Server: Started @23283ms
2023-04-11 15:12:26,055 INFO org.apache.hadoop.yarn.webapp.WebApps: Web app cluster started at 8088
2023-04-11 15:12:26,268 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 100 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2023-04-11 15:12:26,292 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8033
2023-04-11 15:12:26,350 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server
2023-04-11 15:12:26,356 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning to active state
2023-04-11 15:12:26,351 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2023-04-11 15:12:26,352 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8033: starting
2023-04-11 15:12:26,594 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Using state database at /srv/hadoop-yarn/leveldb-state-store/yarn-rm-state for recovery
2023-04-11 15:12:26,869 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Recovery started
2023-04-11 15:12:26,879 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Loaded RM state version info 1.1
2023-04-11 15:12:26,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 8 RM delegation token master keys
2023-04-11 15:12:26,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 0 RM delegation tokens
2023-04-11 15:12:26,968 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 11 applications and 11 application attempts
2023-04-11 15:12:26,980 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 0 reservations
2023-04-11 15:12:26,981 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: recovering RMDelegationTokenSecretManager.
2023-04-11 15:12:26,985 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager: Recovering 11 applications
2023-04-11 15:12:27,071 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:12:27,127 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 2 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:12:27,129 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:12:27,131 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 2 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:12:27,132 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 5 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:12:27,134 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 6 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:12:27,149 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 7 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:12:27,151 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 8 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:12:27,153 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 9 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:12:27,155 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 11 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:12:27,157 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 13 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:12:27,158 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1671022279734_0001
2023-04-11 15:12:27,166 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1671022279734_0001,name=QuasiMonteCarlo,user=yarn,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1671022279734_0001/,appMasterHost=N/A,submitTime=1671022505181,startTime=1671022505266,finishTime=1671022524607,finalStatus=SUCCEEDED,memorySeconds=216125,vcoreSeconds=183,preemptedMemorySeconds=216125,preemptedVcoreSeconds=183,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=216125 MB-seconds\, 183 vcore-seconds,preemptedResourceSeconds=216125 MB-seconds\, 183 vcore-seconds
2023-04-11 15:12:27,167 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1671022279734_0002
2023-04-11 15:12:27,178 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager: Successfully recovered 11 out of 11 applications
2023-04-11 15:12:27,178 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Recovery ended
2023-04-11 15:12:27,179 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-11 15:12:27,185 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-11 15:12:27,185 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-11 15:12:27,185 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 111
2023-04-11 15:12:27,185 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-11 15:12:27,187 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1671022279734_0002,name=QuasiMonteCarlo,user=hdfs,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1671022279734_0002/,appMasterHost=N/A,submitTime=1671023676043,startTime=1671023676044,finishTime=1671023694897,finalStatus=SUCCEEDED,memorySeconds=228352,vcoreSeconds=193,preemptedMemorySeconds=228352,preemptedVcoreSeconds=193,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=228352 MB-seconds\, 193 vcore-seconds,preemptedResourceSeconds=228352 MB-seconds\, 193 vcore-seconds
2023-04-11 15:12:27,194 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2023-04-11 15:12:27,194 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-11 15:12:27,194 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 112
2023-04-11 15:12:27,194 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-11 15:12:27,213 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=root	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0001
2023-04-11 15:12:27,225 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0001,name=word count,user=root,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0001/,appMasterHost=N/A,submitTime=1672378445838,startTime=1672378445838,finishTime=1672378452169,finalStatus=FAILED,memorySeconds=6121,vcoreSeconds=2,preemptedMemorySeconds=6121,preemptedVcoreSeconds=2,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=6121 MB-seconds\, 2 vcore-seconds,preemptedResourceSeconds=6121 MB-seconds\, 2 vcore-seconds
2023-04-11 15:12:27,225 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0002
2023-04-11 15:12:27,226 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0002,name=word count,user=student48,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0002/,appMasterHost=N/A,submitTime=1672378503807,startTime=1672378503809,finishTime=1672378519495,finalStatus=SUCCEEDED,memorySeconds=37951,vcoreSeconds=21,preemptedMemorySeconds=37951,preemptedVcoreSeconds=21,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=37951 MB-seconds\, 21 vcore-seconds,preemptedResourceSeconds=37951 MB-seconds\, 21 vcore-seconds
2023-04-11 15:12:27,226 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0005
2023-04-11 15:12:27,227 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0005,name=word count,user=student48,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0005/,appMasterHost=N/A,submitTime=1674569701969,startTime=1674569701970,finishTime=1674569712186,finalStatus=FAILED,memorySeconds=18149,vcoreSeconds=8,preemptedMemorySeconds=18149,preemptedVcoreSeconds=8,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=18149 MB-seconds\, 8 vcore-seconds,preemptedResourceSeconds=18149 MB-seconds\, 8 vcore-seconds
2023-04-11 15:12:27,227 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0006
2023-04-11 15:12:27,228 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0006,name=word count,user=student48,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0006/,appMasterHost=N/A,submitTime=1674569761159,startTime=1674569761159,finishTime=1674569778485,finalStatus=SUCCEEDED,memorySeconds=45282,vcoreSeconds=25,preemptedMemorySeconds=45282,preemptedVcoreSeconds=25,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=45282 MB-seconds\, 25 vcore-seconds,preemptedResourceSeconds=45282 MB-seconds\, 25 vcore-seconds
2023-04-11 15:12:27,228 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0007
2023-04-11 15:12:27,229 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0007,name=word count,user=student48,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0007/,appMasterHost=N/A,submitTime=1674569840580,startTime=1674569840580,finishTime=1674569854534,finalStatus=SUCCEEDED,memorySeconds=36318,vcoreSeconds=20,preemptedMemorySeconds=36318,preemptedVcoreSeconds=20,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=36318 MB-seconds\, 20 vcore-seconds,preemptedResourceSeconds=36318 MB-seconds\, 20 vcore-seconds
2023-04-11 15:12:27,229 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0008
2023-04-11 15:12:27,235 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0008,name=authors.jar,user=student48,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0008/,appMasterHost=N/A,submitTime=1676746201437,startTime=1676746201438,finishTime=1676746206409,finalStatus=FAILED,memorySeconds=6057,vcoreSeconds=2,preemptedMemorySeconds=6057,preemptedVcoreSeconds=2,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=6057 MB-seconds\, 2 vcore-seconds,preemptedResourceSeconds=6057 MB-seconds\, 2 vcore-seconds
2023-04-11 15:12:27,236 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0009
2023-04-11 15:12:27,238 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0009,name=authors.jar,user=student48,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0009/,appMasterHost=N/A,submitTime=1676746394492,startTime=1676746394493,finishTime=1676746417277,finalStatus=SUCCEEDED,memorySeconds=92284,vcoreSeconds=60,preemptedMemorySeconds=92284,preemptedVcoreSeconds=60,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=92284 MB-seconds\, 60 vcore-seconds,preemptedResourceSeconds=92284 MB-seconds\, 60 vcore-seconds
2023-04-11 15:12:27,250 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=spark	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0011
2023-04-11 15:12:27,255 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0011,name=Spark shell,user=spark,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0011/,appMasterHost=N/A,submitTime=1681215416141,startTime=1681215416142,finishTime=1681215437454,finalStatus=SUCCEEDED,memorySeconds=87266,vcoreSeconds=51,preemptedMemorySeconds=87266,preemptedVcoreSeconds=51,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=SPARK,resourceSeconds=87266 MB-seconds\, 51 vcore-seconds,preemptedResourceSeconds=87266 MB-seconds\, 51 vcore-seconds
2023-04-11 15:12:27,255 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=root	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0013
2023-04-11 15:12:27,256 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0013,name=Spark shell,user=root,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0013/,appMasterHost=N/A,submitTime=1681225335892,startTime=1681225335893,finishTime=1681225340303,finalStatus=SUCCEEDED,memorySeconds=2948,vcoreSeconds=2,preemptedMemorySeconds=2948,preemptedVcoreSeconds=2,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=SPARK,resourceSeconds=2948 MB-seconds\, 2 vcore-seconds,preemptedResourceSeconds=2948 MB-seconds\, 2 vcore-seconds
2023-04-11 15:12:28,706 INFO org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager failed in state STARTED
java.net.ConnectException: Call From adh-test-00-mnode1.ru-central1.internal/10.130.0.4 to adh-test-00-mnode1.ru-central1.internal:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy89.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:653)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy90.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2419)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2395)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1325)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1322)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1339)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1314)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)
	at org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore.init(FileSystemNodeLabelsStore.java:87)
	at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.initNodeLabelStore(CommonNodeLabelsManager.java:253)
	at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.serviceStart(CommonNodeLabelsManager.java:268)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:869)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1228)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1269)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1265)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1265)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1316)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1501)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:690)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:794)
	at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)
	at org.apache.hadoop.ipc.Client.call(Client.java:1403)
	... 40 more
2023-04-11 15:12:28,717 INFO org.apache.hadoop.service.AbstractService: Service RMActiveServices failed in state STARTED
org.apache.hadoop.service.ServiceStateException: java.net.ConnectException: Call From adh-test-00-mnode1.ru-central1.internal/10.130.0.4 to adh-test-00-mnode1.ru-central1.internal:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:203)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:869)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1228)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1269)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1265)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1265)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1316)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1501)
Caused by: java.net.ConnectException: Call From adh-test-00-mnode1.ru-central1.internal/10.130.0.4 to adh-test-00-mnode1.ru-central1.internal:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy89.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:653)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy90.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2419)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2395)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1325)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1322)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1339)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1314)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)
	at org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore.init(FileSystemNodeLabelsStore.java:87)
	at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.initNodeLabelStore(CommonNodeLabelsManager.java:253)
	at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.serviceStart(CommonNodeLabelsManager.java:268)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	... 13 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:690)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:794)
	at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)
	at org.apache.hadoop.ipc.Client.call(Client.java:1403)
	... 40 more
2023-04-11 15:12:28,719 ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted
2023-04-11 15:12:28,721 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: AMLivelinessMonitor thread interrupted
2023-04-11 15:12:28,721 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: AMLivelinessMonitor thread interrupted
2023-04-11 15:12:28,721 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.ContainerAllocationExpirer thread interrupted
2023-04-11 15:12:28,726 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppLifetimeMonitor thread interrupted
2023-04-11 15:12:28,734 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping ResourceManager metrics system...
2023-04-11 15:12:28,734 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system stopped.
2023-04-11 15:12:28,734 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system shutdown complete.
2023-04-11 15:12:28,734 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:12:28,739 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
2023-04-11 15:12:28,818 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
2023-04-11 15:12:28,842 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
2023-04-11 15:12:28,846 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
2023-04-11 15:12:28,846 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreFactory: Using RMStateStore implementation - class org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore
2023-04-11 15:12:28,846 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
2023-04-11 15:12:28,846 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
2023-04-11 15:12:28,846 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
2023-04-11 15:12:28,847 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher
2023-04-11 15:12:28,847 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher
2023-04-11 15:12:28,847 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher
2023-04-11 15:12:28,847 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher
2023-04-11 15:12:28,850 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-resourcemanager.properties,hadoop-metrics2.properties
2023-04-11 15:12:28,852 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2023-04-11 15:12:28,852 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system started
2023-04-11 15:12:28,855 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
2023-04-11 15:12:28,855 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
2023-04-11 15:12:28,855 WARN org.apache.hadoop.metrics2.util.MBeans: Failed to register MBean "Hadoop:service=ResourceManager,name=RMNMInfo": Instance already exists.
2023-04-11 15:12:28,855 INFO org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo: Registered RMNMInfo MBean
2023-04-11 15:12:28,855 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.
2023-04-11 15:12:28,855 INFO org.apache.hadoop.util.HostsFileReader: Refreshing hosts (include/exclude) list
2023-04-11 15:12:28,856 INFO org.apache.hadoop.util.HostsFileReader: Adding a node "adh-test-00-mnode1.ru-central1.internal" to the list of included hosts from /etc/hadoop/conf/include-path
2023-04-11 15:12:28,856 INFO org.apache.hadoop.util.HostsFileReader: Adding a node "adh-test-00-mnode2.ru-central1.internal" to the list of included hosts from /etc/hadoop/conf/include-path
2023-04-11 15:12:28,856 INFO org.apache.hadoop.util.HostsFileReader: Adding a node "adh-test-00-mnode3.ru-central1.internal" to the list of included hosts from /etc/hadoop/conf/include-path
2023-04-11 15:12:28,866 INFO org.apache.hadoop.conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf/capacity-scheduler.xml
2023-04-11 15:12:28,884 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1024, vCores:1>
2023-04-11 15:12:28,884 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Maximum allocation = <memory:4096, vCores:2>
2023-04-11 15:12:28,892 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined
2023-04-11 15:12:28,892 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined
2023-04-11 15:12:28,893 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,
, reservationsContinueLooking=true, orderingPolicy=utilization, priority=0
2023-04-11 15:12:28,893 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root
2023-04-11 15:12:28,958 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined
2023-04-11 15:12:28,958 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined
2023-04-11 15:12:28,960 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Initializing default
capacity = 1.0 [= (float) configuredCapacity / 100 ]
absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]
maxCapacity = 1.0 [= configuredMaxCapacity ]
absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
effectiveMinResource=<memory:0, vCores:0>
 , effectiveMaxResource=<memory:0, vCores:0>
userLimit = 100 [= configuredUserLimit ]
userLimitFactor = 1.0 [= configuredUserLimitFactor ]
maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
maxAMResourcePerQueuePercent = 0.2 [= configuredMaximumAMResourcePercent ]
minimumAllocationFactor = 0.75 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
maximumAllocation = <memory:4096, vCores:2> [= configuredMaxAllocation ]
numContainers = 0 [= currentNumContainers ]
state = RUNNING [= configuredState ]
acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]
nodeLocalityDelay = 40
rackLocalityAdditionalDelay = -1
labels=*,
reservationsContinueLooking = true
preemptionDisabled = true
defaultAppPriorityPerQueue = 0
priority = 0
maxLifetime = -1 seconds
defaultLifetime = -1 seconds
2023-04-11 15:12:28,960 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>
2023-04-11 15:12:28,960 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2023-04-11 15:12:28,961 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2023-04-11 15:12:28,961 INFO org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false
2023-04-11 15:12:28,961 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAllocation=<<memory:4096, vCores:2>>, asynchronousScheduling=false, asyncScheduleInterval=5ms
2023-04-11 15:12:28,961 INFO org.apache.hadoop.conf.Configuration: dynamic-resources.xml not found
2023-04-11 15:12:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].
2023-04-11 15:12:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.
2023-04-11 15:12:28,962 INFO org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. 
2023-04-11 15:12:28,963 INFO org.apache.hadoop.service.AbstractService: Service ResourceManager failed in state STARTED
org.apache.hadoop.service.ServiceStateException: java.net.ConnectException: Call From adh-test-00-mnode1.ru-central1.internal/10.130.0.4 to adh-test-00-mnode1.ru-central1.internal:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:203)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:869)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1228)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1269)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1265)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1265)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1316)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1501)
Caused by: java.net.ConnectException: Call From adh-test-00-mnode1.ru-central1.internal/10.130.0.4 to adh-test-00-mnode1.ru-central1.internal:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy89.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:653)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy90.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2419)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2395)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1325)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1322)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1339)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1314)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)
	at org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore.init(FileSystemNodeLabelsStore.java:87)
	at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.initNodeLabelStore(CommonNodeLabelsManager.java:253)
	at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.serviceStart(CommonNodeLabelsManager.java:268)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	... 13 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:690)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:794)
	at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)
	at org.apache.hadoop.ipc.Client.call(Client.java:1403)
	... 40 more
2023-04-11 15:12:28,970 ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted
2023-04-11 15:12:28,993 INFO org.eclipse.jetty.server.handler.ContextHandler: Stopped o.e.j.w.WebAppContext@ebd06a9{/,null,UNAVAILABLE}{/cluster}
2023-04-11 15:12:28,997 INFO org.eclipse.jetty.server.AbstractConnector: Stopped ServerConnector@43841613{HTTP/1.1,[http/1.1]}{0.0.0.0:8088}
2023-04-11 15:12:28,999 INFO org.eclipse.jetty.server.handler.ContextHandler: Stopped o.e.j.w.WebAppContext@a7ad6e5{/ui2,file:///usr/lib/hadoop-yarn/webapps/ui2/,UNAVAILABLE}
2023-04-11 15:12:28,999 INFO org.eclipse.jetty.server.handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@eb6449b{/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.1.2.jar!/webapps/static,UNAVAILABLE}
2023-04-11 15:12:28,999 INFO org.eclipse.jetty.server.handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@569bf9eb{/logs,file:///var/log/hadoop-yarn/,UNAVAILABLE}
2023-04-11 15:12:29,027 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:12:29,028 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:12:29,029 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:12:29,044 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:12:29,054 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:12:29,055 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:12:29,055 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:12:29,055 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:12:29,055 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:12:29,055 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.
2023-04-11 15:12:29,066 INFO org.apache.hadoop.ipc.Server: Stopping server on 8033
2023-04-11 15:12:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning to standby state
2023-04-11 15:12:29,083 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server Responder
2023-04-11 15:12:29,083 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioned to standby state
2023-04-11 15:12:29,083 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error starting ResourceManager
org.apache.hadoop.service.ServiceStateException: java.net.ConnectException: Call From adh-test-00-mnode1.ru-central1.internal/10.130.0.4 to adh-test-00-mnode1.ru-central1.internal:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:203)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:869)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1228)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1269)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1265)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1265)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1316)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1501)
Caused by: java.net.ConnectException: Call From adh-test-00-mnode1.ru-central1.internal/10.130.0.4 to adh-test-00-mnode1.ru-central1.internal:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy89.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:653)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy90.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2419)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2395)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1325)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1322)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1339)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1314)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)
	at org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore.init(FileSystemNodeLabelsStore.java:87)
	at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.initNodeLabelStore(CommonNodeLabelsManager.java:253)
	at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.serviceStart(CommonNodeLabelsManager.java:268)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
	... 13 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:690)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:794)
	at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)
	at org.apache.hadoop.ipc.Client.call(Client.java:1403)
	... 40 more
2023-04-11 15:12:29,084 INFO org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 8033
2023-04-11 15:12:29,207 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down ResourceManager at adh-test-00-mnode1.ru-central1.internal/10.130.0.4
************************************************************/
2023-04-11 15:38:00,338 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting ResourceManager
STARTUP_MSG:   host = adh-test-00-mnode1.ru-central1.internal/10.130.0.4
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.1.2
STARTUP_MSG:   classpath = /etc/hadoop/conf:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/stax2-api.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/jetty-io-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/jetty-security-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/jetty-server-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/woodstox-core.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/zookeeper-3.4.13.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/jetty-util-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/httpclient-4.5.2.jar:/usr/lib/hadoop/lib/httpcore-4.4.4.jar:/usr/lib/hadoop/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/jackson-annotations-2.7.8.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/jackson-core-2.7.8.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jackson-databind-2.7.8.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-xml-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/jetty-http-9.3.24.v20180605.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/.//hadoop-annotations-3.1.2.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-auth-3.1.2.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-common-3.1.2-tests.jar:/usr/lib/hadoop/.//hadoop-common-3.1.2.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-kms-3.1.2.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-nfs-3.1.2.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.2.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.4.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.3.24.v20180605.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.1.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.1.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.1.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.1.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.1.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-2.8.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.271.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-0.8.2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//lz4-1.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//netty-buffer-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//netty-codec-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//netty-codec-http-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//netty-common-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//netty-handler-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//netty-resolver-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//netty-transport-4.1.17.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.1.2-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-yarn/./:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.1.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar
STARTUP_MSG:   build = git@ssh.gitlab.adsw.io:arenadata/infrastructure/code/ci/prj_adh.git -r 15525ac41ebda8bc83c285e9b6ea7fb9921fd7fd; compiled by 'jenkins' on 2022-09-08T14:33Z
STARTUP_MSG:   java = 1.8.0_362
************************************************************/
2023-04-11 15:38:00,346 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]
2023-04-11 15:38:00,533 INFO org.apache.hadoop.conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf/core-site.xml
2023-04-11 15:38:00,646 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found
2023-04-11 15:38:00,647 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.
2023-04-11 15:38:00,748 INFO org.apache.hadoop.conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf/yarn-site.xml
2023-04-11 15:38:00,791 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher
2023-04-11 15:38:00,827 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms
2023-04-11 15:38:00,830 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms
2023-04-11 15:38:00,836 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms
2023-04-11 15:38:00,893 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreFactory: Using RMStateStore implementation - class org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore
2023-04-11 15:38:00,894 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler
2023-04-11 15:38:00,899 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager
2023-04-11 15:38:00,899 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
2023-04-11 15:38:00,934 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher
2023-04-11 15:38:00,935 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher
2023-04-11 15:38:00,936 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher
2023-04-11 15:38:00,937 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher
2023-04-11 15:38:01,046 WARN org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-resourcemanager.properties,hadoop-metrics2.properties
2023-04-11 15:38:01,064 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2023-04-11 15:38:01,065 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ResourceManager metrics system started
2023-04-11 15:38:01,078 INFO org.apache.hadoop.yarn.security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.
2023-04-11 15:38:01,081 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager
2023-04-11 15:38:01,089 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher
2023-04-11 15:38:01,090 INFO org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo: Registered RMNMInfo MBean
2023-04-11 15:38:01,091 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.
2023-04-11 15:38:01,096 INFO org.apache.hadoop.util.HostsFileReader: Refreshing hosts (include/exclude) list
2023-04-11 15:38:01,097 INFO org.apache.hadoop.util.HostsFileReader: Adding a node "adh-test-00-mnode1.ru-central1.internal" to the list of included hosts from /etc/hadoop/conf/include-path
2023-04-11 15:38:01,097 INFO org.apache.hadoop.util.HostsFileReader: Adding a node "adh-test-00-mnode2.ru-central1.internal" to the list of included hosts from /etc/hadoop/conf/include-path
2023-04-11 15:38:01,097 INFO org.apache.hadoop.util.HostsFileReader: Adding a node "adh-test-00-mnode3.ru-central1.internal" to the list of included hosts from /etc/hadoop/conf/include-path
2023-04-11 15:38:01,128 INFO org.apache.hadoop.conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf/capacity-scheduler.xml
2023-04-11 15:38:01,142 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1024, vCores:1>
2023-04-11 15:38:01,142 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Maximum allocation = <memory:4096, vCores:2>
2023-04-11 15:38:01,181 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined
2023-04-11 15:38:01,181 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined
2023-04-11 15:38:01,192 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=ADMINISTER_QUEUE:*SUBMIT_APP:*, labels=*,
, reservationsContinueLooking=true, orderingPolicy=utilization, priority=0
2023-04-11 15:38:01,192 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root
2023-04-11 15:38:01,206 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined
2023-04-11 15:38:01,207 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined
2023-04-11 15:38:01,212 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Initializing default
capacity = 1.0 [= (float) configuredCapacity / 100 ]
absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]
maxCapacity = 1.0 [= configuredMaxCapacity ]
absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]
effectiveMinResource=<memory:0, vCores:0>
 , effectiveMaxResource=<memory:0, vCores:0>
userLimit = 100 [= configuredUserLimit ]
userLimitFactor = 1.0 [= configuredUserLimitFactor ]
maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]
maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]
usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]
absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]
maxAMResourcePerQueuePercent = 0.2 [= configuredMaximumAMResourcePercent ]
minimumAllocationFactor = 0.75 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]
maximumAllocation = <memory:4096, vCores:2> [= configuredMaxAllocation ]
numContainers = 0 [= currentNumContainers ]
state = RUNNING [= configuredState ]
acls = ADMINISTER_QUEUE:*SUBMIT_APP:* [= configuredAcls ]
nodeLocalityDelay = 40
rackLocalityAdditionalDelay = -1
labels=*,
reservationsContinueLooking = true
preemptionDisabled = true
defaultAppPriorityPerQueue = 0
priority = 0
maxLifetime = -1 seconds
defaultLifetime = -1 seconds
2023-04-11 15:38:01,212 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>
2023-04-11 15:38:01,212 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2023-04-11 15:38:01,214 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0
2023-04-11 15:38:01,215 INFO org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false
2023-04-11 15:38:01,215 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAllocation=<<memory:4096, vCores:2>>, asynchronousScheduling=false, asyncScheduleInterval=5ms
2023-04-11 15:38:01,218 INFO org.apache.hadoop.conf.Configuration: dynamic-resources.xml not found
2023-04-11 15:38:01,220 INFO org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].
2023-04-11 15:38:01,220 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.
2023-04-11 15:38:01,223 INFO org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. 
2023-04-11 15:38:01,426 INFO org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl: Timeline service address: adh-test-00-mnode2.ru-central1.internal:8188
2023-04-11 15:38:01,699 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2023-04-11 15:38:01,699 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2023-04-11 15:38:01,699 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2023-04-11 15:38:01,699 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2023-04-11 15:38:01,699 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2023-04-11 15:38:01,699 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2023-04-11 15:38:01,699 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2023-04-11 15:38:01,699 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2023-04-11 15:38:01,699 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2023-04-11 15:38:01,699 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler
2023-04-11 15:38:01,746 INFO org.eclipse.jetty.util.log: Logging initialized @1969ms
2023-04-11 15:38:01,795 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Using webapps at: file:/usr/lib/hadoop-yarn/webapps/ui2
2023-04-11 15:38:01,847 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2023-04-11 15:38:01,851 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined
2023-04-11 15:38:01,855 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2023-04-11 15:38:01,860 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster
2023-04-11 15:38:01,860 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs
2023-04-11 15:38:01,860 INFO org.apache.hadoop.http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static
2023-04-11 15:38:01,861 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster
2023-04-11 15:38:01,861 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-04-11 15:38:01,861 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-04-11 15:38:01,863 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /cluster/*
2023-04-11 15:38:01,863 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /ws/*
2023-04-11 15:38:01,863 INFO org.apache.hadoop.http.HttpServer2: adding path spec: /app/*
2023-04-11 15:38:02,321 INFO org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules
2023-04-11 15:38:02,329 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 8088
2023-04-11 15:38:02,330 INFO org.eclipse.jetty.server.Server: jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2023-04-11 15:38:02,367 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2023-04-11 15:38:02,378 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-11 15:38:02,380 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2023-04-11 15:38:02,380 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-11 15:38:02,383 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@61526469{/logs,file:///var/log/hadoop-yarn/,AVAILABLE}
2023-04-11 15:38:02,383 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c351808{/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.1.2.jar!/webapps/static,AVAILABLE}
2023-04-11 15:38:03,358 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@16c587de{/,file:///tmp/jetty-0.0.0.0-8088-cluster-_-any-6588752809207018259.dir/webapp/,AVAILABLE}{/cluster}
2023-04-11 15:38:03,395 INFO org.eclipse.jetty.webapp.StandardDescriptorProcessor: NO JSP Support for /ui2, did not find org.eclipse.jetty.jsp.JettyJspServlet
2023-04-11 15:38:03,402 INFO org.eclipse.jetty.server.handler.ContextHandler: Started o.e.j.w.WebAppContext@3b1ed14b{/ui2,file:///usr/lib/hadoop-yarn/webapps/ui2/,AVAILABLE}
2023-04-11 15:38:03,409 INFO org.eclipse.jetty.server.AbstractConnector: Started ServerConnector@13741d5a{HTTP/1.1,[http/1.1]}{0.0.0.0:8088}
2023-04-11 15:38:03,409 INFO org.eclipse.jetty.server.Server: Started @3634ms
2023-04-11 15:38:03,409 INFO org.apache.hadoop.yarn.webapp.WebApps: Web app cluster started at 8088
2023-04-11 15:38:03,586 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 100 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2023-04-11 15:38:03,597 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8033
2023-04-11 15:38:03,629 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server
2023-04-11 15:38:03,630 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2023-04-11 15:38:03,630 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8033: starting
2023-04-11 15:38:03,633 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning to active state
2023-04-11 15:38:03,721 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Using state database at /srv/hadoop-yarn/leveldb-state-store/yarn-rm-state for recovery
2023-04-11 15:38:03,778 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Recovery started
2023-04-11 15:38:03,783 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Loaded RM state version info 1.1
2023-04-11 15:38:03,793 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 10 RM delegation token master keys
2023-04-11 15:38:03,793 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 0 RM delegation tokens
2023-04-11 15:38:03,845 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 11 applications and 11 application attempts
2023-04-11 15:38:03,852 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Recovered 0 reservations
2023-04-11 15:38:03,852 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: recovering RMDelegationTokenSecretManager.
2023-04-11 15:38:03,856 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager: Recovering 11 applications
2023-04-11 15:38:03,922 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:38:03,958 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 2 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:38:03,959 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1671022279734_0001
2023-04-11 15:38:03,960 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:38:03,961 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1671022279734_0001,name=QuasiMonteCarlo,user=yarn,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1671022279734_0001/,appMasterHost=N/A,submitTime=1671022505181,startTime=1671022505266,finishTime=1671022524607,finalStatus=SUCCEEDED,memorySeconds=216125,vcoreSeconds=183,preemptedMemorySeconds=216125,preemptedVcoreSeconds=183,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=216125 MB-seconds\, 183 vcore-seconds,preemptedResourceSeconds=216125 MB-seconds\, 183 vcore-seconds
2023-04-11 15:38:03,962 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1671022279734_0002
2023-04-11 15:38:03,962 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 2 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:38:03,962 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1671022279734_0002,name=QuasiMonteCarlo,user=hdfs,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1671022279734_0002/,appMasterHost=N/A,submitTime=1671023676043,startTime=1671023676044,finishTime=1671023694897,finalStatus=SUCCEEDED,memorySeconds=228352,vcoreSeconds=193,preemptedMemorySeconds=228352,preemptedVcoreSeconds=193,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=228352 MB-seconds\, 193 vcore-seconds,preemptedResourceSeconds=228352 MB-seconds\, 193 vcore-seconds
2023-04-11 15:38:03,962 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=root	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0001
2023-04-11 15:38:03,962 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0001,name=word count,user=root,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0001/,appMasterHost=N/A,submitTime=1672378445838,startTime=1672378445838,finishTime=1672378452169,finalStatus=FAILED,memorySeconds=6121,vcoreSeconds=2,preemptedMemorySeconds=6121,preemptedVcoreSeconds=2,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=6121 MB-seconds\, 2 vcore-seconds,preemptedResourceSeconds=6121 MB-seconds\, 2 vcore-seconds
2023-04-11 15:38:03,969 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 5 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:38:03,969 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0002
2023-04-11 15:38:03,970 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0002,name=word count,user=student48,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0002/,appMasterHost=N/A,submitTime=1672378503807,startTime=1672378503809,finishTime=1672378519495,finalStatus=SUCCEEDED,memorySeconds=37951,vcoreSeconds=21,preemptedMemorySeconds=37951,preemptedVcoreSeconds=21,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=37951 MB-seconds\, 21 vcore-seconds,preemptedResourceSeconds=37951 MB-seconds\, 21 vcore-seconds
2023-04-11 15:38:03,971 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0005
2023-04-11 15:38:03,972 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 6 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:38:03,972 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0005,name=word count,user=student48,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0005/,appMasterHost=N/A,submitTime=1674569701969,startTime=1674569701970,finishTime=1674569712186,finalStatus=FAILED,memorySeconds=18149,vcoreSeconds=8,preemptedMemorySeconds=18149,preemptedVcoreSeconds=8,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=18149 MB-seconds\, 8 vcore-seconds,preemptedResourceSeconds=18149 MB-seconds\, 8 vcore-seconds
2023-04-11 15:38:03,974 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 7 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:38:03,976 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 8 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:38:03,978 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 9 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:38:03,980 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 11 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:38:03,981 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 13 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 15:38:03,983 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager: Successfully recovered 11 out of 11 applications
2023-04-11 15:38:03,983 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Recovery ended
2023-04-11 15:38:03,983 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-11 15:38:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0006
2023-04-11 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-11 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0006,name=word count,user=student48,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0006/,appMasterHost=N/A,submitTime=1674569761159,startTime=1674569761159,finishTime=1674569778485,finalStatus=SUCCEEDED,memorySeconds=45282,vcoreSeconds=25,preemptedMemorySeconds=45282,preemptedVcoreSeconds=25,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=45282 MB-seconds\, 25 vcore-seconds,preemptedResourceSeconds=45282 MB-seconds\, 25 vcore-seconds
2023-04-11 15:38:03,985 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-11 15:38:03,986 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0007
2023-04-11 15:38:03,986 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 113
2023-04-11 15:38:03,986 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-11 15:38:03,986 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0007,name=word count,user=student48,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0007/,appMasterHost=N/A,submitTime=1674569840580,startTime=1674569840580,finishTime=1674569854534,finalStatus=SUCCEEDED,memorySeconds=36318,vcoreSeconds=20,preemptedMemorySeconds=36318,preemptedVcoreSeconds=20,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=36318 MB-seconds\, 20 vcore-seconds,preemptedResourceSeconds=36318 MB-seconds\, 20 vcore-seconds
2023-04-11 15:38:03,986 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0008
2023-04-11 15:38:03,986 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0008,name=authors.jar,user=student48,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0008/,appMasterHost=N/A,submitTime=1676746201437,startTime=1676746201438,finishTime=1676746206409,finalStatus=FAILED,memorySeconds=6057,vcoreSeconds=2,preemptedMemorySeconds=6057,preemptedVcoreSeconds=2,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=6057 MB-seconds\, 2 vcore-seconds,preemptedResourceSeconds=6057 MB-seconds\, 2 vcore-seconds
2023-04-11 15:38:03,986 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=student48	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0009
2023-04-11 15:38:03,987 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0009,name=authors.jar,user=student48,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0009/,appMasterHost=N/A,submitTime=1676746394492,startTime=1676746394493,finishTime=1676746417277,finalStatus=SUCCEEDED,memorySeconds=92284,vcoreSeconds=60,preemptedMemorySeconds=92284,preemptedVcoreSeconds=60,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=92284 MB-seconds\, 60 vcore-seconds,preemptedResourceSeconds=92284 MB-seconds\, 60 vcore-seconds
2023-04-11 15:38:03,987 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=spark	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0011
2023-04-11 15:38:03,987 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0011,name=Spark shell,user=spark,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0011/,appMasterHost=N/A,submitTime=1681215416141,startTime=1681215416142,finishTime=1681215437454,finalStatus=SUCCEEDED,memorySeconds=87266,vcoreSeconds=51,preemptedMemorySeconds=87266,preemptedVcoreSeconds=51,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=SPARK,resourceSeconds=87266 MB-seconds\, 51 vcore-seconds,preemptedResourceSeconds=87266 MB-seconds\, 51 vcore-seconds
2023-04-11 15:38:03,987 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=root	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1672231888656_0013
2023-04-11 15:38:03,987 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2023-04-11 15:38:03,987 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-11 15:38:03,987 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 114
2023-04-11 15:38:03,987 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1672231888656_0013,name=Spark shell,user=root,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1672231888656_0013/,appMasterHost=N/A,submitTime=1681225335892,startTime=1681225335893,finishTime=1681225340303,finalStatus=SUCCEEDED,memorySeconds=2948,vcoreSeconds=2,preemptedMemorySeconds=2948,preemptedVcoreSeconds=2,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=SPARK,resourceSeconds=2948 MB-seconds\, 2 vcore-seconds,preemptedResourceSeconds=2948 MB-seconds\, 2 vcore-seconds
2023-04-11 15:38:03,987 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-11 15:38:04,660 INFO org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager: No Modified Node label Mapping to replace
2023-04-11 15:38:05,264 INFO org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore: Finished write mirror at:hdfs:/system/yarn/node-labels/nodelabel.mirror
2023-04-11 15:38:05,264 INFO org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore: Finished create editlog file at:hdfs:/system/yarn/node-labels/nodelabel.editlog
2023-04-11 15:38:05,266 INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler
2023-04-11 15:38:05,280 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2023-04-11 15:38:05,281 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8031
2023-04-11 15:38:05,283 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server
2023-04-11 15:38:05,283 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2023-04-11 15:38:05,283 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8031: starting
2023-04-11 15:38:05,336 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2023-04-11 15:38:05,353 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2023-04-11 15:38:05,360 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8030
2023-04-11 15:38:05,366 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server
2023-04-11 15:38:05,367 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2023-04-11 15:38:05,388 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8030: starting
2023-04-11 15:38:05,479 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2023-04-11 15:38:05,479 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 8032
2023-04-11 15:38:05,482 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server
2023-04-11 15:38:05,482 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2023-04-11 15:38:05,483 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8032: starting
2023-04-11 15:38:05,492 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioned to active state
2023-04-11 15:38:41,707 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node adh-test-00-mnode2.ru-central1.internal(cmPort: 8041 httpPort: 8042) registered with capability: <memory:7057, vCores:2>, assigned nodeId adh-test-00-mnode2.ru-central1.internal:8041
2023-04-11 15:38:41,714 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: adh-test-00-mnode2.ru-central1.internal:8041 Node Transitioned from NEW to RUNNING
2023-04-11 15:38:41,764 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node adh-test-00-mnode2.ru-central1.internal:8041 clusterResource: <memory:7057, vCores:2>
2023-04-11 15:38:41,854 INFO org.apache.hadoop.yarn.server.resourcemanager.RMActiveServiceContext: Scheduler recovery is done. Start allocating new containers.
2023-04-11 15:38:42,082 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node adh-test-00-mnode1.ru-central1.internal(cmPort: 8041 httpPort: 8042) registered with capability: <memory:7057, vCores:2>, assigned nodeId adh-test-00-mnode1.ru-central1.internal:8041
2023-04-11 15:38:42,082 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: adh-test-00-mnode1.ru-central1.internal:8041 Node Transitioned from NEW to RUNNING
2023-04-11 15:38:42,085 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node adh-test-00-mnode1.ru-central1.internal:8041 clusterResource: <memory:14114, vCores:4>
2023-04-11 15:38:42,669 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node adh-test-00-mnode3.ru-central1.internal(cmPort: 8041 httpPort: 8042) registered with capability: <memory:7057, vCores:2>, assigned nodeId adh-test-00-mnode3.ru-central1.internal:8041
2023-04-11 15:38:42,669 INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: adh-test-00-mnode3.ru-central1.internal:8041 Node Transitioned from NEW to RUNNING
2023-04-11 15:38:42,671 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node adh-test-00-mnode3.ru-central1.internal:8041 clusterResource: <memory:21171, vCores:6>
2023-04-11 15:48:01,128 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Release request cache is cleaned up
2023-04-11 15:52:20,171 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 1
2023-04-11 16:17:17,784 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 2
2023-04-11 16:20:33,520 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 3
2023-04-11 16:20:34,192 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1681227483635_0003' is submitted without priority hence considering default queue/cluster priority: 0
2023-04-11 16:20:34,192 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1681227483635_0003
2023-04-11 16:20:34,193 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 3 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 16:20:34,193 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 3 submitted by user hdfs
2023-04-11 16:20:34,193 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.4	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1681227483635_0003	QUEUENAME=default
2023-04-11 16:20:34,194 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1681227483635_0003
2023-04-11 16:20:34,198 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0003 State change from NEW to NEW_SAVING on event = START
2023-04-11 16:20:34,198 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1681227483635_0003
2023-04-11 16:20:34,225 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0003 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2023-04-11 16:20:34,227 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1681227483635_0003 user: hdfs leaf-queue of parent: root #applications: 1
2023-04-11 16:20:34,228 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1681227483635_0003 from user: hdfs, in queue: default
2023-04-11 16:20:34,229 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0003 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2023-04-11 16:20:34,265 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1681227483635_0003_000001
2023-04-11 16:20:34,266 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0003_000001 State change from NEW to SUBMITTED on event = START
2023-04-11 16:20:34,283 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1681227483635_0003 from user: hdfs activated in queue: default
2023-04-11 16:20:34,283 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1681227483635_0003 user: hdfs, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-11 16:20:34,283 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1681227483635_0003_000001 to scheduler from user hdfs in queue default
2023-04-11 16:20:34,293 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0003_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-11 16:20:34,383 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0003_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-11 16:20:34,387 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0003_01_000001 Container Transitioned from NEW to ALLOCATED
2023-04-11 16:20:34,388 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0003_01_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-04-11 16:20:34,388 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0003	CONTAINERID=container_e04_1681227483635_0003_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-11 16:20:34,411 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e04_1681227483635_0003_01_000001
2023-04-11 16:20:34,422 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0003_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-11 16:20:34,423 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1681227483635_0003_000001
2023-04-11 16:20:34,423 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1681227483635_0003 AttemptId: appattempt_1681227483635_0003_000001 MasterContainer: Container: [ContainerId: container_e04_1681227483635_0003_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ]
2023-04-11 16:20:34,424 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.04836805 absoluteUsedCapacity=0.04836805 used=<memory:1024, vCores:1> cluster=<memory:21171, vCores:6>
2023-04-11 16:20:34,424 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-11 16:20:34,427 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0003_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-11 16:20:34,432 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0003_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-11 16:20:34,436 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1681227483635_0003_000001
2023-04-11 16:20:34,506 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e04_1681227483635_0003_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0003_000001
2023-04-11 16:20:34,507 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1681227483635_0003_000001
2023-04-11 16:20:34,510 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1681227483635_0003_000001
2023-04-11 16:20:34,741 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e04_1681227483635_0003_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0003_000001
2023-04-11 16:20:34,741 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0003_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-11 16:20:35,383 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0003_01_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-11 16:20:38,181 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0003_000001 (auth:SIMPLE)
2023-04-11 16:20:38,200 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1681227483635_0003_000001
2023-04-11 16:20:38,201 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.5	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1681227483635_0003	APPATTEMPTID=appattempt_1681227483635_0003_000001
2023-04-11 16:20:38,201 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0003_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-11 16:20:38,201 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0003 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-11 16:20:38,383 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0003_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-11 16:20:38,383 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0003_01_000002 Container Transitioned from NEW to ALLOCATED
2023-04-11 16:20:38,383 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0003_01_000002 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 2 containers, <memory:3072, vCores:2> used and <memory:3985, vCores:0> available after allocation
2023-04-11 16:20:38,383 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0003	CONTAINERID=container_e04_1681227483635_0003_01_000002	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-11 16:20:38,385 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.14510415 absoluteUsedCapacity=0.14510415 used=<memory:3072, vCores:2> cluster=<memory:21171, vCores:6>
2023-04-11 16:20:38,385 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-11 16:20:38,386 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0003_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-11 16:20:38,387 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0003_01_000003 Container Transitioned from NEW to ALLOCATED
2023-04-11 16:20:38,387 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0003_01_000003 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2023-04-11 16:20:38,387 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0003	CONTAINERID=container_e04_1681227483635_0003_01_000003	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-11 16:20:38,387 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.24184026 absoluteUsedCapacity=0.24184026 used=<memory:5120, vCores:3> cluster=<memory:21171, vCores:6>
2023-04-11 16:20:38,387 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-11 16:20:38,421 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e04_1681227483635_0003_01_000002
2023-04-11 16:20:38,422 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0003_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-11 16:20:38,423 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode1.ru-central1.internal:8041 for container : container_e04_1681227483635_0003_01_000003
2023-04-11 16:20:38,424 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0003_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-11 16:20:39,385 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0003_01_000002 Container Transitioned from ACQUIRED to RUNNING
2023-04-11 16:20:39,400 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0003_01_000003 Container Transitioned from ACQUIRED to RUNNING
2023-04-11 16:20:41,451 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1681227483635_0003
2023-04-11 16:30:06,200 WARN org.apache.hadoop.ipc.Server: Incorrect header or version mismatch from 192.168.126.6:38184 got version 0 expected version 9
2023-04-11 16:30:35,560 WARN org.apache.hadoop.ipc.Server: Incorrect header or version mismatch from 192.168.126.6:49908 got version 0 expected version 9
2023-04-11 16:35:14,863 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1681227483635_0003_000001 with final state: FINISHING, and exit status: -1000
2023-04-11 16:35:14,863 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0003_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2023-04-11 16:35:14,863 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1681227483635_0003 with final state: FINISHING
2023-04-11 16:35:14,864 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0003 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2023-04-11 16:35:14,864 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0003_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2023-04-11 16:35:14,864 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1681227483635_0003
2023-04-11 16:35:14,867 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0003 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2023-04-11 16:35:14,919 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0003_01_000002 Container Transitioned from RUNNING to COMPLETED
2023-04-11 16:35:14,919 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0003	CONTAINERID=container_e04_1681227483635_0003_01_000002	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-11 16:35:14,966 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1681227483635_0003 unregistered successfully. 
2023-04-11 16:35:15,274 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0003_01_000003 Container Transitioned from RUNNING to COMPLETED
2023-04-11 16:35:15,274 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0003	CONTAINERID=container_e04_1681227483635_0003_01_000003	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-11 16:35:15,341 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0003_01_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-11 16:35:15,341 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1681227483635_0003_000001
2023-04-11 16:35:15,341 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0003	CONTAINERID=container_e04_1681227483635_0003_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-11 16:35:15,342 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1681227483635_0003_000001
2023-04-11 16:35:15,488 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0003_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2023-04-11 16:35:15,490 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0003 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2023-04-11 16:35:15,490 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1681227483635_0003_000001 is done. finalState=FINISHED
2023-04-11 16:35:15,490 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1681227483635_0003
2023-04-11 16:35:15,490 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1681227483635_0003 requests cleared
2023-04-11 16:35:15,491 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1681227483635_0003 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-04-11 16:35:15,491 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1681227483635_0003 user: hdfs leaf-queue of parent: root #applications: 0
2023-04-11 16:35:15,491 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1681227483635_0003,name=Spark shell,user=hdfs,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1681227483635_0003/,appMasterHost=10.130.0.5,submitTime=1681230034192,startTime=1681230034193,finishTime=1681230914863,finalStatus=SUCCEEDED,memorySeconds=4493108,vcoreSeconds=2632,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=SPARK,resourceSeconds=4493108 MB-seconds\, 2632 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2023-04-11 16:35:15,491 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1681227483635_0003_000001
2023-04-11 16:38:03,778 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 16:38:03,793 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-11 17:38:03,778 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 17:38:03,784 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-11 18:11:16,425 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 4
2023-04-11 18:11:17,103 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1681227483635_0004' is submitted without priority hence considering default queue/cluster priority: 0
2023-04-11 18:11:17,104 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1681227483635_0004
2023-04-11 18:11:17,104 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 4 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 18:11:17,104 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 4 submitted by user hdfs
2023-04-11 18:11:17,104 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.4	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1681227483635_0004	QUEUENAME=default
2023-04-11 18:11:17,105 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1681227483635_0004
2023-04-11 18:11:17,105 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0004 State change from NEW to NEW_SAVING on event = START
2023-04-11 18:11:17,105 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1681227483635_0004
2023-04-11 18:11:17,106 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0004 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2023-04-11 18:11:17,106 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1681227483635_0004 user: hdfs leaf-queue of parent: root #applications: 1
2023-04-11 18:11:17,106 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1681227483635_0004 from user: hdfs, in queue: default
2023-04-11 18:11:17,107 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0004 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2023-04-11 18:11:17,107 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1681227483635_0004_000001
2023-04-11 18:11:17,107 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0004_000001 State change from NEW to SUBMITTED on event = START
2023-04-11 18:11:17,107 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1681227483635_0004 from user: hdfs activated in queue: default
2023-04-11 18:11:17,107 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1681227483635_0004 user: hdfs, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-11 18:11:17,107 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1681227483635_0004_000001 to scheduler from user hdfs in queue default
2023-04-11 18:11:17,108 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0004_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-11 18:11:17,194 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0004_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-11 18:11:17,195 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0004_01_000001 Container Transitioned from NEW to ALLOCATED
2023-04-11 18:11:17,195 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0004_01_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-04-11 18:11:17,195 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0004	CONTAINERID=container_e04_1681227483635_0004_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-11 18:11:17,198 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_e04_1681227483635_0004_01_000001
2023-04-11 18:11:17,199 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0004_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-11 18:11:17,199 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1681227483635_0004_000001
2023-04-11 18:11:17,199 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1681227483635_0004 AttemptId: appattempt_1681227483635_0004_000001 MasterContainer: Container: [ContainerId: container_e04_1681227483635_0004_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ]
2023-04-11 18:11:17,199 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.04836805 absoluteUsedCapacity=0.04836805 used=<memory:1024, vCores:1> cluster=<memory:21171, vCores:6>
2023-04-11 18:11:17,199 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-11 18:11:17,199 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0004_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-11 18:11:17,200 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0004_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-11 18:11:17,201 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1681227483635_0004_000001
2023-04-11 18:11:17,203 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e04_1681227483635_0004_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0004_000001
2023-04-11 18:11:17,203 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1681227483635_0004_000001
2023-04-11 18:11:17,203 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1681227483635_0004_000001
2023-04-11 18:11:17,393 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e04_1681227483635_0004_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0004_000001
2023-04-11 18:11:17,394 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0004_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-11 18:11:18,206 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0004_01_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-11 18:11:22,302 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0004_000001 (auth:SIMPLE)
2023-04-11 18:11:22,305 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1681227483635_0004_000001
2023-04-11 18:11:22,305 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.6	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1681227483635_0004	APPATTEMPTID=appattempt_1681227483635_0004_000001
2023-04-11 18:11:22,305 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0004_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-11 18:11:22,306 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0004 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-11 18:11:22,481 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0004_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-11 18:11:22,481 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0004_01_000002 Container Transitioned from NEW to ALLOCATED
2023-04-11 18:11:22,482 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0004_01_000002 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2023-04-11 18:11:22,482 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0004	CONTAINERID=container_e04_1681227483635_0004_01_000002	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-11 18:11:22,482 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.14510415 absoluteUsedCapacity=0.14510415 used=<memory:3072, vCores:2> cluster=<memory:21171, vCores:6>
2023-04-11 18:11:22,482 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-11 18:11:22,593 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0004_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-11 18:11:22,594 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0004_01_000003 Container Transitioned from NEW to ALLOCATED
2023-04-11 18:11:22,594 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0004_01_000003 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2023-04-11 18:11:22,594 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0004	CONTAINERID=container_e04_1681227483635_0004_01_000003	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-11 18:11:22,594 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.24184026 absoluteUsedCapacity=0.24184026 used=<memory:5120, vCores:3> cluster=<memory:21171, vCores:6>
2023-04-11 18:11:22,594 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-11 18:11:22,686 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e04_1681227483635_0004_01_000002
2023-04-11 18:11:22,689 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0004_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-11 18:11:22,690 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode1.ru-central1.internal:8041 for container : container_e04_1681227483635_0004_01_000003
2023-04-11 18:11:22,691 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0004_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-11 18:11:23,509 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0004_01_000002 Container Transitioned from ACQUIRED to RUNNING
2023-04-11 18:11:23,626 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0004_01_000003 Container Transitioned from ACQUIRED to RUNNING
2023-04-11 18:11:25,714 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1681227483635_0004
2023-04-11 18:14:52,697 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0004_01_000002 Container Transitioned from RUNNING to COMPLETED
2023-04-11 18:14:52,698 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0004	CONTAINERID=container_e04_1681227483635_0004_01_000002	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-11 18:14:52,715 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0004_01_000003 Container Transitioned from RUNNING to COMPLETED
2023-04-11 18:14:52,715 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0004	CONTAINERID=container_e04_1681227483635_0004_01_000003	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-11 18:14:53,096 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1681227483635_0004_000001 with final state: FINISHING, and exit status: -1000
2023-04-11 18:14:53,096 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0004_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2023-04-11 18:14:53,096 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1681227483635_0004 with final state: FINISHING
2023-04-11 18:14:53,096 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0004 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2023-04-11 18:14:53,096 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1681227483635_0004
2023-04-11 18:14:53,096 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0004_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2023-04-11 18:14:53,097 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0004 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2023-04-11 18:14:53,198 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1681227483635_0004 unregistered successfully. 
2023-04-11 18:14:53,589 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1681227483635_0004_000001
2023-04-11 18:14:53,589 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0004_01_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-11 18:14:53,589 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1681227483635_0004_000001
2023-04-11 18:14:53,589 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0004	CONTAINERID=container_e04_1681227483635_0004_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-11 18:14:53,589 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0004_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2023-04-11 18:14:53,590 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0004 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2023-04-11 18:14:53,590 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1681227483635_0004_000001 is done. finalState=FINISHED
2023-04-11 18:14:53,590 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1681227483635_0004 requests cleared
2023-04-11 18:14:53,590 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1681227483635_0004 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-04-11 18:14:53,590 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1681227483635_0004
2023-04-11 18:14:53,590 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1681227483635_0004 user: hdfs leaf-queue of parent: root #applications: 0
2023-04-11 18:14:53,591 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1681227483635_0004,name=Spark shell,user=hdfs,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1681227483635_0004/,appMasterHost=10.130.0.6,submitTime=1681236677103,startTime=1681236677104,finishTime=1681236893096,finalStatus=SUCCEEDED,memorySeconds=1082439,vcoreSeconds=636,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=SPARK,resourceSeconds=1082439 MB-seconds\, 636 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2023-04-11 18:14:53,591 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1681227483635_0004_000001
2023-04-11 18:15:30,911 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 5
2023-04-11 18:15:31,592 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1681227483635_0005' is submitted without priority hence considering default queue/cluster priority: 0
2023-04-11 18:15:31,592 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1681227483635_0005
2023-04-11 18:15:31,593 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 5 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-11 18:15:31,593 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 5 submitted by user hdfs
2023-04-11 18:15:31,593 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.4	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1681227483635_0005	QUEUENAME=default
2023-04-11 18:15:31,593 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1681227483635_0005
2023-04-11 18:15:31,593 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1681227483635_0005
2023-04-11 18:15:31,593 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0005 State change from NEW to NEW_SAVING on event = START
2023-04-11 18:15:31,594 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0005 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2023-04-11 18:15:31,594 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1681227483635_0005 user: hdfs leaf-queue of parent: root #applications: 1
2023-04-11 18:15:31,594 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1681227483635_0005 from user: hdfs, in queue: default
2023-04-11 18:15:31,595 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0005 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2023-04-11 18:15:31,595 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1681227483635_0005_000001
2023-04-11 18:15:31,595 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0005_000001 State change from NEW to SUBMITTED on event = START
2023-04-11 18:15:31,595 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1681227483635_0005 from user: hdfs activated in queue: default
2023-04-11 18:15:31,595 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1681227483635_0005 user: hdfs, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-11 18:15:31,595 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1681227483635_0005_000001 to scheduler from user hdfs in queue default
2023-04-11 18:15:31,596 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0005_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-11 18:15:31,685 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0005_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-11 18:15:31,685 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0005_01_000001 Container Transitioned from NEW to ALLOCATED
2023-04-11 18:15:31,685 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0005_01_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-04-11 18:15:31,685 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0005	CONTAINERID=container_e04_1681227483635_0005_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-11 18:15:31,686 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_e04_1681227483635_0005_01_000001
2023-04-11 18:15:31,687 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0005_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-11 18:15:31,687 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1681227483635_0005_000001
2023-04-11 18:15:31,687 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1681227483635_0005 AttemptId: appattempt_1681227483635_0005_000001 MasterContainer: Container: [ContainerId: container_e04_1681227483635_0005_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ]
2023-04-11 18:15:31,687 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.04836805 absoluteUsedCapacity=0.04836805 used=<memory:1024, vCores:1> cluster=<memory:21171, vCores:6>
2023-04-11 18:15:31,688 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-11 18:15:31,688 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0005_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-11 18:15:31,688 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0005_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-11 18:15:31,689 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1681227483635_0005_000001
2023-04-11 18:15:31,690 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e04_1681227483635_0005_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0005_000001
2023-04-11 18:15:31,690 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1681227483635_0005_000001
2023-04-11 18:15:31,690 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1681227483635_0005_000001
2023-04-11 18:15:31,706 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e04_1681227483635_0005_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0005_000001
2023-04-11 18:15:31,707 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0005_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-11 18:15:32,688 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0005_01_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-11 18:15:34,704 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0005_000001 (auth:SIMPLE)
2023-04-11 18:15:34,706 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1681227483635_0005_000001
2023-04-11 18:15:34,707 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.6	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1681227483635_0005	APPATTEMPTID=appattempt_1681227483635_0005_000001
2023-04-11 18:15:34,707 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0005_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-11 18:15:34,707 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0005 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-11 18:15:35,703 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0005_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-11 18:15:35,704 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0005_01_000002 Container Transitioned from NEW to ALLOCATED
2023-04-11 18:15:35,704 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0005_01_000002 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 2 containers, <memory:3072, vCores:2> used and <memory:3985, vCores:0> available after allocation
2023-04-11 18:15:35,704 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0005	CONTAINERID=container_e04_1681227483635_0005_01_000002	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-11 18:15:35,704 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.14510415 absoluteUsedCapacity=0.14510415 used=<memory:3072, vCores:2> cluster=<memory:21171, vCores:6>
2023-04-11 18:15:35,704 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-11 18:15:35,769 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0005_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-11 18:15:35,770 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0005_01_000003 Container Transitioned from NEW to ALLOCATED
2023-04-11 18:15:35,770 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0005_01_000003 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2023-04-11 18:15:35,770 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0005	CONTAINERID=container_e04_1681227483635_0005_01_000003	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-11 18:15:35,770 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.24184026 absoluteUsedCapacity=0.24184026 used=<memory:5120, vCores:3> cluster=<memory:21171, vCores:6>
2023-04-11 18:15:35,770 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-11 18:15:36,316 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_e04_1681227483635_0005_01_000002
2023-04-11 18:15:36,317 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0005_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-11 18:15:36,318 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e04_1681227483635_0005_01_000003
2023-04-11 18:15:36,319 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0005_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-11 18:15:37,716 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0005_01_000002 Container Transitioned from ACQUIRED to RUNNING
2023-04-11 18:15:37,782 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0005_01_000003 Container Transitioned from ACQUIRED to RUNNING
2023-04-11 18:15:39,346 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1681227483635_0005
2023-04-11 18:15:40,821 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0005_01_000003 Container Transitioned from RUNNING to COMPLETED
2023-04-11 18:15:40,821 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0005	CONTAINERID=container_e04_1681227483635_0005_01_000003	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-11 18:15:40,831 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1681227483635_0005_000001 with final state: FINISHING, and exit status: -1000
2023-04-11 18:15:40,831 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0005_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2023-04-11 18:15:40,831 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1681227483635_0005 with final state: FINISHING
2023-04-11 18:15:40,831 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0005 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2023-04-11 18:15:40,831 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1681227483635_0005
2023-04-11 18:15:40,831 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0005_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2023-04-11 18:15:40,831 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0005 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2023-04-11 18:15:41,064 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1681227483635_0005 unregistered successfully. 
2023-04-11 18:15:41,439 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1681227483635_0005_000001
2023-04-11 18:15:41,439 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0005_01_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-11 18:15:41,439 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1681227483635_0005_000001
2023-04-11 18:15:41,439 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0005	CONTAINERID=container_e04_1681227483635_0005_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-11 18:15:41,440 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0005_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2023-04-11 18:15:41,440 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0005 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2023-04-11 18:15:41,440 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1681227483635_0005_000001 is done. finalState=FINISHED
2023-04-11 18:15:41,441 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1681227483635_0005
2023-04-11 18:15:41,441 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1681227483635_0005,name=Spark shell,user=hdfs,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1681227483635_0005/,appMasterHost=10.130.0.6,submitTime=1681236931592,startTime=1681236931593,finishTime=1681236940831,finalStatus=SUCCEEDED,memorySeconds=32083,vcoreSeconds=19,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=SPARK,resourceSeconds=32083 MB-seconds\, 19 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2023-04-11 18:15:41,441 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1681227483635_0005_000001
2023-04-11 18:15:41,446 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0005_01_000002 Container Transitioned from RUNNING to KILLED
2023-04-11 18:15:41,446 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0005	CONTAINERID=container_e04_1681227483635_0005_01_000002	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-11 18:15:41,447 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1681227483635_0005 requests cleared
2023-04-11 18:15:41,447 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1681227483635_0005 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-04-11 18:15:41,447 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1681227483635_0005 user: hdfs leaf-queue of parent: root #applications: 0
2023-04-11 18:15:41,464 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Container container_e04_1681227483635_0005_01_000002 completed with event FINISHED, but corresponding RMContainer doesn't exist.
2023-04-11 18:15:42,468 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Container container_e04_1681227483635_0005_01_000002 completed with event FINISHED, but corresponding RMContainer doesn't exist.
2023-04-11 18:38:03,778 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 18:38:03,787 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-11 19:38:03,779 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 19:38:03,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-11 19:43:14,820 WARN org.apache.hadoop.ipc.Server: Incorrect header or version mismatch from 192.168.126.6:36562 got version 0 expected version 9
2023-04-11 19:43:34,824 WARN org.apache.hadoop.ipc.Server: Incorrect header or version mismatch from 192.168.126.6:49944 got version 0 expected version 9
2023-04-11 19:43:54,824 WARN org.apache.hadoop.ipc.Server: Incorrect header or version mismatch from 192.168.126.6:45288 got version 0 expected version 9
2023-04-11 19:44:14,908 WARN org.apache.hadoop.ipc.Server: Incorrect header or version mismatch from 192.168.126.6:46696 got version 0 expected version 9
2023-04-11 19:44:34,912 WARN org.apache.hadoop.ipc.Server: Incorrect header or version mismatch from 192.168.126.6:35282 got version 0 expected version 9
2023-04-11 19:44:54,912 WARN org.apache.hadoop.ipc.Server: Incorrect header or version mismatch from 192.168.126.6:60364 got version 0 expected version 9
2023-04-11 19:45:14,964 WARN org.apache.hadoop.ipc.Server: Incorrect header or version mismatch from 192.168.126.6:40534 got version 0 expected version 9
2023-04-11 19:45:34,964 WARN org.apache.hadoop.ipc.Server: Incorrect header or version mismatch from 192.168.126.6:45142 got version 0 expected version 9
2023-04-11 19:45:54,968 WARN org.apache.hadoop.ipc.Server: Incorrect header or version mismatch from 192.168.126.6:37774 got version 0 expected version 9
2023-04-11 19:46:15,012 WARN org.apache.hadoop.ipc.Server: Incorrect header or version mismatch from 192.168.126.6:58960 got version 0 expected version 9
2023-04-11 19:46:35,012 WARN org.apache.hadoop.ipc.Server: Incorrect header or version mismatch from 192.168.126.6:47162 got version 0 expected version 9
2023-04-11 19:46:55,020 WARN org.apache.hadoop.ipc.Server: Incorrect header or version mismatch from 192.168.126.6:60540 got version 0 expected version 9
2023-04-11 19:50:15,379 WARN org.eclipse.jetty.http.HttpParser: Illegal character 0x0 in state=START for buffer HeapByteBuffer@3bf00694[p=1,l=226,c=8192,r=225]={\x00<<<\x00\x00\x00\x00\x00\x00\xE2\x03u\xD6\xD8\xD0v\xFe\x0bg\x00...ing;xpt\x00\x06Master>>>\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00...\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00}
2023-04-11 19:50:15,380 WARN org.eclipse.jetty.http.HttpParser: bad HTTP parsed: 400 Illegal character 0x0 for HttpChannelOverHttp@4f46e4ed{r=0,c=false,a=IDLE,uri=null}
2023-04-11 19:50:35,360 WARN org.eclipse.jetty.http.HttpParser: Illegal character 0x0 in state=START for buffer HeapByteBuffer@3bf00694[p=1,l=226,c=8192,r=225]={\x00<<<\x00\x00\x00\x00\x00\x00\xE2\x03`\xF8\xF9e\x15\x03\x1f\xFa\x00...ing;xpt\x00\x06Master>>>\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00...\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00}
2023-04-11 19:50:35,360 WARN org.eclipse.jetty.http.HttpParser: bad HTTP parsed: 400 Illegal character 0x0 for HttpChannelOverHttp@22d78b8e{r=0,c=false,a=IDLE,uri=null}
2023-04-11 19:50:55,360 WARN org.eclipse.jetty.http.HttpParser: Illegal character 0x0 in state=START for buffer HeapByteBuffer@3bf00694[p=1,l=226,c=8192,r=225]={\x00<<<\x00\x00\x00\x00\x00\x00\xE2\x03~0\xFc\xE1\xE4\xC4\xF9"\x00...ing;xpt\x00\x06Master>>>\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00...\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00}
2023-04-11 19:50:55,361 WARN org.eclipse.jetty.http.HttpParser: bad HTTP parsed: 400 Illegal character 0x0 for HttpChannelOverHttp@1cccd70b{r=0,c=false,a=IDLE,uri=null}
2023-04-11 20:38:03,779 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 20:38:03,784 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-11 21:38:03,779 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 21:38:03,790 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-11 22:38:03,779 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 22:38:03,789 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-11 23:38:03,779 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-11 23:38:03,794 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-12 00:38:03,780 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 00:38:03,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 31 msec
2023-04-12 01:38:03,780 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 01:38:03,786 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-12 02:38:03,780 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 02:38:03,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-04-12 03:38:03,781 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 03:38:03,794 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-04-12 04:38:03,782 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 04:38:03,791 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-12 05:31:34,833 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 6
2023-04-12 05:31:35,491 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1681227483635_0006' is submitted without priority hence considering default queue/cluster priority: 0
2023-04-12 05:31:35,491 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1681227483635_0006
2023-04-12 05:31:35,491 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 6 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-12 05:31:35,492 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 6 submitted by user hdfs
2023-04-12 05:31:35,492 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.4	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1681227483635_0006	QUEUENAME=default
2023-04-12 05:31:35,493 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1681227483635_0006
2023-04-12 05:31:35,494 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0006 State change from NEW to NEW_SAVING on event = START
2023-04-12 05:31:35,494 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1681227483635_0006
2023-04-12 05:31:35,494 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0006 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2023-04-12 05:31:35,495 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1681227483635_0006 user: hdfs leaf-queue of parent: root #applications: 1
2023-04-12 05:31:35,495 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1681227483635_0006 from user: hdfs, in queue: default
2023-04-12 05:31:35,495 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0006 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2023-04-12 05:31:35,495 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1681227483635_0006_000001
2023-04-12 05:31:35,495 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0006_000001 State change from NEW to SUBMITTED on event = START
2023-04-12 05:31:35,496 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1681227483635_0006 from user: hdfs activated in queue: default
2023-04-12 05:31:35,496 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1681227483635_0006 user: hdfs, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-12 05:31:35,496 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1681227483635_0006_000001 to scheduler from user hdfs in queue default
2023-04-12 05:31:35,497 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0006_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-12 05:31:35,762 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0006_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-12 05:31:35,763 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0006_01_000001 Container Transitioned from NEW to ALLOCATED
2023-04-12 05:31:35,763 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0006_01_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-04-12 05:31:35,763 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0006	CONTAINERID=container_e04_1681227483635_0006_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-12 05:31:35,764 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e04_1681227483635_0006_01_000001
2023-04-12 05:31:35,765 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0006_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-12 05:31:35,765 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1681227483635_0006_000001
2023-04-12 05:31:35,765 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1681227483635_0006 AttemptId: appattempt_1681227483635_0006_000001 MasterContainer: Container: [ContainerId: container_e04_1681227483635_0006_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ]
2023-04-12 05:31:35,766 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0006_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-12 05:31:35,766 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.04836805 absoluteUsedCapacity=0.04836805 used=<memory:1024, vCores:1> cluster=<memory:21171, vCores:6>
2023-04-12 05:31:35,767 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-12 05:31:35,767 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0006_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-12 05:31:35,768 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1681227483635_0006_000001
2023-04-12 05:31:35,769 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e04_1681227483635_0006_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0006_000001
2023-04-12 05:31:35,769 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1681227483635_0006_000001
2023-04-12 05:31:35,769 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1681227483635_0006_000001
2023-04-12 05:31:35,794 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e04_1681227483635_0006_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0006_000001
2023-04-12 05:31:35,795 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0006_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-12 05:31:36,765 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0006_01_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-12 05:31:38,521 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0006_000001 (auth:SIMPLE)
2023-04-12 05:31:38,524 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1681227483635_0006_000001
2023-04-12 05:31:38,525 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.5	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1681227483635_0006	APPATTEMPTID=appattempt_1681227483635_0006_000001
2023-04-12 05:31:38,525 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0006_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-12 05:31:38,525 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0006 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-12 05:31:38,786 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0006_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-12 05:31:38,787 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0006_01_000002 Container Transitioned from NEW to ALLOCATED
2023-04-12 05:31:38,787 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0006_01_000002 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 2 containers, <memory:3072, vCores:2> used and <memory:3985, vCores:0> available after allocation
2023-04-12 05:31:38,787 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0006	CONTAINERID=container_e04_1681227483635_0006_01_000002	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 05:31:38,787 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.14510415 absoluteUsedCapacity=0.14510415 used=<memory:3072, vCores:2> cluster=<memory:21171, vCores:6>
2023-04-12 05:31:38,787 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-12 05:31:38,918 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e04_1681227483635_0006_01_000002
2023-04-12 05:31:38,922 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0006_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-12 05:31:39,152 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0006_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-12 05:31:39,152 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0006_01_000003 Container Transitioned from NEW to ALLOCATED
2023-04-12 05:31:39,152 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0006_01_000003 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2023-04-12 05:31:39,152 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0006	CONTAINERID=container_e04_1681227483635_0006_01_000003	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 05:31:39,153 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.24184026 absoluteUsedCapacity=0.24184026 used=<memory:5120, vCores:3> cluster=<memory:21171, vCores:6>
2023-04-12 05:31:39,153 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-12 05:31:39,355 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_e04_1681227483635_0006_01_000003
2023-04-12 05:31:39,356 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0006_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-12 05:31:39,774 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0006_01_000002 Container Transitioned from ACQUIRED to RUNNING
2023-04-12 05:31:39,775 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0006_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-12 05:31:39,775 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0006_01_000004 Container Transitioned from NEW to ALLOCATED
2023-04-12 05:31:39,775 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0006_01_000004 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 3 containers, <memory:5120, vCores:3> used and <memory:1937, vCores:-1> available after allocation
2023-04-12 05:31:39,775 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0006	CONTAINERID=container_e04_1681227483635_0006_01_000004	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 05:31:39,776 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.33857635 absoluteUsedCapacity=0.33857635 used=<memory:7168, vCores:4> cluster=<memory:21171, vCores:6>
2023-04-12 05:31:39,776 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-12 05:31:40,155 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0006_01_000003 Container Transitioned from ACQUIRED to RUNNING
2023-04-12 05:31:42,389 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1681227483635_0006
2023-04-12 05:31:42,391 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0006_01_000004 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-12 05:31:45,408 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0006_01_000004 Container Transitioned from ACQUIRED to RELEASED
2023-04-12 05:31:45,408 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.5	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0006	CONTAINERID=container_e04_1681227483635_0006_01_000004	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 05:38:03,782 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 05:38:03,803 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-04-12 06:23:41,941 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1681227483635_0006_000001 with final state: FINISHING, and exit status: -1000
2023-04-12 06:23:41,941 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0006_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2023-04-12 06:23:41,941 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1681227483635_0006 with final state: FINISHING
2023-04-12 06:23:41,941 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0006 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2023-04-12 06:23:41,942 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1681227483635_0006
2023-04-12 06:23:41,942 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0006_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2023-04-12 06:23:41,942 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0006 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2023-04-12 06:23:41,971 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0006_01_000003 Container Transitioned from RUNNING to COMPLETED
2023-04-12 06:23:41,971 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0006	CONTAINERID=container_e04_1681227483635_0006_01_000003	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:23:41,979 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0006_01_000002 Container Transitioned from RUNNING to COMPLETED
2023-04-12 06:23:41,979 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0006	CONTAINERID=container_e04_1681227483635_0006_01_000002	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:23:42,050 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1681227483635_0006 unregistered successfully. 
2023-04-12 06:23:42,416 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1681227483635_0006_000001
2023-04-12 06:23:42,416 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0006_01_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-12 06:23:42,416 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1681227483635_0006_000001
2023-04-12 06:23:42,416 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0006	CONTAINERID=container_e04_1681227483635_0006_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-12 06:23:42,417 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0006_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2023-04-12 06:23:42,417 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0006 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2023-04-12 06:23:42,418 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1681227483635_0006_000001 is done. finalState=FINISHED
2023-04-12 06:23:42,418 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1681227483635_0006
2023-04-12 06:23:42,418 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1681227483635_0006 requests cleared
2023-04-12 06:23:42,418 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1681227483635_0006 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-04-12 06:23:42,418 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1681227483635_0006 user: hdfs leaf-queue of parent: root #applications: 0
2023-04-12 06:23:42,418 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1681227483635_0006,name=Spark shell,user=hdfs,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1681227483635_0006/,appMasterHost=10.130.0.5,submitTime=1681277495491,startTime=1681277495491,finishTime=1681280621941,finalStatus=SUCCEEDED,memorySeconds=16005057,vcoreSeconds=9376,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=SPARK,resourceSeconds=16005057 MB-seconds\, 9376 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2023-04-12 06:23:42,418 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1681227483635_0006_000001
2023-04-12 06:25:13,204 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 7
2023-04-12 06:25:13,817 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1681227483635_0007' is submitted without priority hence considering default queue/cluster priority: 0
2023-04-12 06:25:13,817 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1681227483635_0007
2023-04-12 06:25:13,817 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 7 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-12 06:25:13,817 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 7 submitted by user hdfs
2023-04-12 06:25:13,817 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.4	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1681227483635_0007	QUEUENAME=default
2023-04-12 06:25:13,818 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1681227483635_0007
2023-04-12 06:25:13,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1681227483635_0007
2023-04-12 06:25:13,818 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0007 State change from NEW to NEW_SAVING on event = START
2023-04-12 06:25:13,819 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0007 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2023-04-12 06:25:13,819 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1681227483635_0007 user: hdfs leaf-queue of parent: root #applications: 1
2023-04-12 06:25:13,819 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1681227483635_0007 from user: hdfs, in queue: default
2023-04-12 06:25:13,819 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0007 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2023-04-12 06:25:13,819 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1681227483635_0007_000001
2023-04-12 06:25:13,819 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0007_000001 State change from NEW to SUBMITTED on event = START
2023-04-12 06:25:13,820 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1681227483635_0007 from user: hdfs activated in queue: default
2023-04-12 06:25:13,820 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1681227483635_0007 user: hdfs, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-12 06:25:13,820 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1681227483635_0007_000001 to scheduler from user hdfs in queue default
2023-04-12 06:25:13,820 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0007_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-12 06:25:14,155 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0007_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-12 06:25:14,155 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0007_01_000001 Container Transitioned from NEW to ALLOCATED
2023-04-12 06:25:14,155 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0007_01_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-04-12 06:25:14,155 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0007	CONTAINERID=container_e04_1681227483635_0007_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-12 06:25:14,156 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_e04_1681227483635_0007_01_000001
2023-04-12 06:25:14,157 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0007_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-12 06:25:14,157 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1681227483635_0007_000001
2023-04-12 06:25:14,157 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1681227483635_0007 AttemptId: appattempt_1681227483635_0007_000001 MasterContainer: Container: [ContainerId: container_e04_1681227483635_0007_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ]
2023-04-12 06:25:14,157 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.04836805 absoluteUsedCapacity=0.04836805 used=<memory:1024, vCores:1> cluster=<memory:21171, vCores:6>
2023-04-12 06:25:14,157 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-12 06:25:14,157 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0007_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-12 06:25:14,158 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0007_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-12 06:25:14,158 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1681227483635_0007_000001
2023-04-12 06:25:14,159 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e04_1681227483635_0007_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0007_000001
2023-04-12 06:25:14,159 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1681227483635_0007_000001
2023-04-12 06:25:14,159 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1681227483635_0007_000001
2023-04-12 06:25:14,172 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e04_1681227483635_0007_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0007_000001
2023-04-12 06:25:14,172 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0007_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-12 06:25:15,156 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0007_01_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-12 06:25:17,102 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0007_000001 (auth:SIMPLE)
2023-04-12 06:25:17,105 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1681227483635_0007_000001
2023-04-12 06:25:17,105 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.6	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1681227483635_0007	APPATTEMPTID=appattempt_1681227483635_0007_000001
2023-04-12 06:25:17,105 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0007_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-12 06:25:17,105 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0007 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-12 06:25:17,503 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0007_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-12 06:25:17,504 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0007_01_000002 Container Transitioned from NEW to ALLOCATED
2023-04-12 06:25:17,504 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0007_01_000002 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2023-04-12 06:25:17,504 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0007	CONTAINERID=container_e04_1681227483635_0007_01_000002	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:25:17,504 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.14510415 absoluteUsedCapacity=0.14510415 used=<memory:3072, vCores:2> cluster=<memory:21171, vCores:6>
2023-04-12 06:25:17,504 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-12 06:25:17,512 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode1.ru-central1.internal:8041 for container : container_e04_1681227483635_0007_01_000002
2023-04-12 06:25:17,513 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0007_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-12 06:25:17,649 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0007_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-12 06:25:17,649 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0007_01_000003 Container Transitioned from NEW to ALLOCATED
2023-04-12 06:25:17,649 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0007_01_000003 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2023-04-12 06:25:17,649 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0007	CONTAINERID=container_e04_1681227483635_0007_01_000003	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:25:17,649 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.24184026 absoluteUsedCapacity=0.24184026 used=<memory:5120, vCores:3> cluster=<memory:21171, vCores:6>
2023-04-12 06:25:17,649 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-12 06:25:17,932 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e04_1681227483635_0007_01_000003
2023-04-12 06:25:17,933 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0007_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-12 06:25:18,187 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0007_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-12 06:25:18,188 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0007_01_000004 Container Transitioned from NEW to ALLOCATED
2023-04-12 06:25:18,188 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0007_01_000004 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 2 containers, <memory:3072, vCores:2> used and <memory:3985, vCores:0> available after allocation
2023-04-12 06:25:18,188 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0007	CONTAINERID=container_e04_1681227483635_0007_01_000004	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:25:18,188 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.33857635 absoluteUsedCapacity=0.33857635 used=<memory:7168, vCores:4> cluster=<memory:21171, vCores:6>
2023-04-12 06:25:18,188 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-12 06:25:18,523 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0007_01_000002 Container Transitioned from ACQUIRED to RUNNING
2023-04-12 06:25:18,656 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0007_01_000003 Container Transitioned from ACQUIRED to RUNNING
2023-04-12 06:25:20,956 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1681227483635_0007
2023-04-12 06:25:20,957 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_e04_1681227483635_0007_01_000004
2023-04-12 06:25:20,959 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0007_01_000004 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-12 06:25:23,974 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0007_01_000004 Container Transitioned from ACQUIRED to RELEASED
2023-04-12 06:25:23,975 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.6	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0007	CONTAINERID=container_e04_1681227483635_0007_01_000004	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:25:34,937 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0007_01_000003 Container Transitioned from RUNNING to COMPLETED
2023-04-12 06:25:34,937 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0007	CONTAINERID=container_e04_1681227483635_0007_01_000003	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:25:34,937 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1681227483635_0007_000001 with final state: FINISHING, and exit status: -1000
2023-04-12 06:25:34,937 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0007_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2023-04-12 06:25:34,938 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1681227483635_0007 with final state: FINISHING
2023-04-12 06:25:34,938 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0007 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2023-04-12 06:25:34,938 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1681227483635_0007
2023-04-12 06:25:34,938 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0007_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2023-04-12 06:25:34,938 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0007 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2023-04-12 06:25:34,945 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0007_01_000002 Container Transitioned from RUNNING to COMPLETED
2023-04-12 06:25:34,945 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0007	CONTAINERID=container_e04_1681227483635_0007_01_000002	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:25:35,042 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1681227483635_0007 unregistered successfully. 
2023-04-12 06:25:35,416 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1681227483635_0007_000001
2023-04-12 06:25:35,416 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0007_01_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-12 06:25:35,417 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1681227483635_0007_000001
2023-04-12 06:25:35,417 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0007	CONTAINERID=container_e04_1681227483635_0007_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-12 06:25:35,417 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0007_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2023-04-12 06:25:35,417 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0007 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2023-04-12 06:25:35,418 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1681227483635_0007_000001 is done. finalState=FINISHED
2023-04-12 06:25:35,418 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1681227483635_0007
2023-04-12 06:25:35,418 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1681227483635_0007 requests cleared
2023-04-12 06:25:35,418 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1681227483635_0007 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-04-12 06:25:35,418 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1681227483635_0007 user: hdfs leaf-queue of parent: root #applications: 0
2023-04-12 06:25:35,418 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1681227483635_0007,name=Spark shell,user=hdfs,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1681227483635_0007/,appMasterHost=10.130.0.6,submitTime=1681280713817,startTime=1681280713817,finishTime=1681280734938,finalStatus=SUCCEEDED,memorySeconds=104748,vcoreSeconds=60,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=SPARK,resourceSeconds=104748 MB-seconds\, 60 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2023-04-12 06:25:35,418 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1681227483635_0007_000001
2023-04-12 06:27:29,743 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 8
2023-04-12 06:27:30,387 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1681227483635_0008' is submitted without priority hence considering default queue/cluster priority: 0
2023-04-12 06:27:30,387 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1681227483635_0008
2023-04-12 06:27:30,387 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 8 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-12 06:27:30,387 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 8 submitted by user hdfs
2023-04-12 06:27:30,387 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.4	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1681227483635_0008	QUEUENAME=default
2023-04-12 06:27:30,388 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1681227483635_0008
2023-04-12 06:27:30,388 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1681227483635_0008
2023-04-12 06:27:30,388 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0008 State change from NEW to NEW_SAVING on event = START
2023-04-12 06:27:30,389 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0008 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2023-04-12 06:27:30,389 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1681227483635_0008 user: hdfs leaf-queue of parent: root #applications: 1
2023-04-12 06:27:30,389 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1681227483635_0008 from user: hdfs, in queue: default
2023-04-12 06:27:30,389 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0008 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2023-04-12 06:27:30,389 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1681227483635_0008_000001
2023-04-12 06:27:30,389 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0008_000001 State change from NEW to SUBMITTED on event = START
2023-04-12 06:27:30,390 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1681227483635_0008 from user: hdfs activated in queue: default
2023-04-12 06:27:30,390 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1681227483635_0008 user: hdfs, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-12 06:27:30,390 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1681227483635_0008_000001 to scheduler from user hdfs in queue default
2023-04-12 06:27:30,390 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0008_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-12 06:27:30,763 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0008_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-12 06:27:30,763 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0008_01_000001 Container Transitioned from NEW to ALLOCATED
2023-04-12 06:27:30,763 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0008_01_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-04-12 06:27:30,763 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0008	CONTAINERID=container_e04_1681227483635_0008_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-12 06:27:30,763 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.04836805 absoluteUsedCapacity=0.04836805 used=<memory:1024, vCores:1> cluster=<memory:21171, vCores:6>
2023-04-12 06:27:30,763 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-12 06:27:30,764 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_e04_1681227483635_0008_01_000001
2023-04-12 06:27:30,766 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0008_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-12 06:27:30,766 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1681227483635_0008_000001
2023-04-12 06:27:30,766 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1681227483635_0008 AttemptId: appattempt_1681227483635_0008_000001 MasterContainer: Container: [ContainerId: container_e04_1681227483635_0008_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ]
2023-04-12 06:27:30,766 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0008_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-12 06:27:30,767 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0008_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-12 06:27:30,767 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1681227483635_0008_000001
2023-04-12 06:27:30,768 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e04_1681227483635_0008_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0008_000001
2023-04-12 06:27:30,769 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1681227483635_0008_000001
2023-04-12 06:27:30,769 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1681227483635_0008_000001
2023-04-12 06:27:30,783 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e04_1681227483635_0008_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0008_000001
2023-04-12 06:27:30,783 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0008_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-12 06:27:31,764 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0008_01_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-12 06:27:33,687 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0008_000001 (auth:SIMPLE)
2023-04-12 06:27:33,690 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1681227483635_0008_000001
2023-04-12 06:27:33,690 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.6	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1681227483635_0008	APPATTEMPTID=appattempt_1681227483635_0008_000001
2023-04-12 06:27:33,690 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0008_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-12 06:27:33,690 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0008 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-12 06:27:34,140 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0008_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-12 06:27:34,140 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0008_01_000002 Container Transitioned from NEW to ALLOCATED
2023-04-12 06:27:34,140 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0008_01_000002 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2023-04-12 06:27:34,140 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0008	CONTAINERID=container_e04_1681227483635_0008_01_000002	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:27:34,140 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.14510415 absoluteUsedCapacity=0.14510415 used=<memory:3072, vCores:2> cluster=<memory:21171, vCores:6>
2023-04-12 06:27:34,140 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-12 06:27:34,238 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0008_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-12 06:27:34,238 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0008_01_000003 Container Transitioned from NEW to ALLOCATED
2023-04-12 06:27:34,238 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0008_01_000003 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2023-04-12 06:27:34,238 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0008	CONTAINERID=container_e04_1681227483635_0008_01_000003	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:27:34,238 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.24184026 absoluteUsedCapacity=0.24184026 used=<memory:5120, vCores:3> cluster=<memory:21171, vCores:6>
2023-04-12 06:27:34,238 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-12 06:27:34,465 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode1.ru-central1.internal:8041 for container : container_e04_1681227483635_0008_01_000002
2023-04-12 06:27:34,466 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0008_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-12 06:27:34,467 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e04_1681227483635_0008_01_000003
2023-04-12 06:27:34,468 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0008_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-12 06:27:35,141 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0008_01_000002 Container Transitioned from ACQUIRED to RUNNING
2023-04-12 06:27:35,247 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0008_01_000003 Container Transitioned from ACQUIRED to RUNNING
2023-04-12 06:27:37,526 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1681227483635_0008
2023-04-12 06:27:52,390 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1681227483635_0008_000001 with final state: FINISHING, and exit status: -1000
2023-04-12 06:27:52,390 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0008_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2023-04-12 06:27:52,390 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1681227483635_0008 with final state: FINISHING
2023-04-12 06:27:52,390 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0008 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2023-04-12 06:27:52,391 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1681227483635_0008
2023-04-12 06:27:52,391 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0008_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2023-04-12 06:27:52,391 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0008 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2023-04-12 06:27:52,424 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0008_01_000003 Container Transitioned from RUNNING to COMPLETED
2023-04-12 06:27:52,424 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0008	CONTAINERID=container_e04_1681227483635_0008_01_000003	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:27:52,432 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0008_01_000002 Container Transitioned from RUNNING to COMPLETED
2023-04-12 06:27:52,432 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0008	CONTAINERID=container_e04_1681227483635_0008_01_000002	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:27:52,494 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1681227483635_0008 unregistered successfully. 
2023-04-12 06:27:52,927 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1681227483635_0008_000001
2023-04-12 06:27:52,927 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0008_01_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-12 06:27:52,927 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1681227483635_0008_000001
2023-04-12 06:27:52,927 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0008	CONTAINERID=container_e04_1681227483635_0008_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-12 06:27:52,927 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0008_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2023-04-12 06:27:52,928 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0008 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2023-04-12 06:27:52,928 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1681227483635_0008_000001 is done. finalState=FINISHED
2023-04-12 06:27:52,928 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1681227483635_0008
2023-04-12 06:27:52,928 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1681227483635_0008 requests cleared
2023-04-12 06:27:52,928 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1681227483635_0008 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-04-12 06:27:52,928 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1681227483635_0008 user: hdfs leaf-queue of parent: root #applications: 0
2023-04-12 06:27:52,928 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1681227483635_0008,name=Spark shell,user=hdfs,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1681227483635_0008/,appMasterHost=10.130.0.6,submitTime=1681280850387,startTime=1681280850387,finishTime=1681280872390,finalStatus=SUCCEEDED,memorySeconds=97403,vcoreSeconds=58,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=SPARK,resourceSeconds=97403 MB-seconds\, 58 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2023-04-12 06:27:52,929 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1681227483635_0008_000001
2023-04-12 06:28:36,384 WARN org.apache.hadoop.ipc.Server: Incorrect header or version mismatch from 127.0.0.1:50384 got version 0 expected version 9
2023-04-12 06:28:49,462 WARN org.apache.hadoop.ipc.Server: Incorrect header or version mismatch from 127.0.0.1:60434 got version 0 expected version 9
2023-04-12 06:29:04,157 WARN org.apache.hadoop.ipc.Server: Incorrect header or version mismatch from 127.0.0.1:52960 got version 0 expected version 9
2023-04-12 06:29:50,166 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 9
2023-04-12 06:29:50,832 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1681227483635_0009' is submitted without priority hence considering default queue/cluster priority: 0
2023-04-12 06:29:50,832 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1681227483635_0009
2023-04-12 06:29:50,832 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 9 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-12 06:29:50,832 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 9 submitted by user hdfs
2023-04-12 06:29:50,832 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.4	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1681227483635_0009	QUEUENAME=default
2023-04-12 06:29:50,833 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1681227483635_0009
2023-04-12 06:29:50,833 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0009 State change from NEW to NEW_SAVING on event = START
2023-04-12 06:29:50,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1681227483635_0009
2023-04-12 06:29:50,834 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0009 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2023-04-12 06:29:50,834 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1681227483635_0009 user: hdfs leaf-queue of parent: root #applications: 1
2023-04-12 06:29:50,834 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1681227483635_0009 from user: hdfs, in queue: default
2023-04-12 06:29:50,835 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0009 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2023-04-12 06:29:50,835 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1681227483635_0009_000001
2023-04-12 06:29:50,835 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0009_000001 State change from NEW to SUBMITTED on event = START
2023-04-12 06:29:50,835 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1681227483635_0009 from user: hdfs activated in queue: default
2023-04-12 06:29:50,835 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1681227483635_0009 user: hdfs, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-12 06:29:50,835 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1681227483635_0009_000001 to scheduler from user hdfs in queue default
2023-04-12 06:29:50,836 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0009_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-12 06:29:51,266 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0009_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-12 06:29:51,266 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0009_01_000001 Container Transitioned from NEW to ALLOCATED
2023-04-12 06:29:51,266 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0009_01_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-04-12 06:29:51,266 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0009	CONTAINERID=container_e04_1681227483635_0009_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-12 06:29:51,267 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_e04_1681227483635_0009_01_000001
2023-04-12 06:29:51,267 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0009_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-12 06:29:51,268 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1681227483635_0009_000001
2023-04-12 06:29:51,268 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1681227483635_0009 AttemptId: appattempt_1681227483635_0009_000001 MasterContainer: Container: [ContainerId: container_e04_1681227483635_0009_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ]
2023-04-12 06:29:51,268 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.04836805 absoluteUsedCapacity=0.04836805 used=<memory:1024, vCores:1> cluster=<memory:21171, vCores:6>
2023-04-12 06:29:51,268 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-12 06:29:51,268 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0009_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-12 06:29:51,269 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0009_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-12 06:29:51,269 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1681227483635_0009_000001
2023-04-12 06:29:51,271 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e04_1681227483635_0009_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0009_000001
2023-04-12 06:29:51,271 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1681227483635_0009_000001
2023-04-12 06:29:51,271 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1681227483635_0009_000001
2023-04-12 06:29:51,285 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e04_1681227483635_0009_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0009_000001
2023-04-12 06:29:51,286 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0009_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-12 06:29:52,267 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0009_01_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-12 06:29:55,120 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0009_000001 (auth:SIMPLE)
2023-04-12 06:29:55,124 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1681227483635_0009_000001
2023-04-12 06:29:55,124 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.6	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1681227483635_0009	APPATTEMPTID=appattempt_1681227483635_0009_000001
2023-04-12 06:29:55,124 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0009_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-12 06:29:55,124 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0009 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-12 06:29:55,635 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0009_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-12 06:29:55,635 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0009_01_000002 Container Transitioned from NEW to ALLOCATED
2023-04-12 06:29:55,635 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0009_01_000002 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2023-04-12 06:29:55,635 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0009	CONTAINERID=container_e04_1681227483635_0009_01_000002	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:29:55,635 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.14510415 absoluteUsedCapacity=0.14510415 used=<memory:3072, vCores:2> cluster=<memory:21171, vCores:6>
2023-04-12 06:29:55,635 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-12 06:29:55,709 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0009_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-12 06:29:55,709 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0009_01_000003 Container Transitioned from NEW to ALLOCATED
2023-04-12 06:29:55,709 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0009_01_000003 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2023-04-12 06:29:55,709 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0009	CONTAINERID=container_e04_1681227483635_0009_01_000003	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:29:55,710 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.24184026 absoluteUsedCapacity=0.24184026 used=<memory:5120, vCores:3> cluster=<memory:21171, vCores:6>
2023-04-12 06:29:55,710 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-12 06:29:55,915 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode1.ru-central1.internal:8041 for container : container_e04_1681227483635_0009_01_000002
2023-04-12 06:29:55,916 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0009_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-12 06:29:55,916 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e04_1681227483635_0009_01_000003
2023-04-12 06:29:55,918 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0009_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-12 06:29:56,636 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0009_01_000002 Container Transitioned from ACQUIRED to RUNNING
2023-04-12 06:29:56,715 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0009_01_000003 Container Transitioned from ACQUIRED to RUNNING
2023-04-12 06:29:58,943 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1681227483635_0009
2023-04-12 06:31:08,470 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1681227483635_0009_000001 with final state: FINISHING, and exit status: -1000
2023-04-12 06:31:08,470 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0009_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2023-04-12 06:31:08,470 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1681227483635_0009 with final state: FINISHING
2023-04-12 06:31:08,470 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0009 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2023-04-12 06:31:08,470 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1681227483635_0009
2023-04-12 06:31:08,473 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0009_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2023-04-12 06:31:08,474 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0009 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2023-04-12 06:31:08,509 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0009_01_000003 Container Transitioned from RUNNING to COMPLETED
2023-04-12 06:31:08,509 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0009	CONTAINERID=container_e04_1681227483635_0009_01_000003	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:31:08,520 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0009_01_000002 Container Transitioned from RUNNING to COMPLETED
2023-04-12 06:31:08,520 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0009	CONTAINERID=container_e04_1681227483635_0009_01_000002	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:31:08,584 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1681227483635_0009 unregistered successfully. 
2023-04-12 06:31:09,019 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1681227483635_0009_000001
2023-04-12 06:31:09,019 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0009_01_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-12 06:31:09,019 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1681227483635_0009_000001
2023-04-12 06:31:09,019 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0009	CONTAINERID=container_e04_1681227483635_0009_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-12 06:31:09,019 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0009_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2023-04-12 06:31:09,020 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0009 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2023-04-12 06:31:09,020 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1681227483635_0009_000001 is done. finalState=FINISHED
2023-04-12 06:31:09,020 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1681227483635_0009 requests cleared
2023-04-12 06:31:09,020 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1681227483635_0009
2023-04-12 06:31:09,020 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1681227483635_0009 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-04-12 06:31:09,020 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1681227483635_0009 user: hdfs leaf-queue of parent: root #applications: 0
2023-04-12 06:31:09,021 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1681227483635_0009,name=Spark shell,user=hdfs,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1681227483635_0009/,appMasterHost=10.130.0.6,submitTime=1681280990832,startTime=1681280990832,finishTime=1681281068470,finalStatus=SUCCEEDED,memorySeconds=377981,vcoreSeconds=221,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=SPARK,resourceSeconds=377981 MB-seconds\, 221 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2023-04-12 06:31:09,021 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1681227483635_0009_000001
2023-04-12 06:33:12,711 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 10
2023-04-12 06:33:13,333 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1681227483635_0010' is submitted without priority hence considering default queue/cluster priority: 0
2023-04-12 06:33:13,333 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1681227483635_0010
2023-04-12 06:33:13,333 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 10 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-12 06:33:13,333 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 10 submitted by user hdfs
2023-04-12 06:33:13,333 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.4	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1681227483635_0010	QUEUENAME=default
2023-04-12 06:33:13,334 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1681227483635_0010
2023-04-12 06:33:13,334 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0010 State change from NEW to NEW_SAVING on event = START
2023-04-12 06:33:13,334 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1681227483635_0010
2023-04-12 06:33:13,335 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0010 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2023-04-12 06:33:13,335 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1681227483635_0010 user: hdfs leaf-queue of parent: root #applications: 1
2023-04-12 06:33:13,335 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1681227483635_0010 from user: hdfs, in queue: default
2023-04-12 06:33:13,335 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0010 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2023-04-12 06:33:13,335 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1681227483635_0010_000001
2023-04-12 06:33:13,335 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0010_000001 State change from NEW to SUBMITTED on event = START
2023-04-12 06:33:13,336 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1681227483635_0010 from user: hdfs activated in queue: default
2023-04-12 06:33:13,336 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1681227483635_0010 user: hdfs, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-12 06:33:13,336 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1681227483635_0010_000001 to scheduler from user hdfs in queue default
2023-04-12 06:33:13,337 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0010_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-12 06:33:13,753 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0010_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-12 06:33:13,753 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0010_01_000001 Container Transitioned from NEW to ALLOCATED
2023-04-12 06:33:13,753 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0010_01_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-04-12 06:33:13,753 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0010	CONTAINERID=container_e04_1681227483635_0010_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-12 06:33:13,754 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode1.ru-central1.internal:8041 for container : container_e04_1681227483635_0010_01_000001
2023-04-12 06:33:13,755 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0010_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-12 06:33:13,755 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1681227483635_0010_000001
2023-04-12 06:33:13,755 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.04836805 absoluteUsedCapacity=0.04836805 used=<memory:1024, vCores:1> cluster=<memory:21171, vCores:6>
2023-04-12 06:33:13,755 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1681227483635_0010 AttemptId: appattempt_1681227483635_0010_000001 MasterContainer: Container: [ContainerId: container_e04_1681227483635_0010_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode1.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode1.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.4:8041 }, ExecutionType: GUARANTEED, ]
2023-04-12 06:33:13,755 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-12 06:33:13,755 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0010_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-12 06:33:13,756 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0010_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-12 06:33:13,756 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1681227483635_0010_000001
2023-04-12 06:33:13,758 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e04_1681227483635_0010_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode1.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode1.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.4:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0010_000001
2023-04-12 06:33:13,758 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1681227483635_0010_000001
2023-04-12 06:33:13,758 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1681227483635_0010_000001
2023-04-12 06:33:13,772 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e04_1681227483635_0010_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode1.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode1.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.4:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0010_000001
2023-04-12 06:33:13,773 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0010_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-12 06:33:14,753 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0010_01_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-12 06:33:16,724 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0010_000001 (auth:SIMPLE)
2023-04-12 06:33:16,727 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1681227483635_0010_000001
2023-04-12 06:33:16,727 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.4	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1681227483635_0010	APPATTEMPTID=appattempt_1681227483635_0010_000001
2023-04-12 06:33:16,727 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0010_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-12 06:33:16,727 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0010 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-12 06:33:17,347 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0010_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-12 06:33:17,347 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0010_01_000002 Container Transitioned from NEW to ALLOCATED
2023-04-12 06:33:17,347 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0010_01_000002 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2023-04-12 06:33:17,347 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0010	CONTAINERID=container_e04_1681227483635_0010_01_000002	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:33:17,347 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.14510415 absoluteUsedCapacity=0.14510415 used=<memory:3072, vCores:2> cluster=<memory:21171, vCores:6>
2023-04-12 06:33:17,347 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-12 06:33:17,528 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_e04_1681227483635_0010_01_000002
2023-04-12 06:33:17,530 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0010_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-12 06:33:17,764 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0010_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-12 06:33:17,764 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0010_01_000003 Container Transitioned from NEW to ALLOCATED
2023-04-12 06:33:17,764 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0010_01_000003 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 2 containers, <memory:3072, vCores:2> used and <memory:3985, vCores:0> available after allocation
2023-04-12 06:33:17,764 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0010	CONTAINERID=container_e04_1681227483635_0010_01_000003	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:33:17,765 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.24184026 absoluteUsedCapacity=0.24184026 used=<memory:5120, vCores:3> cluster=<memory:21171, vCores:6>
2023-04-12 06:33:17,765 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-12 06:33:18,355 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode1.ru-central1.internal:8041 for container : container_e04_1681227483635_0010_01_000003
2023-04-12 06:33:18,356 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0010_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-12 06:33:18,360 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0010_01_000002 Container Transitioned from ACQUIRED to RUNNING
2023-04-12 06:33:18,367 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0010_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-12 06:33:18,367 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0010_01_000004 Container Transitioned from NEW to ALLOCATED
2023-04-12 06:33:18,367 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0010_01_000004 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 2 containers, <memory:4096, vCores:2> used and <memory:2961, vCores:0> available after allocation
2023-04-12 06:33:18,367 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0010	CONTAINERID=container_e04_1681227483635_0010_01_000004	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:33:18,367 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.33857635 absoluteUsedCapacity=0.33857635 used=<memory:7168, vCores:4> cluster=<memory:21171, vCores:6>
2023-04-12 06:33:18,367 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-12 06:33:18,769 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0010_01_000003 Container Transitioned from ACQUIRED to RUNNING
2023-04-12 06:33:21,381 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1681227483635_0010
2023-04-12 06:33:21,383 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0010_01_000004 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-12 06:33:24,390 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0010_01_000004 Container Transitioned from ACQUIRED to RELEASED
2023-04-12 06:33:24,391 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.4	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0010	CONTAINERID=container_e04_1681227483635_0010_01_000004	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:35:00,815 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1681227483635_0010_000001 with final state: FINISHING, and exit status: -1000
2023-04-12 06:35:00,816 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0010_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2023-04-12 06:35:00,816 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1681227483635_0010 with final state: FINISHING
2023-04-12 06:35:00,816 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0010 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2023-04-12 06:35:00,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1681227483635_0010
2023-04-12 06:35:00,817 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0010_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2023-04-12 06:35:00,817 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0010 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2023-04-12 06:35:00,995 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1681227483635_0010 unregistered successfully. 
2023-04-12 06:35:01,271 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0010_01_000002 Container Transitioned from RUNNING to COMPLETED
2023-04-12 06:35:01,271 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0010	CONTAINERID=container_e04_1681227483635_0010_01_000002	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:35:01,318 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0010_01_000003 Container Transitioned from RUNNING to COMPLETED
2023-04-12 06:35:01,318 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0010	CONTAINERID=container_e04_1681227483635_0010_01_000003	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:35:01,381 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1681227483635_0010_000001
2023-04-12 06:35:01,381 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0010_01_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-12 06:35:01,381 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1681227483635_0010_000001
2023-04-12 06:35:01,381 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0010	CONTAINERID=container_e04_1681227483635_0010_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-12 06:35:01,381 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0010_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2023-04-12 06:35:01,382 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0010 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2023-04-12 06:35:01,382 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1681227483635_0010_000001 is done. finalState=FINISHED
2023-04-12 06:35:01,382 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1681227483635_0010
2023-04-12 06:35:01,382 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1681227483635_0010 requests cleared
2023-04-12 06:35:01,382 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1681227483635_0010 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-04-12 06:35:01,382 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1681227483635_0010 user: hdfs leaf-queue of parent: root #applications: 0
2023-04-12 06:35:01,382 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1681227483635_0010,name=Spark shell,user=hdfs,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1681227483635_0010/,appMasterHost=10.130.0.4,submitTime=1681281193333,startTime=1681281193333,finishTime=1681281300816,finalStatus=SUCCEEDED,memorySeconds=547463,vcoreSeconds=319,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=SPARK,resourceSeconds=547463 MB-seconds\, 319 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2023-04-12 06:35:01,382 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1681227483635_0010_000001
2023-04-12 06:35:32,573 WARN org.apache.hadoop.ipc.Server: Incorrect header or version mismatch from 192.168.126.6:33748 got version 0 expected version 9
2023-04-12 06:36:28,590 WARN org.apache.hadoop.ipc.Server: Incorrect header or version mismatch from 127.0.0.1:54208 got version 0 expected version 9
2023-04-12 06:38:03,782 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 06:38:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 27 msec
2023-04-12 06:40:13,416 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 11
2023-04-12 06:40:14,036 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1681227483635_0011' is submitted without priority hence considering default queue/cluster priority: 0
2023-04-12 06:40:14,036 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1681227483635_0011
2023-04-12 06:40:14,036 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 11 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-12 06:40:14,037 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 11 submitted by user hdfs
2023-04-12 06:40:14,037 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.4	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1681227483635_0011	QUEUENAME=default
2023-04-12 06:40:14,037 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1681227483635_0011
2023-04-12 06:40:14,037 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1681227483635_0011
2023-04-12 06:40:14,037 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0011 State change from NEW to NEW_SAVING on event = START
2023-04-12 06:40:14,038 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0011 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2023-04-12 06:40:14,038 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1681227483635_0011 user: hdfs leaf-queue of parent: root #applications: 1
2023-04-12 06:40:14,038 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1681227483635_0011 from user: hdfs, in queue: default
2023-04-12 06:40:14,038 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0011 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2023-04-12 06:40:14,038 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1681227483635_0011_000001
2023-04-12 06:40:14,038 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0011_000001 State change from NEW to SUBMITTED on event = START
2023-04-12 06:40:14,039 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1681227483635_0011 from user: hdfs activated in queue: default
2023-04-12 06:40:14,039 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1681227483635_0011 user: hdfs, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-12 06:40:14,039 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1681227483635_0011_000001 to scheduler from user hdfs in queue default
2023-04-12 06:40:14,041 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0011_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-12 06:40:14,792 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0011_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-12 06:40:14,793 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0011_01_000001 Container Transitioned from NEW to ALLOCATED
2023-04-12 06:40:14,793 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0011_01_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-04-12 06:40:14,793 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0011	CONTAINERID=container_e04_1681227483635_0011_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-12 06:40:14,793 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode1.ru-central1.internal:8041 for container : container_e04_1681227483635_0011_01_000001
2023-04-12 06:40:14,794 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0011_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-12 06:40:14,794 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1681227483635_0011_000001
2023-04-12 06:40:14,794 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1681227483635_0011 AttemptId: appattempt_1681227483635_0011_000001 MasterContainer: Container: [ContainerId: container_e04_1681227483635_0011_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode1.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode1.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.4:8041 }, ExecutionType: GUARANTEED, ]
2023-04-12 06:40:14,794 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0011_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-12 06:40:14,795 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0011_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-12 06:40:14,795 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.04836805 absoluteUsedCapacity=0.04836805 used=<memory:1024, vCores:1> cluster=<memory:21171, vCores:6>
2023-04-12 06:40:14,795 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-12 06:40:14,796 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1681227483635_0011_000001
2023-04-12 06:40:14,797 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e04_1681227483635_0011_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode1.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode1.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.4:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0011_000001
2023-04-12 06:40:14,797 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1681227483635_0011_000001
2023-04-12 06:40:14,797 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1681227483635_0011_000001
2023-04-12 06:40:14,815 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e04_1681227483635_0011_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode1.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode1.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.4:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0011_000001
2023-04-12 06:40:14,815 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0011_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-12 06:40:15,792 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0011_01_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-12 06:40:17,815 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0011_000001 (auth:SIMPLE)
2023-04-12 06:40:17,818 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1681227483635_0011_000001
2023-04-12 06:40:17,818 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.4	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1681227483635_0011	APPATTEMPTID=appattempt_1681227483635_0011_000001
2023-04-12 06:40:17,818 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0011_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-12 06:40:17,818 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0011 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-12 06:40:18,031 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0011_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-12 06:40:18,032 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0011_01_000002 Container Transitioned from NEW to ALLOCATED
2023-04-12 06:40:18,032 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0011_01_000002 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 1 containers, <memory:2048, vCores:1> used and <memory:5009, vCores:1> available after allocation
2023-04-12 06:40:18,032 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0011	CONTAINERID=container_e04_1681227483635_0011_01_000002	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:40:18,032 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.14510415 absoluteUsedCapacity=0.14510415 used=<memory:3072, vCores:2> cluster=<memory:21171, vCores:6>
2023-04-12 06:40:18,032 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-12 06:40:18,226 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_e04_1681227483635_0011_01_000002
2023-04-12 06:40:18,229 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0011_01_000002 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-12 06:40:18,796 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0011_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-12 06:40:18,797 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0011_01_000003 Container Transitioned from NEW to ALLOCATED
2023-04-12 06:40:18,797 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0011_01_000003 of capacity <memory:2048, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 2 containers, <memory:3072, vCores:2> used and <memory:3985, vCores:0> available after allocation
2023-04-12 06:40:18,797 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0011	CONTAINERID=container_e04_1681227483635_0011_01_000003	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:40:18,797 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.24184026 absoluteUsedCapacity=0.24184026 used=<memory:5120, vCores:3> cluster=<memory:21171, vCores:6>
2023-04-12 06:40:18,797 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-12 06:40:19,032 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0011_01_000002 Container Transitioned from ACQUIRED to RUNNING
2023-04-12 06:40:19,467 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode1.ru-central1.internal:8041 for container : container_e04_1681227483635_0011_01_000003
2023-04-12 06:40:19,467 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0011_01_000003 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-12 06:40:19,803 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0011_01_000003 Container Transitioned from ACQUIRED to RUNNING
2023-04-12 06:40:22,476 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :application_1681227483635_0011
2023-04-12 06:40:27,692 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0011_01_000002 Container Transitioned from RUNNING to COMPLETED
2023-04-12 06:40:27,692 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0011	CONTAINERID=container_e04_1681227483635_0011_01_000002	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:40:27,695 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0011_01_000003 Container Transitioned from RUNNING to COMPLETED
2023-04-12 06:40:27,695 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0011	CONTAINERID=container_e04_1681227483635_0011_01_000003	RESOURCE=<memory:2048, vCores:1>	QUEUENAME=default
2023-04-12 06:40:27,732 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1681227483635_0011_000001 with final state: FINISHING, and exit status: -1000
2023-04-12 06:40:27,732 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0011_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2023-04-12 06:40:27,732 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1681227483635_0011 with final state: FINISHING
2023-04-12 06:40:27,732 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0011 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2023-04-12 06:40:27,732 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1681227483635_0011
2023-04-12 06:40:27,733 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0011_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2023-04-12 06:40:27,733 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0011 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2023-04-12 06:40:27,836 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1681227483635_0011 unregistered successfully. 
2023-04-12 06:40:28,206 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1681227483635_0011_000001
2023-04-12 06:40:28,206 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0011_01_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-12 06:40:28,206 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1681227483635_0011_000001
2023-04-12 06:40:28,206 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0011	CONTAINERID=container_e04_1681227483635_0011_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-12 06:40:28,206 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0011_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2023-04-12 06:40:28,207 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0011 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2023-04-12 06:40:28,207 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1681227483635_0011_000001 is done. finalState=FINISHED
2023-04-12 06:40:28,207 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1681227483635_0011
2023-04-12 06:40:28,207 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1681227483635_0011 requests cleared
2023-04-12 06:40:28,208 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1681227483635_0011 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-04-12 06:40:28,208 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1681227483635_0011 user: hdfs leaf-queue of parent: root #applications: 0
2023-04-12 06:40:28,208 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1681227483635_0011_000001
2023-04-12 06:40:28,208 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1681227483635_0011,name=Spark shell,user=hdfs,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1681227483635_0011/,appMasterHost=10.130.0.4,submitTime=1681281614036,startTime=1681281614036,finishTime=1681281627732,finalStatus=SUCCEEDED,memorySeconds=51745,vcoreSeconds=30,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=SPARK,resourceSeconds=51745 MB-seconds\, 30 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2023-04-12 07:38:03,782 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 07:38:03,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-04-12 08:38:03,782 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 08:38:03,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-12 09:38:03,782 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 09:38:03,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-12 10:38:03,782 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 10:38:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 26 msec
2023-04-12 11:38:03,782 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 11:38:03,786 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-04-12 12:38:03,782 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 12:38:03,793 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-12 13:38:03,783 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 13:38:03,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-12 14:38:03,783 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 14:38:03,838 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 55 msec
2023-04-12 15:38:03,783 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 15:38:03,794 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-12 15:38:03,983 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-04-12 15:38:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-12 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-12 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id 1120025993 in 900000ms
2023-04-12 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-12 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id -1018473539 in 900000ms
2023-04-12 15:38:04,880 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-12 15:38:06,447 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 103
2023-04-12 15:38:06,447 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-12 15:38:06,447 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-12 15:38:06,447 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 115
2023-04-12 15:38:06,447 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-12 15:53:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: -490698613
2023-04-12 15:53:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-12 15:53:03,986 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: 1120025993
2023-04-12 15:53:03,986 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: -1018473539
2023-04-12 16:38:03,783 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 16:38:03,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 33 msec
2023-04-12 17:38:03,784 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 17:38:03,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-12 18:38:03,784 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 18:38:03,790 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-12 19:38:03,784 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 19:38:03,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-04-12 20:38:03,784 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 20:38:03,803 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-04-12 21:38:03,784 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 21:38:03,794 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-12 22:38:03,784 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 22:38:03,791 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-12 23:38:03,785 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-12 23:38:03,803 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-04-13 00:38:03,785 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 00:38:03,800 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-13 01:38:03,785 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 01:38:03,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-04-13 02:38:03,785 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 02:38:03,790 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-13 03:38:03,785 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 03:38:03,797 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-13 04:38:03,786 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 04:38:03,793 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-13 05:38:03,786 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 05:38:03,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-13 06:38:03,786 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 06:38:03,790 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-13 07:38:03,787 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 07:38:03,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-13 08:38:03,787 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 08:38:03,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-13 09:38:03,787 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 09:38:03,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-04-13 10:38:03,787 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 10:38:03,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-13 11:38:03,787 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 11:38:03,794 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-13 12:38:03,787 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 12:38:03,794 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-13 13:38:03,787 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 13:38:03,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 27 msec
2023-04-13 14:38:03,787 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 14:38:03,803 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-04-13 15:38:03,787 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 15:38:03,797 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-13 15:38:03,983 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-04-13 15:38:03,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-13 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-13 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-13 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id -1018473538 in 900000ms
2023-04-13 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id 1120025994 in 900000ms
2023-04-13 15:38:07,277 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-13 15:38:08,912 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 104
2023-04-13 15:38:08,912 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-13 15:38:08,912 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-13 15:38:08,912 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 116
2023-04-13 15:38:08,912 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-13 15:53:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: -490698612
2023-04-13 15:53:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-13 15:53:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: -1018473538
2023-04-13 15:53:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: 1120025994
2023-04-13 16:38:03,787 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 16:38:03,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-04-13 17:38:03,787 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 17:38:03,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-13 18:38:03,787 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 18:38:03,793 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-13 19:38:03,787 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 19:38:03,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-13 20:38:03,787 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 20:38:03,801 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-04-13 21:38:03,787 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 21:38:03,791 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-13 22:38:03,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 22:38:03,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-13 23:38:03,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-13 23:38:03,793 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-14 00:38:03,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 00:38:03,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-14 01:38:03,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 01:38:03,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-14 02:38:03,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 02:38:03,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 26 msec
2023-04-14 03:38:03,789 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 03:38:03,800 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-14 04:38:03,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 04:38:03,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-14 05:38:03,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 05:38:03,800 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-14 06:38:03,788 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 06:38:03,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 27 msec
2023-04-14 07:38:03,789 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 07:38:03,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-04-14 08:38:03,789 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 08:38:03,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-14 09:38:03,789 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 09:38:03,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-04-14 10:38:03,789 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 10:38:03,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-14 11:38:03,789 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 11:38:03,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-04-14 12:38:03,789 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 12:38:03,797 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-14 13:38:03,789 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 13:38:03,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-14 14:38:03,789 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 14:38:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-04-14 15:38:03,789 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 15:38:03,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-14 15:38:03,983 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-04-14 15:38:03,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-14 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-14 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id -1018473537 in 900000ms
2023-04-14 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-14 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id 1120025995 in 900000ms
2023-04-14 15:38:09,754 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-14 15:38:11,174 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 105
2023-04-14 15:38:11,174 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-14 15:38:11,174 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-14 15:38:11,174 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 117
2023-04-14 15:38:11,174 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-14 15:53:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: -490698611
2023-04-14 15:53:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-14 15:53:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: -1018473537
2023-04-14 15:53:03,986 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: 1120025995
2023-04-14 16:38:03,790 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 16:38:03,838 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 49 msec
2023-04-14 17:38:03,790 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 17:38:03,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-14 18:38:03,791 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 18:38:03,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-14 19:38:03,791 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 19:38:03,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-14 20:38:03,791 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 20:38:03,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-14 21:38:03,791 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 21:38:03,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-14 22:38:03,791 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 22:38:03,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-04-14 23:38:03,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-14 23:38:03,800 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-15 00:38:03,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 00:38:03,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-15 01:38:03,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 01:38:03,800 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-15 02:38:03,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 02:38:03,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-04-15 03:38:03,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 03:38:03,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-04-15 04:38:03,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 04:38:03,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-15 05:38:03,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 05:38:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-15 06:38:03,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 06:38:03,803 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-15 07:38:03,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 07:38:03,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-15 08:38:03,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 08:38:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-04-15 09:38:03,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 09:38:03,812 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-04-15 10:38:03,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 10:38:03,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-15 11:38:03,792 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 11:38:03,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-04-15 12:38:03,793 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 12:38:03,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-04-15 13:38:03,794 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 13:38:03,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-04-15 14:38:03,794 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 14:38:03,803 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-15 15:38:03,794 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 15:38:03,797 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-15 15:38:03,983 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-04-15 15:38:03,983 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-15 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-15 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id 1120025996 in 900000ms
2023-04-15 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-15 15:38:03,986 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id -1018473536 in 900000ms
2023-04-15 15:38:12,019 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-15 15:38:13,547 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 106
2023-04-15 15:38:13,547 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-15 15:38:13,547 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-15 15:38:13,547 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 118
2023-04-15 15:38:13,547 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-15 15:53:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: -490698610
2023-04-15 15:53:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-15 15:53:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: 1120025996
2023-04-15 15:53:03,986 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: -1018473536
2023-04-15 16:38:03,794 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 16:38:03,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 31 msec
2023-04-15 17:38:03,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 17:38:03,803 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-15 18:38:03,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 18:38:03,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-15 18:50:27,118 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 12
2023-04-15 18:53:25,769 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 13
2023-04-15 19:00:49,834 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 14
2023-04-15 19:01:31,665 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1681227483635_0014' is submitted without priority hence considering default queue/cluster priority: 0
2023-04-15 19:01:31,665 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1681227483635_0014
2023-04-15 19:01:31,665 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 14 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-15 19:01:31,665 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 14 submitted by user hdfs
2023-04-15 19:01:31,665 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=192.168.126.6	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1681227483635_0014	QUEUENAME=default
2023-04-15 19:01:31,666 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1681227483635_0014
2023-04-15 19:01:31,666 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1681227483635_0014
2023-04-15 19:01:31,666 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0014 State change from NEW to NEW_SAVING on event = START
2023-04-15 19:01:31,666 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0014 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2023-04-15 19:01:31,666 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1681227483635_0014 user: hdfs leaf-queue of parent: root #applications: 1
2023-04-15 19:01:31,667 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1681227483635_0014 from user: hdfs, in queue: default
2023-04-15 19:01:31,667 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0014 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2023-04-15 19:01:31,667 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1681227483635_0014_000001
2023-04-15 19:01:31,667 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0014_000001 State change from NEW to SUBMITTED on event = START
2023-04-15 19:01:31,667 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1681227483635_0014 from user: hdfs activated in queue: default
2023-04-15 19:01:31,667 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1681227483635_0014 user: hdfs, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-15 19:01:31,667 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1681227483635_0014_000001 to scheduler from user hdfs in queue default
2023-04-15 19:01:31,668 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0014_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-15 19:01:31,782 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0014_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-15 19:01:31,782 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0014_01_000001 Container Transitioned from NEW to ALLOCATED
2023-04-15 19:01:31,782 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0014_01_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-04-15 19:01:31,782 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0014	CONTAINERID=container_e04_1681227483635_0014_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:01:31,783 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_e04_1681227483635_0014_01_000001
2023-04-15 19:01:31,784 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0014_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-15 19:01:31,784 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1681227483635_0014_000001
2023-04-15 19:01:31,784 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1681227483635_0014 AttemptId: appattempt_1681227483635_0014_000001 MasterContainer: Container: [ContainerId: container_e04_1681227483635_0014_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ]
2023-04-15 19:01:31,784 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.04836805 absoluteUsedCapacity=0.04836805 used=<memory:1024, vCores:1> cluster=<memory:21171, vCores:6>
2023-04-15 19:01:31,784 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-15 19:01:31,784 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0014_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-15 19:01:31,785 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0014_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-15 19:01:31,785 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1681227483635_0014_000001
2023-04-15 19:01:31,786 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e04_1681227483635_0014_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0014_000001
2023-04-15 19:01:31,787 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1681227483635_0014_000001
2023-04-15 19:01:31,787 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1681227483635_0014_000001
2023-04-15 19:01:31,806 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e04_1681227483635_0014_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0014_000001
2023-04-15 19:01:31,806 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0014_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-15 19:01:32,782 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0014_01_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-15 19:01:35,641 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0014_000001 (auth:SIMPLE)
2023-04-15 19:01:35,650 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1681227483635_0014_000001
2023-04-15 19:01:35,650 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.6	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1681227483635_0014	APPATTEMPTID=appattempt_1681227483635_0014_000001
2023-04-15 19:01:35,650 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0014_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-15 19:01:35,650 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0014 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-15 19:02:23,778 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 15
2023-04-15 19:03:12,446 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 16
2023-04-15 19:03:35,850 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0014_01_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-15 19:03:35,850 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0014	CONTAINERID=container_e04_1681227483635_0014_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:03:35,850 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1681227483635_0014_000001 with final state: FAILED, and exit status: 10
2023-04-15 19:03:35,851 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0014_000001 State change from RUNNING to FINAL_SAVING on event = CONTAINER_FINISHED
2023-04-15 19:03:35,851 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1681227483635_0014_000001
2023-04-15 19:03:35,851 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1681227483635_0014_000001
2023-04-15 19:03:35,851 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0014_000001 State change from FINAL_SAVING to FAILED on event = ATTEMPT_UPDATE_SAVED
2023-04-15 19:03:35,851 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 1. The max attempts is 2
2023-04-15 19:03:35,851 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0014 State change from RUNNING to ACCEPTED on event = ATTEMPT_FAILED
2023-04-15 19:03:35,851 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1681227483635_0014_000001 is done. finalState=FAILED
2023-04-15 19:03:35,851 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1681227483635_0014_000002
2023-04-15 19:03:35,852 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1681227483635_0014 requests cleared
2023-04-15 19:03:35,852 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0014_000002 State change from NEW to SUBMITTED on event = START
2023-04-15 19:03:35,852 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1681227483635_0014 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-04-15 19:03:35,852 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1681227483635_0014_000001
2023-04-15 19:03:35,852 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1681227483635_0014 from user: hdfs activated in queue: default
2023-04-15 19:03:35,852 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1681227483635_0014 user: hdfs, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-15 19:03:35,852 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1681227483635_0014_000002 to scheduler from user hdfs in queue default
2023-04-15 19:03:35,853 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0014_000002 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-15 19:03:35,882 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0014_000002 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-15 19:03:35,883 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0014_02_000001 Container Transitioned from NEW to ALLOCATED
2023-04-15 19:03:35,883 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0014_02_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-04-15 19:03:35,883 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0014	CONTAINERID=container_e04_1681227483635_0014_02_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:03:35,884 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode1.ru-central1.internal:8041 for container : container_e04_1681227483635_0014_02_000001
2023-04-15 19:03:35,885 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0014_02_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-15 19:03:35,885 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1681227483635_0014_000002
2023-04-15 19:03:35,885 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1681227483635_0014 AttemptId: appattempt_1681227483635_0014_000002 MasterContainer: Container: [ContainerId: container_e04_1681227483635_0014_02_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode1.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode1.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.4:8041 }, ExecutionType: GUARANTEED, ]
2023-04-15 19:03:35,885 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0014_000002 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-15 19:03:35,886 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0014_000002 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-15 19:03:35,886 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.04836805 absoluteUsedCapacity=0.04836805 used=<memory:1024, vCores:1> cluster=<memory:21171, vCores:6>
2023-04-15 19:03:35,887 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-15 19:03:35,887 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1681227483635_0014_000002
2023-04-15 19:03:35,888 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e04_1681227483635_0014_02_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode1.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode1.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.4:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0014_000002
2023-04-15 19:03:35,888 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1681227483635_0014_000002
2023-04-15 19:03:35,888 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1681227483635_0014_000002
2023-04-15 19:03:35,899 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e04_1681227483635_0014_02_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode1.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode1.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.4:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0014_000002
2023-04-15 19:03:35,900 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0014_000002 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-15 19:03:36,884 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0014_02_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-15 19:03:39,723 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0014_000002 (auth:SIMPLE)
2023-04-15 19:03:39,731 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1681227483635_0014_000002
2023-04-15 19:03:39,731 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.4	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1681227483635_0014	APPATTEMPTID=appattempt_1681227483635_0014_000002
2023-04-15 19:03:39,731 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0014_000002 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-15 19:03:39,732 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0014 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-15 19:04:24,514 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 17
2023-04-15 19:05:07,547 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1681227483635_0017' is submitted without priority hence considering default queue/cluster priority: 0
2023-04-15 19:05:07,547 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1681227483635_0017
2023-04-15 19:05:07,547 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 17 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-15 19:05:07,548 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 17 submitted by user hdfs
2023-04-15 19:05:07,548 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=192.168.126.6	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1681227483635_0017	QUEUENAME=default
2023-04-15 19:05:07,548 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1681227483635_0017
2023-04-15 19:05:07,548 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1681227483635_0017
2023-04-15 19:05:07,548 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0017 State change from NEW to NEW_SAVING on event = START
2023-04-15 19:05:07,549 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0017 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2023-04-15 19:05:07,549 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1681227483635_0017 user: hdfs leaf-queue of parent: root #applications: 2
2023-04-15 19:05:07,549 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1681227483635_0017 from user: hdfs, in queue: default
2023-04-15 19:05:07,549 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0017 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2023-04-15 19:05:07,549 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1681227483635_0017_000001
2023-04-15 19:05:07,549 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0017_000001 State change from NEW to SUBMITTED on event = START
2023-04-15 19:05:07,550 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1681227483635_0017 from user: hdfs activated in queue: default
2023-04-15 19:05:07,550 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1681227483635_0017 user: hdfs, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 2 #queue-pending-applications: 0 #queue-active-applications: 2
2023-04-15 19:05:07,550 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1681227483635_0017_000001 to scheduler from user hdfs in queue default
2023-04-15 19:05:07,551 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0017_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-15 19:05:07,676 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0017_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-15 19:05:07,676 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0017_01_000001 Container Transitioned from NEW to ALLOCATED
2023-04-15 19:05:07,676 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0017_01_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-04-15 19:05:07,676 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0017	CONTAINERID=container_e04_1681227483635_0017_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:05:07,677 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e04_1681227483635_0017_01_000001
2023-04-15 19:05:07,678 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0017_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-15 19:05:07,678 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1681227483635_0017_000001
2023-04-15 19:05:07,678 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1681227483635_0017 AttemptId: appattempt_1681227483635_0017_000001 MasterContainer: Container: [ContainerId: container_e04_1681227483635_0017_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ]
2023-04-15 19:05:07,678 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.0967361 absoluteUsedCapacity=0.0967361 used=<memory:2048, vCores:2> cluster=<memory:21171, vCores:6>
2023-04-15 19:05:07,678 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-15 19:05:07,678 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0017_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-15 19:05:07,681 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0017_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-15 19:05:07,682 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1681227483635_0017_000001
2023-04-15 19:05:07,683 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e04_1681227483635_0017_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0017_000001
2023-04-15 19:05:07,683 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1681227483635_0017_000001
2023-04-15 19:05:07,683 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1681227483635_0017_000001
2023-04-15 19:05:07,707 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e04_1681227483635_0017_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0017_000001
2023-04-15 19:05:07,707 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0017_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-15 19:05:08,677 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0017_01_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-15 19:05:15,313 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0017_000001 (auth:SIMPLE)
2023-04-15 19:05:15,323 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1681227483635_0017_000001
2023-04-15 19:05:15,323 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.5	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1681227483635_0017	APPATTEMPTID=appattempt_1681227483635_0017_000001
2023-04-15 19:05:15,323 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0017_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-15 19:05:15,323 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0017 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-15 19:05:39,827 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0014_000002 (auth:SIMPLE)
2023-04-15 19:05:39,832 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1681227483635_0014_000002 with final state: FINISHING, and exit status: -1000
2023-04-15 19:05:39,832 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0014_000002 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2023-04-15 19:05:39,832 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1681227483635_0014 with final state: FINISHING
2023-04-15 19:05:39,832 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0014 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2023-04-15 19:05:39,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1681227483635_0014
2023-04-15 19:05:39,833 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0014_000002 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2023-04-15 19:05:39,833 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0014 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2023-04-15 19:05:39,936 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1681227483635_0014 unregistered successfully. 
2023-04-15 19:05:40,754 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1681227483635_0014_000002
2023-04-15 19:05:40,754 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0014_02_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-15 19:05:40,754 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1681227483635_0014_000002
2023-04-15 19:05:40,754 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0014	CONTAINERID=container_e04_1681227483635_0014_02_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:05:40,754 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0014_000002 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2023-04-15 19:05:40,755 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0014 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2023-04-15 19:05:40,755 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1681227483635_0014_000002 is done. finalState=FINISHED
2023-04-15 19:05:40,755 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1681227483635_0014
2023-04-15 19:05:40,755 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1681227483635_0014 requests cleared
2023-04-15 19:05:40,756 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1681227483635_0014 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-15 19:05:40,756 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1681227483635_0014 user: hdfs leaf-queue of parent: root #applications: 1
2023-04-15 19:05:40,756 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1681227483635_0014,name=pyspark-shell,user=hdfs,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1681227483635_0014/,appMasterHost=10.130.0.4,submitTime=1681585291664,startTime=1681585291665,finishTime=1681585539832,finalStatus=FAILED,memorySeconds=254914,vcoreSeconds=248,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=SPARK,resourceSeconds=254914 MB-seconds\, 248 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2023-04-15 19:05:40,756 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1681227483635_0014_000002
2023-04-15 19:05:55,765 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 18
2023-04-15 19:06:38,632 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1681227483635_0018' is submitted without priority hence considering default queue/cluster priority: 0
2023-04-15 19:06:38,632 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1681227483635_0018
2023-04-15 19:06:38,632 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 18 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-15 19:06:38,633 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 18 submitted by user hdfs
2023-04-15 19:06:38,633 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=192.168.126.6	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1681227483635_0018	QUEUENAME=default
2023-04-15 19:06:38,633 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1681227483635_0018
2023-04-15 19:06:38,633 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0018 State change from NEW to NEW_SAVING on event = START
2023-04-15 19:06:38,633 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1681227483635_0018
2023-04-15 19:06:38,634 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0018 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2023-04-15 19:06:38,634 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1681227483635_0018 user: hdfs leaf-queue of parent: root #applications: 2
2023-04-15 19:06:38,634 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1681227483635_0018 from user: hdfs, in queue: default
2023-04-15 19:06:38,634 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0018 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2023-04-15 19:06:38,634 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1681227483635_0018_000001
2023-04-15 19:06:38,634 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0018_000001 State change from NEW to SUBMITTED on event = START
2023-04-15 19:06:38,634 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1681227483635_0018 from user: hdfs activated in queue: default
2023-04-15 19:06:38,634 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1681227483635_0018 user: hdfs, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 2 #queue-pending-applications: 0 #queue-active-applications: 2
2023-04-15 19:06:38,634 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1681227483635_0018_000001 to scheduler from user hdfs in queue default
2023-04-15 19:06:38,635 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0018_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-15 19:06:38,785 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0018_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-15 19:06:38,785 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0018_01_000001 Container Transitioned from NEW to ALLOCATED
2023-04-15 19:06:38,785 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0018_01_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 2 containers, <memory:2048, vCores:2> used and <memory:5009, vCores:0> available after allocation
2023-04-15 19:06:38,785 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0018	CONTAINERID=container_e04_1681227483635_0018_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:06:38,786 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e04_1681227483635_0018_01_000001
2023-04-15 19:06:38,786 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0018_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-15 19:06:38,786 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1681227483635_0018_000001
2023-04-15 19:06:38,787 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1681227483635_0018 AttemptId: appattempt_1681227483635_0018_000001 MasterContainer: Container: [ContainerId: container_e04_1681227483635_0018_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ]
2023-04-15 19:06:38,787 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.0967361 absoluteUsedCapacity=0.0967361 used=<memory:2048, vCores:2> cluster=<memory:21171, vCores:6>
2023-04-15 19:06:38,787 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-15 19:06:38,787 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0018_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-15 19:06:38,787 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0018_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-15 19:06:38,788 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1681227483635_0018_000001
2023-04-15 19:06:38,788 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e04_1681227483635_0018_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0018_000001
2023-04-15 19:06:38,789 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1681227483635_0018_000001
2023-04-15 19:06:38,789 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1681227483635_0018_000001
2023-04-15 19:06:38,801 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e04_1681227483635_0018_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0018_000001
2023-04-15 19:06:38,801 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0018_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-15 19:06:39,786 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0018_01_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-15 19:06:45,072 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0018_000001 (auth:SIMPLE)
2023-04-15 19:06:45,080 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1681227483635_0018_000001
2023-04-15 19:06:45,080 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.5	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1681227483635_0018	APPATTEMPTID=appattempt_1681227483635_0018_000001
2023-04-15 19:06:45,081 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0018_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-15 19:06:45,081 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0018 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-15 19:07:15,605 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0017_01_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-15 19:07:15,605 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0017	CONTAINERID=container_e04_1681227483635_0017_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:07:15,605 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1681227483635_0017_000001 with final state: FAILED, and exit status: 10
2023-04-15 19:07:15,605 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0017_000001 State change from RUNNING to FINAL_SAVING on event = CONTAINER_FINISHED
2023-04-15 19:07:15,605 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1681227483635_0017_000001
2023-04-15 19:07:15,606 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1681227483635_0017_000001
2023-04-15 19:07:15,606 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0017_000001 State change from FINAL_SAVING to FAILED on event = ATTEMPT_UPDATE_SAVED
2023-04-15 19:07:15,606 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 1. The max attempts is 2
2023-04-15 19:07:15,606 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0017 State change from RUNNING to ACCEPTED on event = ATTEMPT_FAILED
2023-04-15 19:07:15,606 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1681227483635_0017_000001 is done. finalState=FAILED
2023-04-15 19:07:15,606 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1681227483635_0017_000002
2023-04-15 19:07:15,606 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1681227483635_0017 requests cleared
2023-04-15 19:07:15,606 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0017_000002 State change from NEW to SUBMITTED on event = START
2023-04-15 19:07:15,606 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1681227483635_0017 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-15 19:07:15,606 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1681227483635_0017_000001
2023-04-15 19:07:15,606 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1681227483635_0017 from user: hdfs activated in queue: default
2023-04-15 19:07:15,606 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1681227483635_0017 user: hdfs, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 2 #queue-pending-applications: 0 #queue-active-applications: 2
2023-04-15 19:07:15,606 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1681227483635_0017_000002 to scheduler from user hdfs in queue default
2023-04-15 19:07:15,607 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0017_000002 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-15 19:07:15,821 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0017_000002 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-15 19:07:15,821 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0017_02_000001 Container Transitioned from NEW to ALLOCATED
2023-04-15 19:07:15,821 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0017_02_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-04-15 19:07:15,821 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0017	CONTAINERID=container_e04_1681227483635_0017_02_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:07:15,822 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode1.ru-central1.internal:8041 for container : container_e04_1681227483635_0017_02_000001
2023-04-15 19:07:15,823 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0017_02_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-15 19:07:15,823 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1681227483635_0017_000002
2023-04-15 19:07:15,823 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1681227483635_0017 AttemptId: appattempt_1681227483635_0017_000002 MasterContainer: Container: [ContainerId: container_e04_1681227483635_0017_02_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode1.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode1.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.4:8041 }, ExecutionType: GUARANTEED, ]
2023-04-15 19:07:15,823 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.0967361 absoluteUsedCapacity=0.0967361 used=<memory:2048, vCores:2> cluster=<memory:21171, vCores:6>
2023-04-15 19:07:15,823 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-15 19:07:15,823 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0017_000002 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-15 19:07:15,824 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0017_000002 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-15 19:07:15,824 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1681227483635_0017_000002
2023-04-15 19:07:15,825 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e04_1681227483635_0017_02_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode1.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode1.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.4:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0017_000002
2023-04-15 19:07:15,825 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1681227483635_0017_000002
2023-04-15 19:07:15,825 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1681227483635_0017_000002
2023-04-15 19:07:15,837 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e04_1681227483635_0017_02_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode1.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode1.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.4:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0017_000002
2023-04-15 19:07:15,837 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0017_000002 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-15 19:07:16,822 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0017_02_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-15 19:07:19,714 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0017_000002 (auth:SIMPLE)
2023-04-15 19:07:19,723 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1681227483635_0017_000002
2023-04-15 19:07:19,723 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.4	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1681227483635_0017	APPATTEMPTID=appattempt_1681227483635_0017_000002
2023-04-15 19:07:19,723 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0017_000002 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-15 19:07:19,723 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0017 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-15 19:08:45,312 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0018_01_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-15 19:08:45,312 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0018	CONTAINERID=container_e04_1681227483635_0018_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:08:45,313 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1681227483635_0018_000001 with final state: FAILED, and exit status: 10
2023-04-15 19:08:45,313 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0018_000001 State change from RUNNING to FINAL_SAVING on event = CONTAINER_FINISHED
2023-04-15 19:08:45,314 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1681227483635_0018_000001
2023-04-15 19:08:45,314 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1681227483635_0018_000001
2023-04-15 19:08:45,314 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0018_000001 State change from FINAL_SAVING to FAILED on event = ATTEMPT_UPDATE_SAVED
2023-04-15 19:08:45,314 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 1. The max attempts is 2
2023-04-15 19:08:45,314 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0018 State change from RUNNING to ACCEPTED on event = ATTEMPT_FAILED
2023-04-15 19:08:45,314 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1681227483635_0018_000002
2023-04-15 19:08:45,314 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0018_000002 State change from NEW to SUBMITTED on event = START
2023-04-15 19:08:45,315 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1681227483635_0018_000001 is done. finalState=FAILED
2023-04-15 19:08:45,315 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1681227483635_0018 requests cleared
2023-04-15 19:08:45,315 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1681227483635_0018 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-15 19:08:45,315 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1681227483635_0018 from user: hdfs activated in queue: default
2023-04-15 19:08:45,315 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1681227483635_0018 user: hdfs, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 2 #queue-pending-applications: 0 #queue-active-applications: 2
2023-04-15 19:08:45,315 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1681227483635_0018_000002 to scheduler from user hdfs in queue default
2023-04-15 19:08:45,315 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0018_000002 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-15 19:08:45,316 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1681227483635_0018_000001
2023-04-15 19:08:45,417 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0018_000002 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-15 19:08:45,417 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0018_02_000001 Container Transitioned from NEW to ALLOCATED
2023-04-15 19:08:45,417 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0018_02_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-04-15 19:08:45,417 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0018	CONTAINERID=container_e04_1681227483635_0018_02_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:08:45,418 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.0967361 absoluteUsedCapacity=0.0967361 used=<memory:2048, vCores:2> cluster=<memory:21171, vCores:6>
2023-04-15 19:08:45,418 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-15 19:08:45,418 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_e04_1681227483635_0018_02_000001
2023-04-15 19:08:45,420 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0018_02_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-15 19:08:45,420 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1681227483635_0018_000002
2023-04-15 19:08:45,420 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1681227483635_0018 AttemptId: appattempt_1681227483635_0018_000002 MasterContainer: Container: [ContainerId: container_e04_1681227483635_0018_02_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ]
2023-04-15 19:08:45,421 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0018_000002 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-15 19:08:45,422 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0018_000002 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-15 19:08:45,422 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1681227483635_0018_000002
2023-04-15 19:08:45,424 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e04_1681227483635_0018_02_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0018_000002
2023-04-15 19:08:45,424 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1681227483635_0018_000002
2023-04-15 19:08:45,424 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1681227483635_0018_000002
2023-04-15 19:08:45,442 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e04_1681227483635_0018_02_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0018_000002
2023-04-15 19:08:45,442 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0018_000002 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-15 19:08:46,419 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0018_02_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-15 19:08:50,636 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0018_000002 (auth:SIMPLE)
2023-04-15 19:08:50,644 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1681227483635_0018_000002
2023-04-15 19:08:50,645 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.6	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1681227483635_0018	APPATTEMPTID=appattempt_1681227483635_0018_000002
2023-04-15 19:08:50,645 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0018_000002 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-15 19:08:50,645 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0018 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-15 19:09:19,813 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0017_000002 (auth:SIMPLE)
2023-04-15 19:09:19,818 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1681227483635_0017_000002 with final state: FINISHING, and exit status: -1000
2023-04-15 19:09:19,818 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0017_000002 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2023-04-15 19:09:19,818 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1681227483635_0017 with final state: FINISHING
2023-04-15 19:09:19,818 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0017 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2023-04-15 19:09:19,822 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1681227483635_0017
2023-04-15 19:09:19,822 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0017_000002 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2023-04-15 19:09:19,823 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0017 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2023-04-15 19:09:19,921 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1681227483635_0017 unregistered successfully. 
2023-04-15 19:09:20,763 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1681227483635_0017_000002
2023-04-15 19:09:20,763 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0017_02_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-15 19:09:20,763 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0017	CONTAINERID=container_e04_1681227483635_0017_02_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:09:20,763 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1681227483635_0017_000002
2023-04-15 19:09:20,764 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0017_000002 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2023-04-15 19:09:20,764 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0017 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2023-04-15 19:09:20,764 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1681227483635_0017_000002 is done. finalState=FINISHED
2023-04-15 19:09:20,764 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1681227483635_0017
2023-04-15 19:09:20,764 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1681227483635_0017 requests cleared
2023-04-15 19:09:20,765 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1681227483635_0017 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-15 19:09:20,765 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1681227483635_0017 user: hdfs leaf-queue of parent: root #applications: 1
2023-04-15 19:09:20,765 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1681227483635_0017,name=pyspark-shell,user=hdfs,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1681227483635_0017/,appMasterHost=10.130.0.4,submitTime=1681585507547,startTime=1681585507547,finishTime=1681585759818,finalStatus=FAILED,memorySeconds=258938,vcoreSeconds=251,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=SPARK,resourceSeconds=258938 MB-seconds\, 251 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2023-04-15 19:09:20,765 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1681227483635_0017_000002
2023-04-15 19:10:50,733 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0018_000002 (auth:SIMPLE)
2023-04-15 19:10:50,739 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1681227483635_0018_000002 with final state: FINISHING, and exit status: -1000
2023-04-15 19:10:50,739 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0018_000002 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2023-04-15 19:10:50,739 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1681227483635_0018 with final state: FINISHING
2023-04-15 19:10:50,739 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0018 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2023-04-15 19:10:50,739 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1681227483635_0018
2023-04-15 19:10:50,740 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0018_000002 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2023-04-15 19:10:50,740 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0018 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2023-04-15 19:10:50,843 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1681227483635_0018 unregistered successfully. 
2023-04-15 19:10:51,590 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1681227483635_0018_000002
2023-04-15 19:10:51,590 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0018_02_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-15 19:10:51,590 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1681227483635_0018_000002
2023-04-15 19:10:51,590 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0018_000002 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2023-04-15 19:10:51,590 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0018	CONTAINERID=container_e04_1681227483635_0018_02_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:10:51,591 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0018 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2023-04-15 19:10:51,591 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1681227483635_0018_000002 is done. finalState=FINISHED
2023-04-15 19:10:51,591 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1681227483635_0018
2023-04-15 19:10:51,591 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1681227483635_0018 requests cleared
2023-04-15 19:10:51,591 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1681227483635_0018 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-04-15 19:10:51,592 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1681227483635_0018 user: hdfs leaf-queue of parent: root #applications: 0
2023-04-15 19:10:51,592 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1681227483635_0018_000002
2023-04-15 19:10:51,592 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1681227483635_0018,name=pyspark-shell,user=hdfs,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1681227483635_0018/,appMasterHost=10.130.0.6,submitTime=1681585598632,startTime=1681585598632,finishTime=1681585850739,finalStatus=FAILED,memorySeconds=258764,vcoreSeconds=252,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=SPARK,resourceSeconds=258764 MB-seconds\, 252 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2023-04-15 19:19:51,134 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 19
2023-04-15 19:20:45,785 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 20
2023-04-15 19:21:44,029 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 21
2023-04-15 19:22:26,776 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1681227483635_0021' is submitted without priority hence considering default queue/cluster priority: 0
2023-04-15 19:22:26,776 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1681227483635_0021
2023-04-15 19:22:26,776 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 21 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-15 19:22:26,776 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 21 submitted by user hdfs
2023-04-15 19:22:26,776 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=192.168.126.6	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1681227483635_0021	QUEUENAME=default
2023-04-15 19:22:26,776 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1681227483635_0021
2023-04-15 19:22:26,777 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0021 State change from NEW to NEW_SAVING on event = START
2023-04-15 19:22:26,777 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1681227483635_0021
2023-04-15 19:22:26,777 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0021 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2023-04-15 19:22:26,777 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1681227483635_0021 user: hdfs leaf-queue of parent: root #applications: 1
2023-04-15 19:22:26,777 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1681227483635_0021 from user: hdfs, in queue: default
2023-04-15 19:22:26,777 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0021 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2023-04-15 19:22:26,777 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1681227483635_0021_000001
2023-04-15 19:22:26,777 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0021_000001 State change from NEW to SUBMITTED on event = START
2023-04-15 19:22:26,778 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1681227483635_0021 from user: hdfs activated in queue: default
2023-04-15 19:22:26,778 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1681227483635_0021 user: hdfs, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-15 19:22:26,778 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1681227483635_0021_000001 to scheduler from user hdfs in queue default
2023-04-15 19:22:26,778 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0021_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-15 19:22:27,326 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0021_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-15 19:22:27,327 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0021_01_000001 Container Transitioned from NEW to ALLOCATED
2023-04-15 19:22:27,327 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0021_01_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-04-15 19:22:27,327 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0021	CONTAINERID=container_e04_1681227483635_0021_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:22:27,327 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e04_1681227483635_0021_01_000001
2023-04-15 19:22:27,329 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0021_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-15 19:22:27,329 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1681227483635_0021_000001
2023-04-15 19:22:27,329 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1681227483635_0021 AttemptId: appattempt_1681227483635_0021_000001 MasterContainer: Container: [ContainerId: container_e04_1681227483635_0021_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ]
2023-04-15 19:22:27,329 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.04836805 absoluteUsedCapacity=0.04836805 used=<memory:1024, vCores:1> cluster=<memory:21171, vCores:6>
2023-04-15 19:22:27,329 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-15 19:22:27,329 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0021_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-15 19:22:27,330 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0021_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-15 19:22:27,331 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1681227483635_0021_000001
2023-04-15 19:22:27,332 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e04_1681227483635_0021_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0021_000001
2023-04-15 19:22:27,332 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1681227483635_0021_000001
2023-04-15 19:22:27,332 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1681227483635_0021_000001
2023-04-15 19:22:27,350 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e04_1681227483635_0021_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0021_000001
2023-04-15 19:22:27,351 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0021_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-15 19:22:28,328 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0021_01_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-15 19:22:31,116 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0021_000001 (auth:SIMPLE)
2023-04-15 19:22:31,126 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1681227483635_0021_000001
2023-04-15 19:22:31,126 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.5	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1681227483635_0021	APPATTEMPTID=appattempt_1681227483635_0021_000001
2023-04-15 19:22:31,127 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0021_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-15 19:22:31,127 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0021 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-15 19:24:31,404 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0021_01_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-15 19:24:31,404 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0021	CONTAINERID=container_e04_1681227483635_0021_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:24:31,405 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1681227483635_0021_000001 with final state: FAILED, and exit status: 10
2023-04-15 19:24:31,405 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0021_000001 State change from RUNNING to FINAL_SAVING on event = CONTAINER_FINISHED
2023-04-15 19:24:31,405 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1681227483635_0021_000001
2023-04-15 19:24:31,405 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1681227483635_0021_000001
2023-04-15 19:24:31,405 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0021_000001 State change from FINAL_SAVING to FAILED on event = ATTEMPT_UPDATE_SAVED
2023-04-15 19:24:31,405 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 1. The max attempts is 2
2023-04-15 19:24:31,405 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0021 State change from RUNNING to ACCEPTED on event = ATTEMPT_FAILED
2023-04-15 19:24:31,405 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1681227483635_0021_000002
2023-04-15 19:24:31,405 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0021_000002 State change from NEW to SUBMITTED on event = START
2023-04-15 19:24:31,406 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1681227483635_0021_000001 is done. finalState=FAILED
2023-04-15 19:24:31,406 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1681227483635_0021 requests cleared
2023-04-15 19:24:31,406 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1681227483635_0021_000001
2023-04-15 19:24:31,406 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1681227483635_0021 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-04-15 19:24:31,406 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1681227483635_0021 from user: hdfs activated in queue: default
2023-04-15 19:24:31,406 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1681227483635_0021 user: hdfs, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-15 19:24:31,406 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1681227483635_0021_000002 to scheduler from user hdfs in queue default
2023-04-15 19:24:31,407 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0021_000002 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-15 19:24:31,442 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0021_000002 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-15 19:24:31,442 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0021_02_000001 Container Transitioned from NEW to ALLOCATED
2023-04-15 19:24:31,442 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0021_02_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-04-15 19:24:31,442 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0021	CONTAINERID=container_e04_1681227483635_0021_02_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:24:31,443 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode1.ru-central1.internal:8041 for container : container_e04_1681227483635_0021_02_000001
2023-04-15 19:24:31,444 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0021_02_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-15 19:24:31,444 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1681227483635_0021_000002
2023-04-15 19:24:31,444 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1681227483635_0021 AttemptId: appattempt_1681227483635_0021_000002 MasterContainer: Container: [ContainerId: container_e04_1681227483635_0021_02_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode1.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode1.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.4:8041 }, ExecutionType: GUARANTEED, ]
2023-04-15 19:24:31,444 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.04836805 absoluteUsedCapacity=0.04836805 used=<memory:1024, vCores:1> cluster=<memory:21171, vCores:6>
2023-04-15 19:24:31,444 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-15 19:24:31,444 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0021_000002 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-15 19:24:31,445 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0021_000002 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-15 19:24:31,445 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1681227483635_0021_000002
2023-04-15 19:24:31,446 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e04_1681227483635_0021_02_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode1.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode1.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.4:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0021_000002
2023-04-15 19:24:31,446 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1681227483635_0021_000002
2023-04-15 19:24:31,446 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1681227483635_0021_000002
2023-04-15 19:24:31,462 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e04_1681227483635_0021_02_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode1.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode1.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.4:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0021_000002
2023-04-15 19:24:31,463 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0021_000002 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-15 19:24:32,443 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0021_02_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-15 19:24:35,367 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0021_000002 (auth:SIMPLE)
2023-04-15 19:24:35,380 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1681227483635_0021_000002
2023-04-15 19:24:35,380 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.4	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1681227483635_0021	APPATTEMPTID=appattempt_1681227483635_0021_000002
2023-04-15 19:24:35,380 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0021_000002 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-15 19:24:35,380 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0021 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-15 19:26:35,478 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0021_000002 (auth:SIMPLE)
2023-04-15 19:26:35,491 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1681227483635_0021_000002 with final state: FINISHING, and exit status: -1000
2023-04-15 19:26:35,491 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0021_000002 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2023-04-15 19:26:35,491 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1681227483635_0021 with final state: FINISHING
2023-04-15 19:26:35,491 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0021 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2023-04-15 19:26:35,491 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1681227483635_0021
2023-04-15 19:26:35,491 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0021_000002 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2023-04-15 19:26:35,492 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0021 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2023-04-15 19:26:35,594 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1681227483635_0021 unregistered successfully. 
2023-04-15 19:26:36,415 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1681227483635_0021_000002
2023-04-15 19:26:36,415 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0021_02_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-15 19:26:36,416 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1681227483635_0021_000002
2023-04-15 19:26:36,416 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0021	CONTAINERID=container_e04_1681227483635_0021_02_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:26:36,416 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0021_000002 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2023-04-15 19:26:36,417 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0021 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2023-04-15 19:26:36,417 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1681227483635_0021_000002 is done. finalState=FINISHED
2023-04-15 19:26:36,417 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1681227483635_0021 requests cleared
2023-04-15 19:26:36,417 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1681227483635_0021
2023-04-15 19:26:36,417 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1681227483635_0021 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-04-15 19:26:36,417 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1681227483635_0021 user: hdfs leaf-queue of parent: root #applications: 0
2023-04-15 19:26:36,417 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1681227483635_0021,name=pyspark-shell,user=hdfs,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1681227483635_0021/,appMasterHost=10.130.0.4,submitTime=1681586546775,startTime=1681586546776,finishTime=1681586795491,finalStatus=FAILED,memorySeconds=255027,vcoreSeconds=248,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=SPARK,resourceSeconds=255027 MB-seconds\, 248 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2023-04-15 19:26:36,417 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1681227483635_0021_000002
2023-04-15 19:38:03,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 19:38:03,823 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 28 msec
2023-04-15 19:42:45,597 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 22
2023-04-15 19:43:27,840 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1681227483635_0022' is submitted without priority hence considering default queue/cluster priority: 0
2023-04-15 19:43:27,840 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1681227483635_0022
2023-04-15 19:43:27,840 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 22 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-15 19:43:27,840 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 22 submitted by user hdfs
2023-04-15 19:43:27,840 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=192.168.126.6	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1681227483635_0022	QUEUENAME=default
2023-04-15 19:43:27,841 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1681227483635_0022
2023-04-15 19:43:27,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1681227483635_0022
2023-04-15 19:43:27,841 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0022 State change from NEW to NEW_SAVING on event = START
2023-04-15 19:43:27,841 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0022 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2023-04-15 19:43:27,841 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1681227483635_0022 user: hdfs leaf-queue of parent: root #applications: 1
2023-04-15 19:43:27,841 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1681227483635_0022 from user: hdfs, in queue: default
2023-04-15 19:43:27,841 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0022 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2023-04-15 19:43:27,841 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1681227483635_0022_000001
2023-04-15 19:43:27,841 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0022_000001 State change from NEW to SUBMITTED on event = START
2023-04-15 19:43:27,842 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1681227483635_0022 from user: hdfs activated in queue: default
2023-04-15 19:43:27,842 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1681227483635_0022 user: hdfs, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-15 19:43:27,842 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1681227483635_0022_000001 to scheduler from user hdfs in queue default
2023-04-15 19:43:27,843 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0022_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-15 19:43:28,106 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0022_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-15 19:43:28,107 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0022_01_000001 Container Transitioned from NEW to ALLOCATED
2023-04-15 19:43:28,107 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0022_01_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode3.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-04-15 19:43:28,107 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0022	CONTAINERID=container_e04_1681227483635_0022_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:43:28,107 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode3.ru-central1.internal:8041 for container : container_e04_1681227483635_0022_01_000001
2023-04-15 19:43:28,108 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0022_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-15 19:43:28,108 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1681227483635_0022_000001
2023-04-15 19:43:28,108 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1681227483635_0022 AttemptId: appattempt_1681227483635_0022_000001 MasterContainer: Container: [ContainerId: container_e04_1681227483635_0022_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ]
2023-04-15 19:43:28,108 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.04836805 absoluteUsedCapacity=0.04836805 used=<memory:1024, vCores:1> cluster=<memory:21171, vCores:6>
2023-04-15 19:43:28,108 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-15 19:43:28,109 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0022_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-15 19:43:28,110 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0022_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-15 19:43:28,110 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1681227483635_0022_000001
2023-04-15 19:43:28,112 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e04_1681227483635_0022_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0022_000001
2023-04-15 19:43:28,112 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1681227483635_0022_000001
2023-04-15 19:43:28,112 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1681227483635_0022_000001
2023-04-15 19:43:28,136 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e04_1681227483635_0022_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode3.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode3.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.6:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0022_000001
2023-04-15 19:43:28,136 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0022_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-15 19:43:29,108 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0022_01_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-15 19:43:34,163 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0022_000001 (auth:SIMPLE)
2023-04-15 19:43:34,171 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1681227483635_0022_000001
2023-04-15 19:43:34,171 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.6	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1681227483635_0022	APPATTEMPTID=appattempt_1681227483635_0022_000001
2023-04-15 19:43:34,171 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0022_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-15 19:43:34,172 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0022 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-15 19:45:34,408 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0022_01_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-15 19:45:34,408 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0022	CONTAINERID=container_e04_1681227483635_0022_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:45:34,408 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1681227483635_0022_000001 with final state: FAILED, and exit status: 10
2023-04-15 19:45:34,408 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0022_000001 State change from RUNNING to FINAL_SAVING on event = CONTAINER_FINISHED
2023-04-15 19:45:34,409 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1681227483635_0022_000001
2023-04-15 19:45:34,409 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1681227483635_0022_000001
2023-04-15 19:45:34,409 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0022_000001 State change from FINAL_SAVING to FAILED on event = ATTEMPT_UPDATE_SAVED
2023-04-15 19:45:34,409 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 1. The max attempts is 2
2023-04-15 19:45:34,409 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0022 State change from RUNNING to ACCEPTED on event = ATTEMPT_FAILED
2023-04-15 19:45:34,409 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1681227483635_0022_000001 is done. finalState=FAILED
2023-04-15 19:45:34,409 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1681227483635_0022_000002
2023-04-15 19:45:34,409 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1681227483635_0022 requests cleared
2023-04-15 19:45:34,409 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1681227483635_0022 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-04-15 19:45:34,409 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0022_000002 State change from NEW to SUBMITTED on event = START
2023-04-15 19:45:34,410 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1681227483635_0022 from user: hdfs activated in queue: default
2023-04-15 19:45:34,410 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1681227483635_0022 user: hdfs, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-15 19:45:34,410 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1681227483635_0022_000002 to scheduler from user hdfs in queue default
2023-04-15 19:45:34,410 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1681227483635_0022_000001
2023-04-15 19:45:34,410 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0022_000002 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-15 19:45:34,911 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0022_000002 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-15 19:45:34,911 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0022_02_000001 Container Transitioned from NEW to ALLOCATED
2023-04-15 19:45:34,911 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0022_02_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-04-15 19:45:34,911 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0022	CONTAINERID=container_e04_1681227483635_0022_02_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:45:34,912 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e04_1681227483635_0022_02_000001
2023-04-15 19:45:34,913 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0022_02_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-15 19:45:34,913 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1681227483635_0022_000002
2023-04-15 19:45:34,913 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1681227483635_0022 AttemptId: appattempt_1681227483635_0022_000002 MasterContainer: Container: [ContainerId: container_e04_1681227483635_0022_02_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ]
2023-04-15 19:45:34,913 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.04836805 absoluteUsedCapacity=0.04836805 used=<memory:1024, vCores:1> cluster=<memory:21171, vCores:6>
2023-04-15 19:45:34,913 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-15 19:45:34,913 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0022_000002 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-15 19:45:34,914 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0022_000002 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-15 19:45:34,914 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1681227483635_0022_000002
2023-04-15 19:45:34,915 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e04_1681227483635_0022_02_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0022_000002
2023-04-15 19:45:34,915 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1681227483635_0022_000002
2023-04-15 19:45:34,915 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1681227483635_0022_000002
2023-04-15 19:45:34,928 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e04_1681227483635_0022_02_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0022_000002
2023-04-15 19:45:34,928 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0022_000002 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-15 19:45:35,912 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0022_02_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-15 19:45:39,093 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0022_000002 (auth:SIMPLE)
2023-04-15 19:45:39,104 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1681227483635_0022_000002
2023-04-15 19:45:39,104 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.5	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1681227483635_0022	APPATTEMPTID=appattempt_1681227483635_0022_000002
2023-04-15 19:45:39,104 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0022_000002 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-15 19:45:39,104 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0022 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-15 19:47:39,182 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0022_000002 (auth:SIMPLE)
2023-04-15 19:47:39,190 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1681227483635_0022_000002 with final state: FINISHING, and exit status: -1000
2023-04-15 19:47:39,190 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0022_000002 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2023-04-15 19:47:39,190 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1681227483635_0022 with final state: FINISHING
2023-04-15 19:47:39,190 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0022 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2023-04-15 19:47:39,190 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1681227483635_0022
2023-04-15 19:47:39,190 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0022_000002 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2023-04-15 19:47:39,190 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0022 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2023-04-15 19:47:39,293 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1681227483635_0022 unregistered successfully. 
2023-04-15 19:47:40,026 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1681227483635_0022_000002
2023-04-15 19:47:40,026 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0022_02_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-15 19:47:40,026 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1681227483635_0022_000002
2023-04-15 19:47:40,026 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0022	CONTAINERID=container_e04_1681227483635_0022_02_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:47:40,026 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0022_000002 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2023-04-15 19:47:40,027 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0022 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2023-04-15 19:47:40,027 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1681227483635_0022_000002 is done. finalState=FINISHED
2023-04-15 19:47:40,027 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1681227483635_0022
2023-04-15 19:47:40,027 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1681227483635_0022 requests cleared
2023-04-15 19:47:40,028 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1681227483635_0022 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-04-15 19:47:40,028 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1681227483635_0022 user: hdfs leaf-queue of parent: root #applications: 0
2023-04-15 19:47:40,028 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1681227483635_0022,name=pyspark-shell,user=hdfs,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1681227483635_0022/,appMasterHost=10.130.0.5,submitTime=1681587807840,startTime=1681587807840,finishTime=1681588059190,finalStatus=FAILED,memorySeconds=257451,vcoreSeconds=251,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=SPARK,resourceSeconds=257451 MB-seconds\, 251 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2023-04-15 19:47:40,028 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1681227483635_0022_000002
2023-04-15 19:51:14,326 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: 23
2023-04-15 19:51:56,391 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application 'application_1681227483635_0023' is submitted without priority hence considering default queue/cluster priority: 0
2023-04-15 19:51:56,391 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1681227483635_0023
2023-04-15 19:51:56,391 WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: 0 for application: 23 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.
2023-04-15 19:51:56,391 INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id 23 submitted by user hdfs
2023-04-15 19:51:56,391 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=192.168.126.6	OPERATION=Submit Application Request	TARGET=ClientRMService	RESULT=SUCCESS	APPID=application_1681227483635_0023	QUEUENAME=default
2023-04-15 19:51:56,392 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id application_1681227483635_0023
2023-04-15 19:51:56,392 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: application_1681227483635_0023
2023-04-15 19:51:56,392 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0023 State change from NEW to NEW_SAVING on event = START
2023-04-15 19:51:56,393 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0023 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED
2023-04-15 19:51:56,393 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added - appId: application_1681227483635_0023 user: hdfs leaf-queue of parent: root #applications: 1
2023-04-15 19:51:56,393 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application application_1681227483635_0023 from user: hdfs, in queue: default
2023-04-15 19:51:56,393 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0023 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED
2023-04-15 19:51:56,393 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1681227483635_0023_000001
2023-04-15 19:51:56,393 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0023_000001 State change from NEW to SUBMITTED on event = START
2023-04-15 19:51:56,393 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1681227483635_0023 from user: hdfs activated in queue: default
2023-04-15 19:51:56,393 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1681227483635_0023 user: hdfs, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-15 19:51:56,393 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1681227483635_0023_000001 to scheduler from user hdfs in queue default
2023-04-15 19:51:56,394 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0023_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-15 19:51:56,535 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0023_000001 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-15 19:51:56,535 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0023_01_000001 Container Transitioned from NEW to ALLOCATED
2023-04-15 19:51:56,535 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0023_01_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode1.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-04-15 19:51:56,535 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0023	CONTAINERID=container_e04_1681227483635_0023_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:51:56,536 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode1.ru-central1.internal:8041 for container : container_e04_1681227483635_0023_01_000001
2023-04-15 19:51:56,537 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0023_01_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-15 19:51:56,537 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1681227483635_0023_000001
2023-04-15 19:51:56,537 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1681227483635_0023 AttemptId: appattempt_1681227483635_0023_000001 MasterContainer: Container: [ContainerId: container_e04_1681227483635_0023_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode1.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode1.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.4:8041 }, ExecutionType: GUARANTEED, ]
2023-04-15 19:51:56,537 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.04836805 absoluteUsedCapacity=0.04836805 used=<memory:1024, vCores:1> cluster=<memory:21171, vCores:6>
2023-04-15 19:51:56,537 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-15 19:51:56,537 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0023_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-15 19:51:56,538 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0023_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-15 19:51:56,539 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1681227483635_0023_000001
2023-04-15 19:51:56,540 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e04_1681227483635_0023_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode1.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode1.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.4:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0023_000001
2023-04-15 19:51:56,540 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1681227483635_0023_000001
2023-04-15 19:51:56,540 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1681227483635_0023_000001
2023-04-15 19:51:56,555 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e04_1681227483635_0023_01_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode1.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode1.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.4:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0023_000001
2023-04-15 19:51:56,556 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0023_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-15 19:51:57,536 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0023_01_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-15 19:52:00,794 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0023_000001 (auth:SIMPLE)
2023-04-15 19:52:00,805 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1681227483635_0023_000001
2023-04-15 19:52:00,805 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.4	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1681227483635_0023	APPATTEMPTID=appattempt_1681227483635_0023_000001
2023-04-15 19:52:00,805 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0023_000001 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-15 19:52:00,805 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0023 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-15 19:54:01,055 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0023_01_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-15 19:54:01,055 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0023	CONTAINERID=container_e04_1681227483635_0023_01_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:54:01,055 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1681227483635_0023_000001 with final state: FAILED, and exit status: 10
2023-04-15 19:54:01,055 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0023_000001 State change from RUNNING to FINAL_SAVING on event = CONTAINER_FINISHED
2023-04-15 19:54:01,055 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1681227483635_0023_000001
2023-04-15 19:54:01,055 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1681227483635_0023_000001
2023-04-15 19:54:01,055 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0023_000001 State change from FINAL_SAVING to FAILED on event = ATTEMPT_UPDATE_SAVED
2023-04-15 19:54:01,055 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The number of failed attempts is 1. The max attempts is 2
2023-04-15 19:54:01,056 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0023 State change from RUNNING to ACCEPTED on event = ATTEMPT_FAILED
2023-04-15 19:54:01,056 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1681227483635_0023_000001 is done. finalState=FAILED
2023-04-15 19:54:01,056 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1681227483635_0023_000002
2023-04-15 19:54:01,056 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0023_000002 State change from NEW to SUBMITTED on event = START
2023-04-15 19:54:01,056 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1681227483635_0023 requests cleared
2023-04-15 19:54:01,056 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1681227483635_0023 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-04-15 19:54:01,056 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1681227483635_0023_000001
2023-04-15 19:54:01,056 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application application_1681227483635_0023 from user: hdfs activated in queue: default
2023-04-15 19:54:01,056 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added - appId: application_1681227483635_0023 user: hdfs, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1
2023-04-15 19:54:01,056 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt appattempt_1681227483635_0023_000002 to scheduler from user hdfs in queue default
2023-04-15 19:54:01,056 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0023_000002 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED
2023-04-15 19:54:01,475 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1681227483635_0023_000002 container=null queue=default clusterResource=<memory:21171, vCores:6> type=OFF_SWITCH requestedPartition=
2023-04-15 19:54:01,475 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0023_02_000001 Container Transitioned from NEW to ALLOCATED
2023-04-15 19:54:01,475 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: Assigned container container_e04_1681227483635_0023_02_000001 of capacity <memory:1024, vCores:1> on host adh-test-00-mnode2.ru-central1.internal:8041, which has 1 containers, <memory:1024, vCores:1> used and <memory:6033, vCores:1> available after allocation
2023-04-15 19:54:01,475 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Allocated Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0023	CONTAINERID=container_e04_1681227483635_0023_02_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:54:01,476 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : adh-test-00-mnode2.ru-central1.internal:8041 for container : container_e04_1681227483635_0023_02_000001
2023-04-15 19:54:01,477 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0023_02_000001 Container Transitioned from ALLOCATED to ACQUIRED
2023-04-15 19:54:01,477 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for appattempt_1681227483635_0023_000002
2023-04-15 19:54:01,477 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1681227483635_0023 AttemptId: appattempt_1681227483635_0023_000002 MasterContainer: Container: [ContainerId: container_e04_1681227483635_0023_02_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ]
2023-04-15 19:54:01,477 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.04836805 absoluteUsedCapacity=0.04836805 used=<memory:1024, vCores:1> cluster=<memory:21171, vCores:6>
2023-04-15 19:54:01,477 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Allocation proposal accepted
2023-04-15 19:54:01,477 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0023_000002 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED
2023-04-15 19:54:01,478 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0023_000002 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED
2023-04-15 19:54:01,479 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching masterappattempt_1681227483635_0023_000002
2023-04-15 19:54:01,479 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_e04_1681227483635_0023_02_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0023_000002
2023-04-15 19:54:01,479 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1681227483635_0023_000002
2023-04-15 19:54:01,479 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for appattempt_1681227483635_0023_000002
2023-04-15 19:54:01,495 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_e04_1681227483635_0023_02_000001, AllocationRequestId: -1, Version: 0, NodeId: adh-test-00-mnode2.ru-central1.internal:8041, NodeHttpAddress: adh-test-00-mnode2.ru-central1.internal:8042, Resource: <memory:1024, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.130.0.5:8041 }, ExecutionType: GUARANTEED, ] for AM appattempt_1681227483635_0023_000002
2023-04-15 19:54:01,495 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0023_000002 State change from ALLOCATED to LAUNCHED on event = LAUNCHED
2023-04-15 19:54:02,476 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0023_02_000001 Container Transitioned from ACQUIRED to RUNNING
2023-04-15 19:54:05,961 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0023_000002 (auth:SIMPLE)
2023-04-15 19:54:05,970 INFO org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor: AM registration appattempt_1681227483635_0023_000002
2023-04-15 19:54:05,970 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	IP=10.130.0.5	OPERATION=Register App Master	TARGET=ApplicationMasterService	RESULT=SUCCESS	APPID=application_1681227483635_0023	APPATTEMPTID=appattempt_1681227483635_0023_000002
2023-04-15 19:54:05,971 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0023_000002 State change from LAUNCHED to RUNNING on event = REGISTERED
2023-04-15 19:54:05,971 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0023 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED
2023-04-15 19:56:06,060 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1681227483635_0023_000002 (auth:SIMPLE)
2023-04-15 19:56:06,067 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt appattempt_1681227483635_0023_000002 with final state: FINISHING, and exit status: -1000
2023-04-15 19:56:06,067 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0023_000002 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED
2023-04-15 19:56:06,067 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application application_1681227483635_0023 with final state: FINISHING
2023-04-15 19:56:06,067 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0023 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED
2023-04-15 19:56:06,067 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: application_1681227483635_0023
2023-04-15 19:56:06,067 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0023_000002 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED
2023-04-15 19:56:06,067 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0023 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED
2023-04-15 19:56:06,171 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: application_1681227483635_0023 unregistered successfully. 
2023-04-15 19:56:06,960 INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1681227483635_0023_000002
2023-04-15 19:56:06,960 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1681227483635_0023_02_000001 Container Transitioned from RUNNING to COMPLETED
2023-04-15 19:56:06,961 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1681227483635_0023_000002
2023-04-15 19:56:06,961 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=AM Released Container	TARGET=SchedulerApp	RESULT=SUCCESS	APPID=application_1681227483635_0023	CONTAINERID=container_e04_1681227483635_0023_02_000001	RESOURCE=<memory:1024, vCores:1>	QUEUENAME=default
2023-04-15 19:56:06,961 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1681227483635_0023_000002 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED
2023-04-15 19:56:06,962 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1681227483635_0023 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED
2023-04-15 19:56:06,962 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=hdfs	OPERATION=Application Finished - Succeeded	TARGET=RMAppManager	RESULT=SUCCESS	APPID=application_1681227483635_0023
2023-04-15 19:56:06,962 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt appattempt_1681227483635_0023_000002 is done. finalState=FINISHED
2023-04-15 19:56:06,962 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: appId=application_1681227483635_0023,name=pyspark-shell,user=hdfs,queue=default,state=FINISHED,trackingUrl=http://adh-test-00-mnode1.ru-central1.internal:8088/proxy/application_1681227483635_0023/,appMasterHost=10.130.0.5,submitTime=1681588316391,startTime=1681588316391,finishTime=1681588566067,finalStatus=FAILED,memorySeconds=256003,vcoreSeconds=249,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\, vCores:0>,applicationType=SPARK,resourceSeconds=256003 MB-seconds\, 249 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\, 0 vcore-seconds
2023-04-15 19:56:06,962 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application application_1681227483635_0023 requests cleared
2023-04-15 19:56:06,962 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed - appId: application_1681227483635_0023 user: hdfs queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0
2023-04-15 19:56:06,962 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed - appId: application_1681227483635_0023 user: hdfs leaf-queue of parent: root #applications: 0
2023-04-15 19:56:06,962 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning master appattempt_1681227483635_0023_000002
2023-04-15 20:38:03,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 20:38:03,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 30 msec
2023-04-15 21:38:03,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 21:38:03,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-15 22:38:03,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 22:38:03,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-04-15 23:38:03,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-15 23:38:03,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-16 00:38:03,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 00:38:03,812 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-16 01:38:03,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 01:38:03,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-04-16 02:38:03,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 02:38:03,800 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-16 03:38:03,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 03:38:03,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-16 04:38:03,795 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 04:38:03,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-16 05:38:03,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 05:38:03,800 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-16 06:38:03,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 06:38:03,800 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-16 07:38:03,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 07:38:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-16 08:38:03,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 08:38:03,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-16 09:38:03,796 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 09:38:03,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-04-16 10:38:03,797 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 10:38:03,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-04-16 11:38:03,797 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 11:38:03,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-16 12:38:03,797 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 12:38:03,822 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-04-16 13:38:03,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 13:38:03,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-04-16 14:38:03,798 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 14:38:03,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-16 15:38:03,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 15:38:03,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-16 15:38:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-04-16 15:38:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-16 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-16 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id -1018473535 in 900000ms
2023-04-16 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-16 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id 1120025997 in 900000ms
2023-04-16 15:38:14,368 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-16 15:38:15,790 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 107
2023-04-16 15:38:15,791 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-16 15:38:15,791 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-16 15:38:15,791 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 119
2023-04-16 15:38:15,791 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-16 15:53:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: -490698609
2023-04-16 15:53:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-16 15:53:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: -1018473535
2023-04-16 15:53:03,986 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: 1120025997
2023-04-16 16:38:03,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 16:38:03,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 26 msec
2023-04-16 17:38:03,799 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 17:38:03,822 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-04-16 18:38:03,800 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 18:38:03,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-16 19:38:03,801 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 19:38:03,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-16 20:38:03,801 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 20:38:03,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-04-16 21:38:03,801 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 21:38:03,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-16 22:38:03,801 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 22:38:03,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-16 23:38:03,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-16 23:38:03,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-17 00:38:03,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 00:38:03,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-04-17 01:38:03,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 01:38:03,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-17 02:38:03,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 02:38:03,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-17 03:38:03,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 03:38:03,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-04-17 04:38:03,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 04:38:03,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-17 05:38:03,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 05:38:03,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-17 06:38:03,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 06:38:03,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-17 07:38:03,802 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 07:38:03,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-17 08:38:03,803 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 08:38:03,823 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-04-17 09:38:03,803 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 09:38:03,838 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 35 msec
2023-04-17 10:38:03,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 10:38:03,822 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-04-17 11:38:03,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 11:38:03,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 30 msec
2023-04-17 12:38:03,804 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 12:38:03,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-17 13:38:03,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 13:38:03,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-17 14:38:03,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 14:38:03,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-17 15:38:03,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 15:38:03,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-17 15:38:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-04-17 15:38:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-17 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-17 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id 1120025998 in 900000ms
2023-04-17 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-17 15:38:03,986 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id -1018473534 in 900000ms
2023-04-17 15:38:16,613 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-17 15:38:18,109 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 108
2023-04-17 15:38:18,109 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-17 15:38:18,110 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-17 15:38:18,110 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 120
2023-04-17 15:38:18,110 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-17 15:53:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: -490698608
2023-04-17 15:53:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-17 15:53:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: 1120025998
2023-04-17 15:53:03,986 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: -1018473534
2023-04-17 16:38:03,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 16:38:03,836 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 31 msec
2023-04-17 17:38:03,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 17:38:03,812 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-17 18:38:03,805 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 18:38:03,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-17 19:38:03,806 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 19:38:03,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-17 20:38:03,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 20:38:03,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-17 21:38:03,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 21:38:03,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-17 22:38:03,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 22:38:03,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-17 23:38:03,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-17 23:38:03,831 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-04-18 00:38:03,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 00:38:03,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 26 msec
2023-04-18 01:38:03,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 01:38:03,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-04-18 02:38:03,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 02:38:03,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-18 03:38:03,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 03:38:03,822 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-18 04:38:03,807 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 04:38:03,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-04-18 05:38:03,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 05:38:03,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-04-18 06:38:03,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 06:38:03,823 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-04-18 07:38:03,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 07:38:03,812 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-18 08:38:03,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 08:38:03,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-04-18 09:38:03,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 09:38:03,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-18 10:38:03,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 10:38:03,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 2 msec
2023-04-18 11:38:03,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 11:38:03,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-18 12:38:03,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 12:38:03,845 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 38 msec
2023-04-18 13:38:03,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 13:38:03,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-04-18 14:38:03,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 14:38:03,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-18 15:38:03,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 15:38:03,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-18 15:38:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-04-18 15:38:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-18 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-18 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id -1018473533 in 900000ms
2023-04-18 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-18 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id 1120025999 in 900000ms
2023-04-18 15:38:18,979 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-18 15:38:20,419 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 113
2023-04-18 15:38:20,419 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-18 15:38:20,419 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 109
2023-04-18 15:38:20,419 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-18 15:38:20,419 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-18 15:38:20,419 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 121
2023-04-18 15:38:20,419 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-18 15:53:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: -490698607
2023-04-18 15:53:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-18 15:53:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: -1018473533
2023-04-18 15:53:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: 1120025999
2023-04-18 16:38:03,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 16:38:03,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-04-18 17:38:03,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 17:38:03,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-18 18:38:03,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 18:38:03,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-18 19:38:03,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 19:38:03,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-18 20:38:03,808 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 20:38:03,823 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-18 21:38:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 21:38:03,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-18 22:38:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 22:38:03,835 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 26 msec
2023-04-18 23:38:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-18 23:38:03,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-19 00:38:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 00:38:03,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 40 msec
2023-04-19 01:38:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 01:38:03,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-19 02:38:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 02:38:03,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-19 03:38:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 03:38:03,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-19 04:38:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 04:38:03,822 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-04-19 05:38:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 05:38:03,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-04-19 06:38:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 06:38:03,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-19 07:38:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 07:38:03,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-04-19 08:38:03,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 08:38:03,836 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 27 msec
2023-04-19 09:38:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 09:38:03,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-19 10:38:03,809 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 10:38:03,824 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-19 11:38:03,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 11:38:03,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-19 12:38:03,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 12:38:03,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-19 13:38:03,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 13:38:03,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-19 14:38:03,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 14:38:03,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-04-19 15:38:03,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 15:38:03,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-04-19 15:38:03,983 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-04-19 15:38:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-19 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-19 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id -1018473532 in 900000ms
2023-04-19 15:38:03,986 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-19 15:38:03,986 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id 1120026000 in 900000ms
2023-04-19 15:38:21,237 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-19 15:38:22,696 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 112
2023-04-19 15:38:22,696 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-19 15:38:22,696 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 114
2023-04-19 15:38:22,697 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-19 15:38:22,697 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 110
2023-04-19 15:38:22,697 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-19 15:38:22,697 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 111
2023-04-19 15:38:22,697 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-19 15:38:22,697 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-19 15:38:22,697 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 122
2023-04-19 15:38:22,697 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-19 15:53:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: -490698606
2023-04-19 15:53:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-19 15:53:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: -1018473532
2023-04-19 15:53:03,986 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: 1120026000
2023-04-19 16:38:03,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 16:38:03,847 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 38 msec
2023-04-19 17:38:03,810 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 17:38:03,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-19 18:38:03,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 18:38:03,829 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-04-19 19:38:03,811 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 19:38:03,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-19 20:38:03,812 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 20:38:03,840 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 28 msec
2023-04-19 21:38:03,812 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 21:38:03,824 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-19 22:38:03,812 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 22:38:03,823 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-19 23:38:03,812 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-19 23:38:03,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-04-20 00:38:03,812 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 00:38:03,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-20 01:38:03,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 01:38:03,831 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-04-20 02:38:03,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 02:38:03,829 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-20 03:38:03,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 03:38:03,823 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-20 04:38:03,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 04:38:03,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-20 05:38:03,813 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 05:38:03,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-20 06:38:03,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 06:38:03,838 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-04-20 07:38:03,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 07:38:03,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-20 08:38:03,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 08:38:03,822 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-20 09:38:03,814 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 09:38:03,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-20 10:38:03,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 10:38:03,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-20 11:38:03,815 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 11:38:03,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-20 12:38:03,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 12:38:03,839 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 24 msec
2023-04-20 13:38:03,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 13:38:03,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-20 14:38:03,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 14:38:03,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-20 15:38:03,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 15:38:03,853 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 36 msec
2023-04-20 15:38:03,983 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-04-20 15:38:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-20 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-20 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-20 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id 1120026001 in 900000ms
2023-04-20 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id -1018473531 in 900000ms
2023-04-20 15:38:23,593 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-20 15:38:25,076 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 115
2023-04-20 15:38:25,076 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-20 15:38:25,076 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-20 15:38:25,076 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 123
2023-04-20 15:38:25,076 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-20 15:53:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: -490698605
2023-04-20 15:53:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-20 15:53:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: -1018473531
2023-04-20 15:53:03,986 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: 1120026001
2023-04-20 16:38:03,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 16:38:03,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-20 17:38:03,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 17:38:03,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-04-20 18:38:03,816 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 18:38:03,838 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-04-20 19:38:03,817 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 19:38:03,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-04-20 20:38:03,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 20:38:03,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-04-20 21:38:03,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 21:38:03,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-20 22:38:03,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 22:38:03,824 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-20 23:38:03,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-20 23:38:03,838 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-04-21 00:38:03,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 00:38:03,826 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-21 01:38:03,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 01:38:03,823 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-21 02:38:03,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 02:38:03,824 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-21 03:38:03,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 03:38:03,836 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-04-21 04:38:03,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 04:38:03,840 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 22 msec
2023-04-21 05:38:03,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 05:38:03,838 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-04-21 06:38:03,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 06:38:03,836 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 18 msec
2023-04-21 07:38:03,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 07:38:03,824 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-21 08:38:03,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 08:38:03,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-21 09:38:03,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 09:38:03,838 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-04-21 10:38:03,818 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 10:38:03,835 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 17 msec
2023-04-21 11:38:03,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 11:38:03,831 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-21 12:38:03,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 12:38:03,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-04-21 13:38:03,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 13:38:03,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-21 14:38:03,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 14:38:03,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-21 15:38:03,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 15:38:03,840 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 20 msec
2023-04-21 15:38:03,983 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-04-21 15:38:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-21 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-21 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id 1120026002 in 900000ms
2023-04-21 15:38:03,986 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-21 15:38:03,986 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id -1018473530 in 900000ms
2023-04-21 15:38:25,863 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-21 15:38:27,344 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 116
2023-04-21 15:38:27,344 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-21 15:38:27,344 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-21 15:38:27,344 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 124
2023-04-21 15:38:27,344 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-21 15:53:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: -490698604
2023-04-21 15:53:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-21 15:53:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: 1120026002
2023-04-21 15:53:03,986 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: -1018473530
2023-04-21 16:38:03,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 16:38:03,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-21 17:38:03,819 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 17:38:03,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-21 18:38:03,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 18:38:03,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 30 msec
2023-04-21 19:38:03,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 19:38:03,833 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-21 20:38:03,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 20:38:03,848 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 27 msec
2023-04-21 21:38:03,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 21:38:03,878 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 57 msec
2023-04-21 22:38:03,820 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 22:38:03,823 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 3 msec
2023-04-21 23:38:03,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-21 23:38:03,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-22 00:38:03,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 00:38:03,829 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-22 01:38:03,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 01:38:03,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-22 02:38:03,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 02:38:03,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-22 03:38:03,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 03:38:03,838 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-22 04:38:03,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 04:38:03,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-22 05:38:03,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 05:38:03,835 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-22 06:38:03,821 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 06:38:03,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-22 07:38:03,822 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 07:38:03,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 10 msec
2023-04-22 08:38:03,822 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 08:38:03,837 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-22 09:38:03,822 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 09:38:03,831 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-22 10:38:03,823 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 10:38:03,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-04-22 11:38:03,823 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 11:38:03,828 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-22 12:38:03,823 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 12:38:03,838 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 14 msec
2023-04-22 13:38:03,823 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 13:38:03,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 9 msec
2023-04-22 14:38:03,824 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 14:38:03,836 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-22 15:38:03,824 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 15:38:03,829 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 6 msec
2023-04-22 15:38:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Rolling master-key for amrm-tokens
2023-04-22 15:38:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-22 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens
2023-04-22 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Going to activate master-key with key-id -1018473529 in 900000ms
2023-04-22 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Rolling master-key for container-tokens
2023-04-22 15:38:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Going to activate master-key with key-id 1120026003 in 900000ms
2023-04-22 15:38:28,222 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-22 15:38:29,712 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: removing master key with keyID 117
2023-04-22 15:38:29,712 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Removing RMDTMasterKey.
2023-04-22 15:38:29,712 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2023-04-22 15:38:29,712 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID 125
2023-04-22 15:38:29,712 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing RMDTMasterKey.
2023-04-22 15:53:03,984 INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Activating next master key with id: -490698603
2023-04-22 15:53:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken
2023-04-22 15:53:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Activating next master key with id: -1018473529
2023-04-22 15:53:03,985 INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: Activating next master key with id: 1120026003
2023-04-22 16:38:03,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 16:38:03,850 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 25 msec
2023-04-22 17:38:03,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 17:38:03,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-22 18:38:03,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 18:38:03,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-22 19:38:03,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 19:38:03,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 4 msec
2023-04-22 20:38:03,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 20:38:03,844 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-04-22 21:38:03,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 21:38:03,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-22 22:38:03,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 22:38:03,848 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 23 msec
2023-04-22 23:38:03,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-22 23:38:03,841 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 16 msec
2023-04-23 00:38:03,826 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-23 00:38:03,834 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 8 msec
2023-04-23 01:38:03,825 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-23 01:38:03,840 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-23 02:38:03,826 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-23 02:38:03,832 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 7 msec
2023-04-23 03:38:03,826 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-23 03:38:03,842 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 15 msec
2023-04-23 04:38:03,826 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-23 04:38:03,839 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 12 msec
2023-04-23 05:38:03,826 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-23 05:38:03,839 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 13 msec
2023-04-23 06:38:03,826 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-23 06:38:03,830 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
2023-04-23 07:38:03,826 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-23 07:38:03,847 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 21 msec
2023-04-23 08:38:03,826 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-23 08:38:03,845 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 19 msec
2023-04-23 09:38:03,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-23 09:38:03,837 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 11 msec
2023-04-23 10:38:03,827 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Starting full compaction cycle
2023-04-23 10:38:03,831 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: Full compaction cycle completed in 5 msec
